--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
File location :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): Linear(in_features=6, out_features=6, bias=True)
    (2): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
No pre-trained models available, initializing model weights
--------------------------------------------------
Training model with: num_epochs=2600, start_lr=0.005
Epoch [   1/2600] -> Loss: 3822.5605
Epoch [   2/2600] -> Loss: 2961.9339
Epoch [   3/2600] -> Loss: 2816.9322
Epoch [   4/2600] -> Loss: 2720.7932
Epoch [   5/2600] -> Loss: 2453.5509
Epoch [   6/2600] -> Loss: 2273.2119
Epoch [   7/2600] -> Loss: 2005.8736
Epoch [   8/2600] -> Loss: 1886.2587
Epoch [   9/2600] -> Loss: 1923.5279
Epoch [  10/2600] -> Loss: 1854.8364
Epoch [  11/2600] -> Loss: 1786.4553
Epoch [  12/2600] -> Loss: 1809.2937
Epoch [  13/2600] -> Loss: 1746.3014
Epoch [  14/2600] -> Loss: 1787.4637
Epoch [  15/2600] -> Loss: 1717.2928
Epoch [  16/2600] -> Loss: 1729.1612
Epoch [  17/2600] -> Loss: 1682.2147
Epoch [  18/2600] -> Loss: 1726.1681
Epoch [  19/2600] -> Loss: 1705.3522
Epoch [  20/2600] -> Loss: 1736.8340
Epoch [  21/2600] -> Loss: 1720.6010
Epoch [  22/2600] -> Loss: 1736.7061
Epoch [  23/2600] -> Loss: 1684.7635
Epoch [  24/2600] -> Loss: 1663.3422
Epoch [  25/2600] -> Loss: 1684.6279
Epoch [  26/2600] -> Loss: 1731.9923
Epoch [  27/2600] -> Loss: 1675.9045
Epoch [  28/2600] -> Loss: 1640.8066
Epoch [  29/2600] -> Loss: 1680.5475
Epoch [  30/2600] -> Loss: 1695.4153
Epoch [  31/2600] -> Loss: 1691.3591
Epoch [  32/2600] -> Loss: 1647.7051
Epoch [  33/2600] -> Loss: 1670.1620
Epoch [  34/2600] -> Loss: 1690.1596
Epoch [  35/2600] -> Loss: 1739.7124
Epoch [  36/2600] -> Loss: 1683.6474
Epoch [  37/2600] -> Loss: 1674.3643
Epoch [  38/2600] -> Loss: 1647.4167
Epoch    39: reducing learning rate of group 0 to 2.5000e-03.
Epoch [  39/2600] -> Loss: 1661.8885
Epoch [  40/2600] -> Loss: 1670.8817
Epoch [  41/2600] -> Loss: 1637.8682
Epoch [  42/2600] -> Loss: 1631.4433
Epoch [  43/2600] -> Loss: 1650.7101
Epoch [  44/2600] -> Loss: 1682.7176
Epoch [  45/2600] -> Loss: 1644.6166
Epoch [  46/2600] -> Loss: 1646.4234
Epoch [  47/2600] -> Loss: 1647.8412
Epoch [  48/2600] -> Loss: 1650.0737
Epoch [  49/2600] -> Loss: 1642.7710
Epoch [  50/2600] -> Loss: 1633.6158
Epoch [  51/2600] -> Loss: 1656.2704
Epoch [  52/2600] -> Loss: 1643.2339
Epoch    53: reducing learning rate of group 0 to 1.2500e-03.
Epoch [  53/2600] -> Loss: 1642.3715
Epoch [  54/2600] -> Loss: 1626.8913
Epoch [  55/2600] -> Loss: 1633.3785
Epoch [  56/2600] -> Loss: 1622.5377
Epoch [  57/2600] -> Loss: 1618.7686
Epoch [  58/2600] -> Loss: 1625.1962
Epoch [  59/2600] -> Loss: 1639.2008
Epoch [  60/2600] -> Loss: 1627.7331
Epoch [  61/2600] -> Loss: 1633.9604
Epoch [  62/2600] -> Loss: 1629.9882
Epoch [  63/2600] -> Loss: 1628.8190
Epoch [  64/2600] -> Loss: 1627.7440
Epoch [  65/2600] -> Loss: 1630.9035
Epoch [  66/2600] -> Loss: 1643.2152
Epoch [  67/2600] -> Loss: 1629.5799
Epoch    68: reducing learning rate of group 0 to 6.2500e-04.
Epoch [  68/2600] -> Loss: 1624.8092
Epoch [  69/2600] -> Loss: 1637.5483
Epoch [  70/2600] -> Loss: 1619.6255
Epoch [  71/2600] -> Loss: 1630.7685
Epoch [  72/2600] -> Loss: 1621.9930
Epoch [  73/2600] -> Loss: 1621.1325
Epoch [  74/2600] -> Loss: 1619.6347
Epoch [  75/2600] -> Loss: 1630.1019
Epoch [  76/2600] -> Loss: 1618.5836
Epoch [  77/2600] -> Loss: 1625.5929
Epoch [  78/2600] -> Loss: 1624.0169
Epoch [  79/2600] -> Loss: 1619.4136
Epoch [  80/2600] -> Loss: 1632.5930
Epoch [  81/2600] -> Loss: 1625.5425
Epoch [  82/2600] -> Loss: 1620.6435
Epoch [  83/2600] -> Loss: 1621.9936
Epoch [  84/2600] -> Loss: 1622.9379
Epoch [  85/2600] -> Loss: 1649.9677
Epoch [  86/2600] -> Loss: 1616.4414
Epoch [  87/2600] -> Loss: 1622.4785
Epoch [  88/2600] -> Loss: 1621.8011
Epoch [  89/2600] -> Loss: 1617.5239
Epoch [  90/2600] -> Loss: 1627.9856
Epoch [  91/2600] -> Loss: 1617.6756
Epoch [  92/2600] -> Loss: 1625.4210
Epoch [  93/2600] -> Loss: 1620.1257
Epoch [  94/2600] -> Loss: 1624.8938
Epoch [  95/2600] -> Loss: 1621.8607
Epoch [  96/2600] -> Loss: 1625.3069
Epoch    97: reducing learning rate of group 0 to 3.1250e-04.
Epoch [  97/2600] -> Loss: 1632.0730
Epoch [  98/2600] -> Loss: 1618.5124
Epoch [  99/2600] -> Loss: 1619.1751
Epoch [ 100/2600] -> Loss: 1627.0175
Epoch [ 101/2600] -> Loss: 1619.9115
Epoch [ 102/2600] -> Loss: 1615.7025
Epoch [ 103/2600] -> Loss: 1618.2952
Epoch [ 104/2600] -> Loss: 1622.1503
Epoch [ 105/2600] -> Loss: 1615.8672
Epoch [ 106/2600] -> Loss: 1620.1165
Epoch [ 107/2600] -> Loss: 1618.2368
Epoch [ 108/2600] -> Loss: 1622.7577
Epoch [ 109/2600] -> Loss: 1616.4101
Epoch [ 110/2600] -> Loss: 1616.7235
Epoch [ 111/2600] -> Loss: 1618.9935
Epoch [ 112/2600] -> Loss: 1622.5619
Epoch   113: reducing learning rate of group 0 to 1.5625e-04.
Epoch [ 113/2600] -> Loss: 1619.5992
Epoch [ 114/2600] -> Loss: 1616.3704
Epoch [ 115/2600] -> Loss: 1614.9320
Epoch [ 116/2600] -> Loss: 1618.1658
Epoch [ 117/2600] -> Loss: 1614.4017
Epoch [ 118/2600] -> Loss: 1615.3235
Epoch [ 119/2600] -> Loss: 1616.2914
Epoch [ 120/2600] -> Loss: 1619.4793
Epoch [ 121/2600] -> Loss: 1617.3567
