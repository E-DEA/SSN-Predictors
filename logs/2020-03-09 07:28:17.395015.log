--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
File location :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
Pre-trained model available, loading model weights
--------------------------------------------------
Training model with: num_epochs=1800, start_lr=0.0005
Epoch [   1/1800] -> Loss: 30.8409
Epoch [   2/1800] -> Loss: 30.6973
Epoch [   3/1800] -> Loss: 30.1455
Epoch [   4/1800] -> Loss: 30.2357
Epoch [   5/1800] -> Loss: 29.9656
Epoch [   6/1800] -> Loss: 29.7739
Epoch [   7/1800] -> Loss: 29.6452
Epoch [   8/1800] -> Loss: 29.4877
Epoch [   9/1800] -> Loss: 29.3897
Epoch [  10/1800] -> Loss: 29.2813
Epoch [  11/1800] -> Loss: 29.1576
Epoch [  12/1800] -> Loss: 29.0591
Epoch [  13/1800] -> Loss: 28.9250
Epoch [  14/1800] -> Loss: 28.8804
Epoch [  15/1800] -> Loss: 28.7524
Epoch [  16/1800] -> Loss: 28.7571
Epoch [  17/1800] -> Loss: 28.6490
Epoch [  18/1800] -> Loss: 28.5391
Epoch [  19/1800] -> Loss: 28.4825
Epoch [  20/1800] -> Loss: 28.4866
Epoch [  21/1800] -> Loss: 28.5698
Epoch [  22/1800] -> Loss: 28.5035
Epoch [  23/1800] -> Loss: 28.3120
Epoch [  24/1800] -> Loss: 28.2758
Epoch [  25/1800] -> Loss: 28.3061
Epoch [  26/1800] -> Loss: 28.2504
Epoch [  27/1800] -> Loss: 28.2405
Epoch [  28/1800] -> Loss: 28.1196
Epoch [  29/1800] -> Loss: 28.2053
Epoch [  30/1800] -> Loss: 28.2242
Epoch [  31/1800] -> Loss: 28.1833
Epoch [  32/1800] -> Loss: 28.1592
Epoch [  33/1800] -> Loss: 28.2535
Epoch [  34/1800] -> Loss: 28.0975
Epoch [  35/1800] -> Loss: 28.1086
Epoch [  36/1800] -> Loss: 28.0725
Epoch [  37/1800] -> Loss: 28.0113
Epoch [  38/1800] -> Loss: 28.1311
Epoch [  39/1800] -> Loss: 27.9357
Epoch [  40/1800] -> Loss: 28.0251
Epoch [  41/1800] -> Loss: 28.0375
Epoch [  42/1800] -> Loss: 27.9607
Epoch [  43/1800] -> Loss: 27.8812
Epoch [  44/1800] -> Loss: 28.1004
Epoch [  45/1800] -> Loss: 28.0276
Epoch [  46/1800] -> Loss: 27.9506
Epoch [  47/1800] -> Loss: 28.0147
Epoch [  48/1800] -> Loss: 28.0040
Epoch [  49/1800] -> Loss: 27.8693
Epoch [  50/1800] -> Loss: 27.8738
Epoch [  51/1800] -> Loss: 27.9096
Epoch [  52/1800] -> Loss: 28.0091
Epoch [  53/1800] -> Loss: 27.8119
Epoch [  54/1800] -> Loss: 27.7671
Epoch [  55/1800] -> Loss: 27.9531
Epoch [  56/1800] -> Loss: 27.8791
Epoch [  57/1800] -> Loss: 27.8563
Epoch [  58/1800] -> Loss: 27.8672
Epoch [  59/1800] -> Loss: 27.9439
Epoch [  60/1800] -> Loss: 27.7773
Epoch [  61/1800] -> Loss: 27.7441
Epoch [  62/1800] -> Loss: 27.9125
Epoch [  63/1800] -> Loss: 27.8830
Epoch [  64/1800] -> Loss: 27.8522
Epoch [  65/1800] -> Loss: 27.6803
Epoch [  66/1800] -> Loss: 27.8941
Epoch [  67/1800] -> Loss: 27.8012
Epoch [  68/1800] -> Loss: 27.8064
Epoch [  69/1800] -> Loss: 27.9377
Epoch [  70/1800] -> Loss: 27.6930
Epoch [  71/1800] -> Loss: 27.8271
Epoch [  72/1800] -> Loss: 27.7579
Epoch [  73/1800] -> Loss: 27.7210
Epoch [  74/1800] -> Loss: 27.6273
Epoch [  75/1800] -> Loss: 27.7575
Epoch [  76/1800] -> Loss: 27.7503
Epoch [  77/1800] -> Loss: 27.8808
Epoch [  78/1800] -> Loss: 27.7126
Epoch [  79/1800] -> Loss: 27.7963
Epoch [  80/1800] -> Loss: 27.7541
Epoch [  81/1800] -> Loss: 27.7325
Epoch [  82/1800] -> Loss: 27.6468
Epoch [  83/1800] -> Loss: 27.6760
Epoch [  84/1800] -> Loss: 27.5866
Epoch [  85/1800] -> Loss: 27.6670
Epoch [  86/1800] -> Loss: 27.7580
Epoch [  87/1800] -> Loss: 27.6986
Epoch [  88/1800] -> Loss: 27.6640
Epoch [  89/1800] -> Loss: 27.6120
Epoch [  90/1800] -> Loss: 27.6631
Epoch [  91/1800] -> Loss: 27.5624
Epoch [  92/1800] -> Loss: 27.7310
Epoch [  93/1800] -> Loss: 27.6004
Epoch [  94/1800] -> Loss: 27.5957
Epoch [  95/1800] -> Loss: 27.4928
Epoch [  96/1800] -> Loss: 27.5755
Epoch [  97/1800] -> Loss: 27.5291
Epoch [  98/1800] -> Loss: 27.6145
Epoch [  99/1800] -> Loss: 27.5771
--------------------------------------------------
Model checkpoint saved as FFNN_100.pth
--------------------------------------------------
Epoch [ 100/1800] -> Loss: 27.7028
Epoch [ 101/1800] -> Loss: 27.3489
Epoch [ 102/1800] -> Loss: 27.5878
Epoch [ 103/1800] -> Loss: 27.5371
Epoch [ 104/1800] -> Loss: 27.7115
Epoch [ 105/1800] -> Loss: 27.5933
Epoch [ 106/1800] -> Loss: 27.4460
Epoch [ 107/1800] -> Loss: 27.5932
Epoch [ 108/1800] -> Loss: 27.4316
Epoch [ 109/1800] -> Loss: 27.5308
Epoch [ 110/1800] -> Loss: 27.4930
Epoch [ 111/1800] -> Loss: 27.4531
Epoch   112: reducing learning rate of group 0 to 2.5000e-04.
Epoch [ 112/1800] -> Loss: 27.5020
Epoch [ 113/1800] -> Loss: 27.3851
Epoch [ 114/1800] -> Loss: 27.3527
Epoch [ 115/1800] -> Loss: 27.3197
Epoch [ 116/1800] -> Loss: 27.3619
Epoch [ 117/1800] -> Loss: 27.3437
Epoch [ 118/1800] -> Loss: 27.3340
Epoch [ 119/1800] -> Loss: 27.4054
Epoch [ 120/1800] -> Loss: 27.4072
Epoch [ 121/1800] -> Loss: 27.4455
Epoch [ 122/1800] -> Loss: 27.3654
Epoch [ 123/1800] -> Loss: 27.3741
Epoch [ 124/1800] -> Loss: 27.3176
Epoch [ 125/1800] -> Loss: 27.3088
Epoch [ 126/1800] -> Loss: 27.3727
Epoch [ 127/1800] -> Loss: 27.2460
Epoch [ 128/1800] -> Loss: 27.3117
Epoch [ 129/1800] -> Loss: 27.3536
Epoch [ 130/1800] -> Loss: 27.3778
Epoch [ 131/1800] -> Loss: 27.3523
Epoch [ 132/1800] -> Loss: 27.3170
Epoch [ 133/1800] -> Loss: 27.3008
Epoch [ 134/1800] -> Loss: 27.3077
Epoch [ 135/1800] -> Loss: 27.3181
Epoch [ 136/1800] -> Loss: 27.3553
Epoch [ 137/1800] -> Loss: 27.2366
Epoch [ 138/1800] -> Loss: 27.1789
Epoch [ 139/1800] -> Loss: 27.3608
Epoch [ 140/1800] -> Loss: 27.3247
Epoch [ 141/1800] -> Loss: 27.2828
Epoch [ 142/1800] -> Loss: 27.2865
Epoch [ 143/1800] -> Loss: 27.2260
Epoch [ 144/1800] -> Loss: 27.3253
Epoch [ 145/1800] -> Loss: 27.2930
Epoch [ 146/1800] -> Loss: 27.2620
Epoch [ 147/1800] -> Loss: 27.3531
Epoch [ 148/1800] -> Loss: 27.2613
Epoch   149: reducing learning rate of group 0 to 1.2500e-04.
Epoch [ 149/1800] -> Loss: 27.1966
Epoch [ 150/1800] -> Loss: 27.2443
Epoch [ 151/1800] -> Loss: 27.1868
Epoch [ 152/1800] -> Loss: 27.2263
