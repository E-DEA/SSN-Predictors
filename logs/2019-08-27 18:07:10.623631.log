--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
Dataset source : SILSO, ISGI
File location :
    SSN - /home/extern/Documents/Research/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/data/ISGI/aa_1869-08-01_2017-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
No pre-trained models available, initializing model weights
--------------------------------------------------
Training model with: num_epochs=450, start_lr=0.0005
Epoch [   1/450] -> Loss: 3178.8078
Epoch [   2/450] -> Loss: 1042.7468
Epoch [   3/450] -> Loss: 1029.4331
Epoch [   4/450] -> Loss: 1021.2434
Epoch [   5/450] -> Loss: 1018.9979
Epoch [   6/450] -> Loss: 1017.6417
Epoch [   7/450] -> Loss: 1010.1181
Epoch [   8/450] -> Loss: 1005.1313
Epoch [   9/450] -> Loss: 1010.2047
Epoch [  10/450] -> Loss: 1003.9150
Epoch [  11/450] -> Loss: 1005.0499
Epoch [  12/450] -> Loss: 1005.7183
Epoch [  13/450] -> Loss: 1007.1915
Epoch [  14/450] -> Loss: 1002.2591
Epoch [  15/450] -> Loss: 1002.1016
Epoch [  16/450] -> Loss: 1001.6105
Epoch [  17/450] -> Loss: 998.3027
Epoch [  18/450] -> Loss: 998.4276
Epoch [  19/450] -> Loss: 997.3262
Epoch [  20/450] -> Loss: 997.9082
Epoch [  21/450] -> Loss: 995.6187
Epoch [  22/450] -> Loss: 995.7338
Epoch [  23/450] -> Loss: 995.1412
Epoch [  24/450] -> Loss: 995.1345
Epoch [  25/450] -> Loss: 996.3144
Epoch [  26/450] -> Loss: 994.1310
Epoch [  27/450] -> Loss: 993.2850
Epoch [  28/450] -> Loss: 992.4608
Epoch [  29/450] -> Loss: 987.3069
Epoch [  30/450] -> Loss: 992.6577
Epoch [  31/450] -> Loss: 988.0874
Epoch [  32/450] -> Loss: 989.0322
Epoch [  33/450] -> Loss: 984.1730
Epoch [  34/450] -> Loss: 988.8246
Epoch [  35/450] -> Loss: 983.5488
Epoch [  36/450] -> Loss: 984.0605
Epoch [  37/450] -> Loss: 982.7240
Epoch [  38/450] -> Loss: 980.9060
Epoch [  39/450] -> Loss: 982.5854
Epoch [  40/450] -> Loss: 979.2661
Epoch [  41/450] -> Loss: 981.0486
Epoch [  42/450] -> Loss: 978.0107
Epoch [  43/450] -> Loss: 977.5153
Epoch [  44/450] -> Loss: 976.6645
Epoch [  45/450] -> Loss: 976.1291
Epoch [  46/450] -> Loss: 972.2235
Epoch [  47/450] -> Loss: 977.4189
Epoch [  48/450] -> Loss: 967.8127
Epoch [  49/450] -> Loss: 979.5899
--------------------------------------------------
Model checkpoint saved as FFNN_50.pth
--------------------------------------------------
Epoch [  50/450] -> Loss: 969.2535
Epoch [  51/450] -> Loss: 972.6511
Epoch [  52/450] -> Loss: 972.9205
Epoch [  53/450] -> Loss: 969.0557
Epoch [  54/450] -> Loss: 969.2657
Epoch [  55/450] -> Loss: 968.9099
Epoch [  56/450] -> Loss: 963.6208
Epoch [  57/450] -> Loss: 969.2919
Epoch [  58/450] -> Loss: 964.7001
Epoch [  59/450] -> Loss: 965.7013
Epoch [  60/450] -> Loss: 961.5232
Epoch [  61/450] -> Loss: 964.2994
Epoch [  62/450] -> Loss: 961.6931
Epoch [  63/450] -> Loss: 957.9166
Epoch [  64/450] -> Loss: 959.1892
Epoch [  65/450] -> Loss: 962.1361
Epoch [  66/450] -> Loss: 957.9018
Epoch [  67/450] -> Loss: 954.6387
Epoch [  68/450] -> Loss: 958.9803
Epoch [  69/450] -> Loss: 948.1225
Epoch [  70/450] -> Loss: 954.0366
Epoch [  71/450] -> Loss: 953.1011
Epoch [  72/450] -> Loss: 953.0289
Epoch [  73/450] -> Loss: 953.8655
Epoch [  74/450] -> Loss: 952.0943
Epoch [  75/450] -> Loss: 950.9640
Epoch [  76/450] -> Loss: 949.4850
Epoch [  77/450] -> Loss: 946.3875
Epoch [  78/450] -> Loss: 948.4775
Epoch [  79/450] -> Loss: 946.8213
Epoch [  80/450] -> Loss: 945.5295
Epoch [  81/450] -> Loss: 939.7906
Epoch [  82/450] -> Loss: 941.6995
Epoch [  83/450] -> Loss: 944.0114
Epoch [  84/450] -> Loss: 942.6952
Epoch [  85/450] -> Loss: 943.9642
Epoch [  86/450] -> Loss: 937.7647
Epoch [  87/450] -> Loss: 939.2946
Epoch [  88/450] -> Loss: 937.4947
Epoch [  89/450] -> Loss: 937.6485
Epoch [  90/450] -> Loss: 935.9567
Epoch [  91/450] -> Loss: 932.7896
Epoch [  92/450] -> Loss: 935.3533
Epoch [  93/450] -> Loss: 932.2708
Epoch [  94/450] -> Loss: 926.7046
Epoch [  95/450] -> Loss: 928.7078
Epoch [  96/450] -> Loss: 925.4540
Epoch [  97/450] -> Loss: 930.3902
Epoch [  98/450] -> Loss: 927.6790
Epoch [  99/450] -> Loss: 926.6706
--------------------------------------------------
Model checkpoint saved as FFNN_100.pth
--------------------------------------------------
Epoch [ 100/450] -> Loss: 924.3105
Epoch [ 101/450] -> Loss: 922.1057
Epoch [ 102/450] -> Loss: 922.1612
Epoch [ 103/450] -> Loss: 922.1206
Epoch [ 104/450] -> Loss: 917.3530
Epoch [ 105/450] -> Loss: 918.2482
Epoch [ 106/450] -> Loss: 919.5814
Epoch [ 107/450] -> Loss: 920.2343
Epoch [ 108/450] -> Loss: 913.1995
Epoch [ 109/450] -> Loss: 917.2531
Epoch [ 110/450] -> Loss: 916.7182
Epoch [ 111/450] -> Loss: 918.5068
Epoch [ 112/450] -> Loss: 910.1746
Epoch [ 113/450] -> Loss: 909.1416
Epoch [ 114/450] -> Loss: 912.6397
Epoch [ 115/450] -> Loss: 902.8248
Epoch [ 116/450] -> Loss: 906.7356
Epoch [ 117/450] -> Loss: 906.8331
Epoch [ 118/450] -> Loss: 905.5346
Epoch [ 119/450] -> Loss: 907.4140
Epoch [ 120/450] -> Loss: 901.3534
Epoch [ 121/450] -> Loss: 901.6690
Epoch [ 122/450] -> Loss: 902.3434
Epoch [ 123/450] -> Loss: 901.0241
Epoch [ 124/450] -> Loss: 902.4262
Epoch [ 125/450] -> Loss: 897.8470
Epoch [ 126/450] -> Loss: 898.2722
Epoch [ 127/450] -> Loss: 900.1677
Epoch [ 128/450] -> Loss: 895.8512
Epoch [ 129/450] -> Loss: 895.3076
Epoch [ 130/450] -> Loss: 893.5680
Epoch [ 131/450] -> Loss: 896.8944
Epoch [ 132/450] -> Loss: 895.3143
Epoch [ 133/450] -> Loss: 892.1715
Epoch [ 134/450] -> Loss: 891.5767
Epoch [ 135/450] -> Loss: 893.9401
Epoch [ 136/450] -> Loss: 891.0965
Epoch [ 137/450] -> Loss: 890.6252
Epoch [ 138/450] -> Loss: 889.9640
Epoch [ 139/450] -> Loss: 891.0006
Epoch [ 140/450] -> Loss: 887.3674
Epoch [ 141/450] -> Loss: 888.0585
Epoch [ 142/450] -> Loss: 884.6775
Epoch [ 143/450] -> Loss: 886.4760
Epoch [ 144/450] -> Loss: 886.9472
Epoch [ 145/450] -> Loss: 883.8531
Epoch [ 146/450] -> Loss: 882.8764
Epoch [ 147/450] -> Loss: 882.7441
Epoch [ 148/450] -> Loss: 883.2076
Epoch [ 149/450] -> Loss: 885.2688
--------------------------------------------------
Model checkpoint saved as FFNN_150.pth
--------------------------------------------------
Epoch [ 150/450] -> Loss: 877.7503
Epoch [ 151/450] -> Loss: 878.8223
Epoch [ 152/450] -> Loss: 881.9603
Epoch [ 153/450] -> Loss: 877.2514
Epoch [ 154/450] -> Loss: 874.6532
Epoch [ 155/450] -> Loss: 873.0583
Epoch [ 156/450] -> Loss: 878.9339
Epoch [ 157/450] -> Loss: 880.3536
Epoch [ 158/450] -> Loss: 875.8738
Epoch [ 159/450] -> Loss: 877.3446
Epoch [ 160/450] -> Loss: 873.0490
Epoch [ 161/450] -> Loss: 877.7379
Epoch [ 162/450] -> Loss: 873.7140
Epoch [ 163/450] -> Loss: 878.3012
Epoch [ 164/450] -> Loss: 873.8677
Epoch [ 165/450] -> Loss: 873.4859
Epoch   165: reducing learning rate of group 0 to 2.5000e-04.
Epoch [ 166/450] -> Loss: 875.3516
Epoch [ 167/450] -> Loss: 869.3674
Epoch [ 168/450] -> Loss: 872.4660
Epoch [ 169/450] -> Loss: 867.1133
Epoch [ 170/450] -> Loss: 870.3555
Epoch [ 171/450] -> Loss: 873.5056
Epoch [ 172/450] -> Loss: 869.7896
Epoch [ 173/450] -> Loss: 869.8249
Epoch [ 174/450] -> Loss: 870.2356
Epoch [ 175/450] -> Loss: 868.0569
Epoch [ 176/450] -> Loss: 869.9763
Epoch [ 177/450] -> Loss: 869.7750
Epoch [ 178/450] -> Loss: 870.2005
Epoch [ 179/450] -> Loss: 866.3815
Epoch [ 180/450] -> Loss: 870.1900
Epoch [ 181/450] -> Loss: 869.7920
Epoch [ 182/450] -> Loss: 868.6765
Epoch [ 183/450] -> Loss: 869.5705
Epoch [ 184/450] -> Loss: 869.4711
Epoch [ 185/450] -> Loss: 870.6071
Epoch [ 186/450] -> Loss: 868.4300
Epoch [ 187/450] -> Loss: 868.4020
Epoch [ 188/450] -> Loss: 870.3579
Epoch [ 189/450] -> Loss: 869.3858
Epoch   189: reducing learning rate of group 0 to 1.2500e-04.
Epoch [ 190/450] -> Loss: 868.3388
Epoch [ 191/450] -> Loss: 865.0172
Epoch [ 192/450] -> Loss: 867.8750
Epoch [ 193/450] -> Loss: 867.5905
Epoch [ 194/450] -> Loss: 866.7559
Epoch [ 195/450] -> Loss: 867.7340
Epoch [ 196/450] -> Loss: 866.3405
Epoch [ 197/450] -> Loss: 867.0231
Epoch [ 198/450] -> Loss: 866.7454
Epoch [ 199/450] -> Loss: 866.2264
--------------------------------------------------
Model checkpoint saved as FFNN_200.pth
--------------------------------------------------
Epoch [ 200/450] -> Loss: 867.1320
Epoch [ 201/450] -> Loss: 866.3773
Epoch   201: reducing learning rate of group 0 to 6.2500e-05.
Epoch [ 202/450] -> Loss: 867.2644
Epoch [ 203/450] -> Loss: 865.4235
Epoch [ 204/450] -> Loss: 865.2771
Epoch [ 205/450] -> Loss: 866.4097
Epoch [ 206/450] -> Loss: 865.6384
Epoch [ 207/450] -> Loss: 865.0804
Epoch [ 208/450] -> Loss: 865.4026
Epoch [ 209/450] -> Loss: 865.8314
Epoch [ 210/450] -> Loss: 865.5534
Epoch [ 211/450] -> Loss: 865.4650
Epoch [ 212/450] -> Loss: 865.4848
Epoch   212: reducing learning rate of group 0 to 3.1250e-05.
Epoch [ 213/450] -> Loss: 865.7356
Epoch [ 214/450] -> Loss: 864.9706
Epoch [ 215/450] -> Loss: 865.2386
Epoch [ 216/450] -> Loss: 865.2206
Epoch [ 217/450] -> Loss: 865.0705
Epoch [ 218/450] -> Loss: 865.2948
Epoch [ 219/450] -> Loss: 864.9159
Epoch [ 220/450] -> Loss: 865.0532
Epoch [ 221/450] -> Loss: 865.0476
Epoch [ 222/450] -> Loss: 864.9471
Epoch [ 223/450] -> Loss: 865.0553
Epoch [ 224/450] -> Loss: 864.8932
Epoch [ 225/450] -> Loss: 864.9286
Epoch [ 226/450] -> Loss: 865.0365
Epoch [ 227/450] -> Loss: 865.1512
Epoch [ 228/450] -> Loss: 864.8851
Epoch [ 229/450] -> Loss: 864.9487
Epoch   229: reducing learning rate of group 0 to 1.5625e-05.
Epoch [ 230/450] -> Loss: 864.8678
Epoch [ 231/450] -> Loss: 864.6294
Epoch [ 232/450] -> Loss: 864.5293
Epoch [ 233/450] -> Loss: 864.6342
Epoch [ 234/450] -> Loss: 864.6138
Epoch [ 235/450] -> Loss: 864.5405
Epoch [ 236/450] -> Loss: 864.5433
Epoch [ 237/450] -> Loss: 864.6054
Epoch [ 238/450] -> Loss: 864.5971
Epoch [ 239/450] -> Loss: 864.5643
Epoch [ 240/450] -> Loss: 864.5052
Epoch [ 241/450] -> Loss: 864.5782
Epoch [ 242/450] -> Loss: 864.5743
Epoch   242: reducing learning rate of group 0 to 7.8125e-06.
Epoch [ 243/450] -> Loss: 864.5525
Epoch [ 244/450] -> Loss: 864.3762
Epoch [ 245/450] -> Loss: 864.4054
Epoch [ 246/450] -> Loss: 864.3859
Epoch [ 247/450] -> Loss: 864.3756
Epoch [ 248/450] -> Loss: 864.4320
Epoch [ 249/450] -> Loss: 864.3981
--------------------------------------------------
Model checkpoint saved as FFNN_250.pth
--------------------------------------------------
Epoch [ 250/450] -> Loss: 864.4008
Epoch [ 251/450] -> Loss: 864.4019
Epoch [ 252/450] -> Loss: 864.3531
Epoch [ 253/450] -> Loss: 864.3894
Epoch [ 254/450] -> Loss: 864.4179
Epoch   254: reducing learning rate of group 0 to 3.9063e-06.
Epoch [ 255/450] -> Loss: 864.4652
Epoch [ 256/450] -> Loss: 864.2888
Epoch [ 257/450] -> Loss: 864.3035
Epoch [ 258/450] -> Loss: 864.2913
Epoch [ 259/450] -> Loss: 864.2949
Epoch [ 260/450] -> Loss: 864.2736
Epoch [ 261/450] -> Loss: 864.2811
Epoch [ 262/450] -> Loss: 864.2858
Epoch [ 263/450] -> Loss: 864.3174
Epoch [ 264/450] -> Loss: 864.2970
Epoch [ 265/450] -> Loss: 864.2904
Epoch [ 266/450] -> Loss: 864.2928
Epoch   266: reducing learning rate of group 0 to 1.9531e-06.
Epoch [ 267/450] -> Loss: 864.2765
Epoch [ 268/450] -> Loss: 864.2318
Epoch [ 269/450] -> Loss: 864.2424
Epoch [ 270/450] -> Loss: 864.2391
Epoch [ 271/450] -> Loss: 864.2353
Epoch [ 272/450] -> Loss: 864.2405
Epoch [ 273/450] -> Loss: 864.2356
Epoch [ 274/450] -> Loss: 864.2330
Epoch [ 275/450] -> Loss: 864.2446
Epoch [ 276/450] -> Loss: 864.2303
Epoch [ 277/450] -> Loss: 864.2389
Epoch   277: reducing learning rate of group 0 to 9.7656e-07.
Epoch [ 278/450] -> Loss: 864.2295
Epoch [ 279/450] -> Loss: 864.2109
Epoch [ 280/450] -> Loss: 864.2171
Epoch [ 281/450] -> Loss: 864.2163
Epoch [ 282/450] -> Loss: 864.2144
Epoch [ 283/450] -> Loss: 864.2186
Epoch [ 284/450] -> Loss: 864.2103
Epoch [ 285/450] -> Loss: 864.2125
Epoch [ 286/450] -> Loss: 864.2074
Epoch [ 287/450] -> Loss: 864.2118
Epoch [ 288/450] -> Loss: 864.2170
Epoch   288: reducing learning rate of group 0 to 4.8828e-07.
Epoch [ 289/450] -> Loss: 864.2133
Epoch [ 290/450] -> Loss: 864.2035
Epoch [ 291/450] -> Loss: 864.2030
Epoch [ 292/450] -> Loss: 864.1998
Epoch [ 293/450] -> Loss: 864.2024
Epoch [ 294/450] -> Loss: 864.2018
Epoch [ 295/450] -> Loss: 864.2008
Epoch [ 296/450] -> Loss: 864.2016
Epoch [ 297/450] -> Loss: 864.2006
Epoch [ 298/450] -> Loss: 864.2052
Epoch [ 299/450] -> Loss: 864.1974
--------------------------------------------------
Model checkpoint saved as FFNN_300.pth
--------------------------------------------------
Epoch [ 300/450] -> Loss: 864.1995
Epoch [ 301/450] -> Loss: 864.2002
Epoch [ 302/450] -> Loss: 864.1996
Epoch   302: reducing learning rate of group 0 to 2.4414e-07.
Epoch [ 303/450] -> Loss: 864.2010
Epoch [ 304/450] -> Loss: 864.1925
Epoch [ 305/450] -> Loss: 864.1937
Epoch [ 306/450] -> Loss: 864.1960
Epoch [ 307/450] -> Loss: 864.1951
Epoch [ 308/450] -> Loss: 864.1957
Epoch [ 309/450] -> Loss: 864.1934
Epoch [ 310/450] -> Loss: 864.1946
Epoch [ 311/450] -> Loss: 864.1942
Epoch [ 312/450] -> Loss: 864.1940
Epoch [ 313/450] -> Loss: 864.1948
Epoch   313: reducing learning rate of group 0 to 1.2207e-07.
Epoch [ 314/450] -> Loss: 864.1942
Epoch [ 315/450] -> Loss: 864.1922
Epoch [ 316/450] -> Loss: 864.1916
Epoch [ 317/450] -> Loss: 864.1922
Epoch [ 318/450] -> Loss: 864.1918
Epoch [ 319/450] -> Loss: 864.1923
Epoch [ 320/450] -> Loss: 864.1916
Epoch [ 321/450] -> Loss: 864.1911
Epoch [ 322/450] -> Loss: 864.1908
Epoch [ 323/450] -> Loss: 864.1902
Epoch [ 324/450] -> Loss: 864.1921
Epoch   324: reducing learning rate of group 0 to 6.1035e-08.
Epoch [ 325/450] -> Loss: 864.1907
Epoch [ 326/450] -> Loss: 864.1893
Epoch [ 327/450] -> Loss: 864.1896
Epoch [ 328/450] -> Loss: 864.1894
Epoch [ 329/450] -> Loss: 864.1892
Epoch [ 330/450] -> Loss: 864.1891
Epoch [ 331/450] -> Loss: 864.1893
Epoch [ 332/450] -> Loss: 864.1891
Epoch [ 333/450] -> Loss: 864.1896
Epoch [ 334/450] -> Loss: 864.1895
