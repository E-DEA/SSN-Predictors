--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
File location :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
No pre-trained models available, initializing model weights
--------------------------------------------------
Training model with: num_epochs=2600, start_lr=0.0005
Epoch [   1/2600] -> Loss: 14064.3463
Epoch [   2/2600] -> Loss: 13764.1722
Epoch [   3/2600] -> Loss: 13645.8156
Epoch [   4/2600] -> Loss: 13443.6962
Epoch [   5/2600] -> Loss: 13292.9081
Epoch [   6/2600] -> Loss: 13185.2761
Epoch [   7/2600] -> Loss: 13013.4581
Epoch [   8/2600] -> Loss: 12923.8510
Epoch [   9/2600] -> Loss: 12778.4845
Epoch [  10/2600] -> Loss: 12757.0366
Epoch [  11/2600] -> Loss: 12589.5464
Epoch [  12/2600] -> Loss: 12439.5669
Epoch [  13/2600] -> Loss: 12448.4262
Epoch [  14/2600] -> Loss: 12153.8567
Epoch [  15/2600] -> Loss: 12036.2179
Epoch [  16/2600] -> Loss: 11744.3529
Epoch [  17/2600] -> Loss: 11486.1895
Epoch [  18/2600] -> Loss: 11198.3863
Epoch [  19/2600] -> Loss: 10928.1167
Epoch [  20/2600] -> Loss: 10670.7719
Epoch [  21/2600] -> Loss: 10434.4220
Epoch [  22/2600] -> Loss: 10152.5348
Epoch [  23/2600] -> Loss: 9857.1728
Epoch [  24/2600] -> Loss: 9489.4006
Epoch [  25/2600] -> Loss: 9256.6841
Epoch [  26/2600] -> Loss: 8931.9580
Epoch [  27/2600] -> Loss: 8566.7483
Epoch [  28/2600] -> Loss: 8260.9349
Epoch [  29/2600] -> Loss: 7907.5039
Epoch [  30/2600] -> Loss: 7553.8154
Epoch [  31/2600] -> Loss: 7209.1657
Epoch [  32/2600] -> Loss: 6914.6582
Epoch [  33/2600] -> Loss: 6587.6118
Epoch [  34/2600] -> Loss: 6284.3154
Epoch [  35/2600] -> Loss: 6013.3232
Epoch [  36/2600] -> Loss: 5662.0834
Epoch [  37/2600] -> Loss: 5410.6429
Epoch [  38/2600] -> Loss: 5125.8834
Epoch [  39/2600] -> Loss: 4891.8394
Epoch [  40/2600] -> Loss: 4673.2342
Epoch [  41/2600] -> Loss: 4442.4657
Epoch [  42/2600] -> Loss: 4247.9775
Epoch [  43/2600] -> Loss: 4081.3568
Epoch [  44/2600] -> Loss: 3925.2885
Epoch [  45/2600] -> Loss: 3772.9038
Epoch [  46/2600] -> Loss: 3649.4781
Epoch [  47/2600] -> Loss: 3555.5512
Epoch [  48/2600] -> Loss: 3462.7410
Epoch [  49/2600] -> Loss: 3395.4926
Epoch [  50/2600] -> Loss: 3321.4941
Epoch [  51/2600] -> Loss: 3282.9505
Epoch [  52/2600] -> Loss: 3233.6279
Epoch [  53/2600] -> Loss: 3215.3878
Epoch [  54/2600] -> Loss: 3155.8818
Epoch [  55/2600] -> Loss: 3149.1677
Epoch [  56/2600] -> Loss: 3142.0991
Epoch [  57/2600] -> Loss: 3128.3794
Epoch [  58/2600] -> Loss: 3107.0638
Epoch [  59/2600] -> Loss: 3088.1719
Epoch [  60/2600] -> Loss: 3095.2919
Epoch [  61/2600] -> Loss: 3082.4357
Epoch [  62/2600] -> Loss: 3078.8484
Epoch [  63/2600] -> Loss: 3086.5725
Epoch [  64/2600] -> Loss: 3070.1696
Epoch [  65/2600] -> Loss: 3082.7347
Epoch [  66/2600] -> Loss: 3077.6777
Epoch [  67/2600] -> Loss: 3078.1128
Epoch [  68/2600] -> Loss: 3074.5698
Epoch [  69/2600] -> Loss: 3064.7616
Epoch [  70/2600] -> Loss: 3066.6943
Epoch [  71/2600] -> Loss: 3078.7642
Epoch [  72/2600] -> Loss: 3074.6514
Epoch [  73/2600] -> Loss: 3071.8527
Epoch [  74/2600] -> Loss: 3050.8803
Epoch [  75/2600] -> Loss: 3065.7013
Epoch [  76/2600] -> Loss: 3063.4672
Epoch [  77/2600] -> Loss: 3068.3672
Epoch [  78/2600] -> Loss: 3055.6712
Epoch [  79/2600] -> Loss: 3047.6423
Epoch [  80/2600] -> Loss: 3051.0607
Epoch [  81/2600] -> Loss: 3055.9563
Epoch [  82/2600] -> Loss: 3055.3372
Epoch [  83/2600] -> Loss: 3071.8727
Epoch [  84/2600] -> Loss: 3047.5967
Epoch [  85/2600] -> Loss: 3055.6585
Epoch [  86/2600] -> Loss: 3041.9491
Epoch [  87/2600] -> Loss: 3056.2462
Epoch [  88/2600] -> Loss: 3071.2156
Epoch [  89/2600] -> Loss: 3057.6691
Epoch [  90/2600] -> Loss: 3037.8303
Epoch [  91/2600] -> Loss: 3057.5642
Epoch [  92/2600] -> Loss: 3034.9299
Epoch [  93/2600] -> Loss: 3046.7276
Epoch [  94/2600] -> Loss: 3063.9478
Epoch [  95/2600] -> Loss: 3057.9089
Epoch [  96/2600] -> Loss: 3038.6134
Epoch [  97/2600] -> Loss: 3047.9298
Epoch [  98/2600] -> Loss: 3047.3437
Epoch [  99/2600] -> Loss: 3053.4555
Epoch [ 100/2600] -> Loss: 3037.9678
Epoch [ 101/2600] -> Loss: 3030.8993
Epoch [ 102/2600] -> Loss: 3027.7599
Epoch [ 103/2600] -> Loss: 3031.4733
Epoch [ 104/2600] -> Loss: 3049.8661
Epoch [ 105/2600] -> Loss: 3039.7765
Epoch [ 106/2600] -> Loss: 3034.5948
Epoch [ 107/2600] -> Loss: 3061.6326
Epoch [ 108/2600] -> Loss: 3039.7174
Epoch [ 109/2600] -> Loss: 3042.5668
Epoch [ 110/2600] -> Loss: 3039.8847
Epoch [ 111/2600] -> Loss: 3053.8218
Epoch [ 112/2600] -> Loss: 3041.9990
Epoch   113: reducing learning rate of group 0 to 2.5000e-04.
Epoch [ 113/2600] -> Loss: 3041.6470
Epoch [ 114/2600] -> Loss: 3025.2217
Epoch [ 115/2600] -> Loss: 3023.9373
Epoch [ 116/2600] -> Loss: 3040.4614
Epoch [ 117/2600] -> Loss: 3033.3374
Epoch [ 118/2600] -> Loss: 3016.3429
Epoch [ 119/2600] -> Loss: 3034.2130
Epoch [ 120/2600] -> Loss: 3030.6327
Epoch [ 121/2600] -> Loss: 3026.7687
Epoch [ 122/2600] -> Loss: 3051.6343
Epoch [ 123/2600] -> Loss: 3036.1084
Epoch [ 124/2600] -> Loss: 3052.1593
Epoch [ 125/2600] -> Loss: 3051.3524
Epoch [ 126/2600] -> Loss: 3030.4655
Epoch [ 127/2600] -> Loss: 3029.8945
Epoch [ 128/2600] -> Loss: 3030.9761
Epoch   129: reducing learning rate of group 0 to 1.2500e-04.
Epoch [ 129/2600] -> Loss: 3038.6260
Epoch [ 130/2600] -> Loss: 3034.3307
Epoch [ 131/2600] -> Loss: 3022.8156
Epoch [ 132/2600] -> Loss: 3024.0959
Epoch [ 133/2600] -> Loss: 3015.7920
Epoch [ 134/2600] -> Loss: 3026.5781
Epoch [ 135/2600] -> Loss: 3018.0052
Epoch [ 136/2600] -> Loss: 3035.9931
Epoch [ 137/2600] -> Loss: 3012.8335
Epoch [ 138/2600] -> Loss: 3029.3942
Epoch [ 139/2600] -> Loss: 3033.2033
Epoch [ 140/2600] -> Loss: 3035.2691
Epoch [ 141/2600] -> Loss: 3020.1933
Epoch [ 142/2600] -> Loss: 3031.6255
Epoch [ 143/2600] -> Loss: 3031.0307
Epoch [ 144/2600] -> Loss: 3030.1404
Epoch [ 145/2600] -> Loss: 3027.4269
Epoch [ 146/2600] -> Loss: 3029.6774
Epoch [ 147/2600] -> Loss: 3036.2980
Epoch   148: reducing learning rate of group 0 to 6.2500e-05.
Epoch [ 148/2600] -> Loss: 3023.6820
Epoch [ 149/2600] -> Loss: 3027.1149
Epoch [ 150/2600] -> Loss: 3019.1176
Epoch [ 151/2600] -> Loss: 3020.8764
Epoch [ 152/2600] -> Loss: 3022.3741
Epoch [ 153/2600] -> Loss: 3014.5751
Epoch [ 154/2600] -> Loss: 3040.7877
Epoch [ 155/2600] -> Loss: 3022.9234
Epoch [ 156/2600] -> Loss: 3025.7991
Epoch [ 157/2600] -> Loss: 3029.9895
Epoch [ 158/2600] -> Loss: 3014.1464
Epoch   159: reducing learning rate of group 0 to 3.1250e-05.
Epoch [ 159/2600] -> Loss: 3019.2661
Epoch [ 160/2600] -> Loss: 3016.6728
Epoch [ 161/2600] -> Loss: 3009.4695
Epoch [ 162/2600] -> Loss: 3031.8629
Epoch [ 163/2600] -> Loss: 3005.0446
Epoch [ 164/2600] -> Loss: 3016.7108
Epoch [ 165/2600] -> Loss: 3041.1105
Epoch [ 166/2600] -> Loss: 3041.1521
Epoch [ 167/2600] -> Loss: 3021.7259
Epoch [ 168/2600] -> Loss: 3007.6765
Epoch [ 169/2600] -> Loss: 3013.5016
Epoch [ 170/2600] -> Loss: 3020.8718
Epoch [ 171/2600] -> Loss: 3035.3744
Epoch [ 172/2600] -> Loss: 3023.0760
Epoch [ 173/2600] -> Loss: 3009.1387
Epoch   174: reducing learning rate of group 0 to 1.5625e-05.
Epoch [ 174/2600] -> Loss: 3021.1700
Epoch [ 175/2600] -> Loss: 3024.8972
Epoch [ 176/2600] -> Loss: 3021.3138
Epoch [ 177/2600] -> Loss: 3021.7360
Epoch [ 178/2600] -> Loss: 3030.9198
Epoch [ 179/2600] -> Loss: 3021.3861
Epoch [ 180/2600] -> Loss: 3038.4184
Epoch [ 181/2600] -> Loss: 3013.4359
Epoch [ 182/2600] -> Loss: 3020.4343
Epoch [ 183/2600] -> Loss: 3031.0589
Epoch [ 184/2600] -> Loss: 3022.9420
Epoch   185: reducing learning rate of group 0 to 7.8125e-06.
Epoch [ 185/2600] -> Loss: 3011.6877
Epoch [ 186/2600] -> Loss: 3049.3369
Epoch [ 187/2600] -> Loss: 3029.0732
Epoch [ 188/2600] -> Loss: 3021.6923
Epoch [ 189/2600] -> Loss: 3034.3416
Epoch [ 190/2600] -> Loss: 3041.2353
Epoch [ 191/2600] -> Loss: 3020.4929
Epoch [ 192/2600] -> Loss: 3026.3779
Epoch [ 193/2600] -> Loss: 3015.1314
Epoch [ 194/2600] -> Loss: 3036.2130
Epoch [ 195/2600] -> Loss: 3026.4179
Epoch   196: reducing learning rate of group 0 to 3.9063e-06.
Epoch [ 196/2600] -> Loss: 3028.0829
Epoch [ 197/2600] -> Loss: 3030.9926
Epoch [ 198/2600] -> Loss: 3012.6102
Epoch [ 199/2600] -> Loss: 3013.2698
Epoch [ 200/2600] -> Loss: 3044.2229
Epoch [ 201/2600] -> Loss: 3035.2234
Epoch [ 202/2600] -> Loss: 3021.7068
Epoch [ 203/2600] -> Loss: 3003.9800
Epoch [ 204/2600] -> Loss: 3013.9335
Epoch [ 205/2600] -> Loss: 3029.6393
Epoch [ 206/2600] -> Loss: 3003.7435
Epoch [ 207/2600] -> Loss: 3020.1092
Epoch [ 208/2600] -> Loss: 3012.2187
Epoch [ 209/2600] -> Loss: 3020.3418
Epoch [ 210/2600] -> Loss: 3025.4462
Epoch [ 211/2600] -> Loss: 3025.5847
Epoch [ 212/2600] -> Loss: 3015.7319
Epoch [ 213/2600] -> Loss: 3024.7228
Epoch   214: reducing learning rate of group 0 to 1.9531e-06.
Epoch [ 214/2600] -> Loss: 3022.1942
Epoch [ 215/2600] -> Loss: 3020.7802
Epoch [ 216/2600] -> Loss: 3014.8190
Epoch [ 217/2600] -> Loss: 3015.0441
Epoch [ 218/2600] -> Loss: 3026.7678
Epoch [ 219/2600] -> Loss: 3028.0765
Epoch [ 220/2600] -> Loss: 3025.5369
Epoch [ 221/2600] -> Loss: 3014.0338
Epoch [ 222/2600] -> Loss: 3008.9942
Epoch [ 223/2600] -> Loss: 3013.7635
Epoch [ 224/2600] -> Loss: 3019.0522
Epoch   225: reducing learning rate of group 0 to 9.7656e-07.
Epoch [ 225/2600] -> Loss: 3014.0428
Epoch [ 226/2600] -> Loss: 3056.5766
Epoch [ 227/2600] -> Loss: 3036.3809
Epoch [ 228/2600] -> Loss: 3030.8492
Epoch [ 229/2600] -> Loss: 3025.3762
Epoch [ 230/2600] -> Loss: 3057.3185
Epoch [ 231/2600] -> Loss: 3012.3991
Epoch [ 232/2600] -> Loss: 3007.2376
Epoch [ 233/2600] -> Loss: 3036.3737
Epoch [ 234/2600] -> Loss: 3029.2945
Epoch [ 235/2600] -> Loss: 3031.7326
Epoch [ 236/2600] -> Loss: 3002.2560
Epoch [ 237/2600] -> Loss: 3021.1709
Epoch [ 238/2600] -> Loss: 3017.7926
Epoch [ 239/2600] -> Loss: 3011.0443
Epoch [ 240/2600] -> Loss: 3006.1479
Epoch [ 241/2600] -> Loss: 3018.3085
Epoch [ 242/2600] -> Loss: 3019.1196
Epoch [ 243/2600] -> Loss: 3010.8965
Epoch [ 244/2600] -> Loss: 3017.7083
Epoch [ 245/2600] -> Loss: 3043.4798
Epoch [ 246/2600] -> Loss: 3019.2094
Epoch   247: reducing learning rate of group 0 to 4.8828e-07.
Epoch [ 247/2600] -> Loss: 3026.6784
Epoch [ 248/2600] -> Loss: 3033.3851
Epoch [ 249/2600] -> Loss: 3025.4536
Epoch [ 250/2600] -> Loss: 3008.1108
Epoch [ 251/2600] -> Loss: 3026.6569
Epoch [ 252/2600] -> Loss: 3019.7508
Epoch [ 253/2600] -> Loss: 3022.2982
Epoch [ 254/2600] -> Loss: 3027.4276
Epoch [ 255/2600] -> Loss: 3020.8185
Epoch [ 256/2600] -> Loss: 3040.8284
Epoch [ 257/2600] -> Loss: 3027.3000
Epoch   258: reducing learning rate of group 0 to 2.4414e-07.
Epoch [ 258/2600] -> Loss: 3023.4826
Epoch [ 259/2600] -> Loss: 3034.3338
Epoch [ 260/2600] -> Loss: 3013.4831
Epoch [ 261/2600] -> Loss: 3023.0560
Epoch [ 262/2600] -> Loss: 3008.6646
Epoch [ 263/2600] -> Loss: 3026.3418
Epoch [ 264/2600] -> Loss: 3026.6155
Epoch [ 265/2600] -> Loss: 3023.8574
Epoch [ 266/2600] -> Loss: 3030.9819
Epoch [ 267/2600] -> Loss: 3030.1331
Epoch [ 268/2600] -> Loss: 3025.6051
Epoch   269: reducing learning rate of group 0 to 1.2207e-07.
Epoch [ 269/2600] -> Loss: 3035.9907
Epoch [ 270/2600] -> Loss: 3033.7130
Epoch [ 271/2600] -> Loss: 3019.9498
Epoch [ 272/2600] -> Loss: 3022.5319
Epoch [ 273/2600] -> Loss: 3027.1110
Epoch [ 274/2600] -> Loss: 3020.2892
Epoch [ 275/2600] -> Loss: 3015.5067
Epoch [ 276/2600] -> Loss: 3022.0777
Epoch [ 277/2600] -> Loss: 3028.7700
Epoch [ 278/2600] -> Loss: 3003.6947
Epoch [ 279/2600] -> Loss: 3024.8661
Epoch   280: reducing learning rate of group 0 to 6.1035e-08.
Epoch [ 280/2600] -> Loss: 3015.4984
Epoch [ 281/2600] -> Loss: 3019.7874
Epoch [ 282/2600] -> Loss: 3032.0405
Epoch [ 283/2600] -> Loss: 3016.2724
Epoch [ 284/2600] -> Loss: 3046.5576
Epoch [ 285/2600] -> Loss: 3015.6161
Epoch [ 286/2600] -> Loss: 3029.4525
Epoch [ 287/2600] -> Loss: 3017.7126
Epoch [ 288/2600] -> Loss: 3038.7748
Epoch [ 289/2600] -> Loss: 3023.2343
Epoch [ 290/2600] -> Loss: 3005.4807
Epoch   291: reducing learning rate of group 0 to 3.0518e-08.
Epoch [ 291/2600] -> Loss: 3030.0574
Epoch [ 292/2600] -> Loss: 3028.5550
Epoch [ 293/2600] -> Loss: 3033.1561
Epoch [ 294/2600] -> Loss: 3037.7212
Epoch [ 295/2600] -> Loss: 3018.3633
Epoch [ 296/2600] -> Loss: 3018.8904
Epoch [ 297/2600] -> Loss: 3028.9159
Epoch [ 298/2600] -> Loss: 3008.7680
Epoch [ 299/2600] -> Loss: 3021.0307
Epoch [ 300/2600] -> Loss: 3016.7629
Epoch [ 301/2600] -> Loss: 3019.3434
Epoch   302: reducing learning rate of group 0 to 1.5259e-08.
Epoch [ 302/2600] -> Loss: 3023.8350
Epoch [ 303/2600] -> Loss: 3026.1823
Epoch [ 304/2600] -> Loss: 3027.5773
Epoch [ 305/2600] -> Loss: 3044.7801
Epoch [ 306/2600] -> Loss: 3014.1421
Epoch [ 307/2600] -> Loss: 3019.2003
Epoch [ 308/2600] -> Loss: 3033.9098
Epoch [ 309/2600] -> Loss: 3045.1570
Epoch [ 310/2600] -> Loss: 3020.6386
Epoch [ 311/2600] -> Loss: 3021.8233
Epoch [ 312/2600] -> Loss: 3011.2762
Epoch [ 313/2600] -> Loss: 3029.9996
Epoch [ 314/2600] -> Loss: 3025.0644
Epoch [ 315/2600] -> Loss: 3020.3450
Epoch [ 316/2600] -> Loss: 3025.8705
Epoch [ 317/2600] -> Loss: 3021.8712
Epoch [ 318/2600] -> Loss: 3024.6580
Epoch [ 319/2600] -> Loss: 3017.8030
Epoch [ 320/2600] -> Loss: 3044.6864
Epoch [ 321/2600] -> Loss: 3022.2928
Epoch [ 322/2600] -> Loss: 3021.7125
Epoch [ 323/2600] -> Loss: 3009.5126
Epoch [ 324/2600] -> Loss: 3041.4748
Epoch [ 325/2600] -> Loss: 3002.0490
Epoch [ 326/2600] -> Loss: 3039.5561
Epoch [ 327/2600] -> Loss: 3027.4760
Epoch [ 328/2600] -> Loss: 3027.9847
Epoch [ 329/2600] -> Loss: 3008.1979
Epoch [ 330/2600] -> Loss: 3016.6908
Epoch [ 331/2600] -> Loss: 3022.8774
Epoch [ 332/2600] -> Loss: 3021.0461
Epoch [ 333/2600] -> Loss: 3027.2801
Epoch [ 334/2600] -> Loss: 3027.0722
Epoch [ 335/2600] -> Loss: 3042.7579
Epoch [ 336/2600] -> Loss: 3018.6985
Epoch [ 337/2600] -> Loss: 3027.4029
Epoch [ 338/2600] -> Loss: 3016.0669
Epoch [ 339/2600] -> Loss: 3035.8063
Epoch [ 340/2600] -> Loss: 3018.7761
Epoch [ 341/2600] -> Loss: 3014.9842
Epoch [ 342/2600] -> Loss: 3040.3831
Epoch [ 343/2600] -> Loss: 3024.1554
Epoch [ 344/2600] -> Loss: 3021.6517
Epoch [ 345/2600] -> Loss: 3014.3321
Epoch [ 346/2600] -> Loss: 3029.1319
Epoch [ 347/2600] -> Loss: 3032.3140
Epoch [ 348/2600] -> Loss: 3019.5956
Epoch [ 349/2600] -> Loss: 3031.0347
Epoch [ 350/2600] -> Loss: 3022.7123
Epoch [ 351/2600] -> Loss: 3019.8535
Epoch [ 352/2600] -> Loss: 3014.4566
Epoch [ 353/2600] -> Loss: 3022.7191
Epoch [ 354/2600] -> Loss: 3038.4646
Epoch [ 355/2600] -> Loss: 3024.2605
Epoch [ 356/2600] -> Loss: 3026.0673
Epoch [ 357/2600] -> Loss: 3024.2460
Epoch [ 358/2600] -> Loss: 3033.7139
Epoch [ 359/2600] -> Loss: 3015.4199
Epoch [ 360/2600] -> Loss: 3024.0861
Epoch [ 361/2600] -> Loss: 3019.9184
Epoch [ 362/2600] -> Loss: 3021.7220
Epoch [ 363/2600] -> Loss: 3035.8705
Epoch [ 364/2600] -> Loss: 3030.5764
Epoch [ 365/2600] -> Loss: 3002.7673
Epoch [ 366/2600] -> Loss: 3008.6247
Epoch [ 367/2600] -> Loss: 3018.6308
Epoch [ 368/2600] -> Loss: 3023.8132
Epoch [ 369/2600] -> Loss: 3015.0633
Epoch [ 370/2600] -> Loss: 3027.3098
Epoch [ 371/2600] -> Loss: 3024.6885
Epoch [ 372/2600] -> Loss: 3030.4136
Epoch [ 373/2600] -> Loss: 3038.3252
Epoch [ 374/2600] -> Loss: 3020.8990
Epoch [ 375/2600] -> Loss: 3021.6791
Epoch [ 376/2600] -> Loss: 3039.7035
Epoch [ 377/2600] -> Loss: 3014.3880
Epoch [ 378/2600] -> Loss: 3036.2486
Epoch [ 379/2600] -> Loss: 3030.7377
Epoch [ 380/2600] -> Loss: 3020.5196
Epoch [ 381/2600] -> Loss: 3019.4849
Epoch [ 382/2600] -> Loss: 3015.3830
Epoch [ 383/2600] -> Loss: 3021.5078
Epoch [ 384/2600] -> Loss: 3018.1398
Epoch [ 385/2600] -> Loss: 3014.7645
Epoch [ 386/2600] -> Loss: 3027.6192
Epoch [ 387/2600] -> Loss: 3031.9109
Epoch [ 388/2600] -> Loss: 3032.9562
Epoch [ 389/2600] -> Loss: 3015.7361
Epoch [ 390/2600] -> Loss: 3020.5297
Epoch [ 391/2600] -> Loss: 3012.7481
Epoch [ 392/2600] -> Loss: 3028.8757
Epoch [ 393/2600] -> Loss: 3020.3188
Epoch [ 394/2600] -> Loss: 3019.7180
