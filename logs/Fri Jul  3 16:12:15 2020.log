Usage: python3 SSN_predictor.py <path_to_ssn_datafile> <path_to_aa_datafile>
----------------------------------------------------------------
Code running on device: cuda
----------------------------------------------------------------
Data loaded from file locations :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
----------------------------------------------------------------
Solar cycle data loaded/saved as: cycle_data.pickle
----------------------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=3, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=3, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
----------------------------------------------------------------
No pre-trained models available, initializing model weights
----------------------------------------------------------------
Training model with: num_epochs=1200, start_lr=0.0005
Epoch [   1/1200] -> Loss: 66.9848
Epoch [   2/1200] -> Loss: 40.9426
Epoch [   3/1200] -> Loss: 40.4885
Epoch [   4/1200] -> Loss: 40.1994
Epoch [   5/1200] -> Loss: 39.9495
Epoch [   6/1200] -> Loss: 39.7619
Epoch [   7/1200] -> Loss: 39.5416
Epoch [   8/1200] -> Loss: 39.4046
Epoch [   9/1200] -> Loss: 39.2435
Epoch [  10/1200] -> Loss: 39.0985
Epoch [  11/1200] -> Loss: 38.9367
Epoch [  12/1200] -> Loss: 38.7673
Epoch [  13/1200] -> Loss: 38.5631
Epoch [  14/1200] -> Loss: 38.4031
Epoch [  15/1200] -> Loss: 38.2204
Epoch [  16/1200] -> Loss: 37.6614
Epoch [  17/1200] -> Loss: 36.8842
Epoch [  18/1200] -> Loss: 36.4400
Epoch [  19/1200] -> Loss: 36.0252
Epoch [  20/1200] -> Loss: 35.8118
Epoch [  21/1200] -> Loss: 35.3219
Epoch [  22/1200] -> Loss: 35.1277
Epoch [  23/1200] -> Loss: 34.7149
Epoch [  24/1200] -> Loss: 34.4424
Epoch [  25/1200] -> Loss: 34.2887
Epoch [  26/1200] -> Loss: 34.0485
Epoch [  27/1200] -> Loss: 33.7838
Epoch [  28/1200] -> Loss: 33.5443
Epoch [  29/1200] -> Loss: 33.3360
Epoch [  30/1200] -> Loss: 33.1234
Epoch [  31/1200] -> Loss: 32.9465
Epoch [  32/1200] -> Loss: 32.7430
Epoch [  33/1200] -> Loss: 32.6565
Epoch [  34/1200] -> Loss: 32.3648
Epoch [  35/1200] -> Loss: 32.3253
Epoch [  36/1200] -> Loss: 32.2437
Epoch [  37/1200] -> Loss: 32.0580
Epoch [  38/1200] -> Loss: 31.9616
Epoch [  39/1200] -> Loss: 31.8331
Epoch [  40/1200] -> Loss: 31.6332
Epoch [  41/1200] -> Loss: 31.6122
Epoch [  42/1200] -> Loss: 31.4708
Epoch [  43/1200] -> Loss: 31.4062
Epoch [  44/1200] -> Loss: 31.4452
Epoch [  45/1200] -> Loss: 31.1360
Epoch [  46/1200] -> Loss: 31.1975
Epoch [  47/1200] -> Loss: 30.9860
Epoch [  48/1200] -> Loss: 30.9964
Epoch [  49/1200] -> Loss: 30.9677
Epoch [  50/1200] -> Loss: 30.8806
Epoch [  51/1200] -> Loss: 30.7303
Epoch [  52/1200] -> Loss: 30.7825
Epoch [  53/1200] -> Loss: 30.7110
Epoch [  54/1200] -> Loss: 30.6923
Epoch [  55/1200] -> Loss: 30.6029
Epoch [  56/1200] -> Loss: 30.5623
Epoch [  57/1200] -> Loss: 30.5458
Epoch [  58/1200] -> Loss: 30.4224
Epoch [  59/1200] -> Loss: 30.4095
Epoch [  60/1200] -> Loss: 30.3996
Epoch [  61/1200] -> Loss: 30.3669
Epoch [  62/1200] -> Loss: 30.3798
Epoch [  63/1200] -> Loss: 30.4243
Epoch [  64/1200] -> Loss: 30.3054
Epoch [  65/1200] -> Loss: 30.3471
Epoch [  66/1200] -> Loss: 30.2571
Epoch [  67/1200] -> Loss: 30.2948
Epoch [  68/1200] -> Loss: 30.1857
Epoch [  69/1200] -> Loss: 30.2226
Epoch [  70/1200] -> Loss: 30.2313
Epoch [  71/1200] -> Loss: 30.1346
Epoch [  72/1200] -> Loss: 30.2044
Epoch [  73/1200] -> Loss: 30.0637
Epoch [  74/1200] -> Loss: 30.0024
Epoch [  75/1200] -> Loss: 30.1404
Epoch [  76/1200] -> Loss: 30.0997
Epoch [  77/1200] -> Loss: 30.1184
Epoch [  78/1200] -> Loss: 30.1310
Epoch [  79/1200] -> Loss: 30.1438
Epoch [  80/1200] -> Loss: 30.0553
Epoch [  81/1200] -> Loss: 30.0294
Epoch [  82/1200] -> Loss: 30.0726
Epoch [  83/1200] -> Loss: 30.1676
Epoch [  84/1200] -> Loss: 30.1591
Epoch [  85/1200] -> Loss: 29.9576
Epoch [  86/1200] -> Loss: 29.7227
Epoch [  87/1200] -> Loss: 29.9664
Epoch [  88/1200] -> Loss: 29.9388
Epoch [  89/1200] -> Loss: 29.8812
Epoch [  90/1200] -> Loss: 29.9886
Epoch [  91/1200] -> Loss: 30.1572
Epoch [  92/1200] -> Loss: 30.0088
Epoch [  93/1200] -> Loss: 30.0684
Epoch [  94/1200] -> Loss: 29.9234
Epoch [  95/1200] -> Loss: 30.1163
Epoch [  96/1200] -> Loss: 29.9586
Epoch    97: reducing learning rate of group 0 to 2.5000e-04.
Epoch [  97/1200] -> Loss: 30.0344
Epoch [  98/1200] -> Loss: 29.8137
Epoch [  99/1200] -> Loss: 29.8092
----------------------------------------------------------------
Model checkpoint saved as FFNN_100.pth
----------------------------------------------------------------
Epoch [ 100/1200] -> Loss: 29.7898
Epoch [ 101/1200] -> Loss: 29.8201
Epoch [ 102/1200] -> Loss: 29.7459
Epoch [ 103/1200] -> Loss: 29.8637
Epoch [ 104/1200] -> Loss: 29.8108
Epoch [ 105/1200] -> Loss: 29.7465
Epoch [ 106/1200] -> Loss: 29.8587
Epoch [ 107/1200] -> Loss: 29.8385
Epoch [ 108/1200] -> Loss: 29.6942
Epoch [ 109/1200] -> Loss: 29.8502
Epoch [ 110/1200] -> Loss: 29.7835
Epoch [ 111/1200] -> Loss: 29.7934
Epoch [ 112/1200] -> Loss: 29.7647
Epoch [ 113/1200] -> Loss: 29.7830
Epoch [ 114/1200] -> Loss: 29.7371
Epoch [ 115/1200] -> Loss: 29.7006
Epoch [ 116/1200] -> Loss: 29.7797
Epoch [ 117/1200] -> Loss: 29.7186
Epoch [ 118/1200] -> Loss: 29.8023
Epoch   119: reducing learning rate of group 0 to 1.2500e-04.
Epoch [ 119/1200] -> Loss: 29.8080
Epoch [ 120/1200] -> Loss: 29.7170
Epoch [ 121/1200] -> Loss: 29.7416
Epoch [ 122/1200] -> Loss: 29.6671
