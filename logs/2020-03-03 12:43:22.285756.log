--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
File location :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
No pre-trained models available, initializing model weights
--------------------------------------------------
Training model with: num_epochs=2600, start_lr=0.0005
Epoch [   1/2600] -> Loss: 13053.5109
Epoch [   2/2600] -> Loss: 11944.7725
Epoch [   3/2600] -> Loss: 10025.2067
Epoch [   4/2600] -> Loss: 8096.8951
Epoch [   5/2600] -> Loss: 7208.4911
Epoch [   6/2600] -> Loss: 6958.9467
Epoch [   7/2600] -> Loss: 6895.0155
Epoch [   8/2600] -> Loss: 6725.0877
Epoch [   9/2600] -> Loss: 6649.4619
Epoch [  10/2600] -> Loss: 6599.1148
Epoch [  11/2600] -> Loss: 6476.1324
Epoch [  12/2600] -> Loss: 6449.3439
Epoch [  13/2600] -> Loss: 6304.4246
Epoch [  14/2600] -> Loss: 6184.1458
Epoch [  15/2600] -> Loss: 6053.0607
Epoch [  16/2600] -> Loss: 6038.8562
Epoch [  17/2600] -> Loss: 5958.4178
Epoch [  18/2600] -> Loss: 5934.6152
Epoch [  19/2600] -> Loss: 5836.0272
Epoch [  20/2600] -> Loss: 5816.8959
Epoch [  21/2600] -> Loss: 5825.3178
Epoch [  22/2600] -> Loss: 5747.1209
Epoch [  23/2600] -> Loss: 5753.8406
Epoch [  24/2600] -> Loss: 5774.3241
Epoch [  25/2600] -> Loss: 5733.9858
Epoch [  26/2600] -> Loss: 5688.6099
Epoch [  27/2600] -> Loss: 5711.0001
Epoch [  28/2600] -> Loss: 5669.4329
Epoch [  29/2600] -> Loss: 5674.7020
Epoch [  30/2600] -> Loss: 5626.6326
Epoch [  31/2600] -> Loss: 5693.9900
Epoch [  32/2600] -> Loss: 5632.0859
Epoch [  33/2600] -> Loss: 5688.0936
Epoch [  34/2600] -> Loss: 5681.2437
Epoch [  35/2600] -> Loss: 5685.4156
Epoch [  36/2600] -> Loss: 5626.9322
Epoch [  37/2600] -> Loss: 5622.8521
Epoch [  38/2600] -> Loss: 5646.6313
Epoch [  39/2600] -> Loss: 5616.9556
Epoch [  40/2600] -> Loss: 5642.9630
Epoch [  41/2600] -> Loss: 5616.2057
Epoch [  42/2600] -> Loss: 5610.2234
Epoch [  43/2600] -> Loss: 5603.1812
Epoch [  44/2600] -> Loss: 5583.1732
Epoch [  45/2600] -> Loss: 5615.6917
Epoch [  46/2600] -> Loss: 5601.5908
Epoch [  47/2600] -> Loss: 5625.4053
Epoch [  48/2600] -> Loss: 5558.3501
Epoch [  49/2600] -> Loss: 5602.3268
Epoch [  50/2600] -> Loss: 5627.6676
Epoch [  51/2600] -> Loss: 5585.7133
Epoch [  52/2600] -> Loss: 5547.7364
Epoch [  53/2600] -> Loss: 5580.6716
Epoch [  54/2600] -> Loss: 5585.6884
Epoch [  55/2600] -> Loss: 5534.8175
Epoch [  56/2600] -> Loss: 5596.7016
Epoch [  57/2600] -> Loss: 5554.8924
Epoch [  58/2600] -> Loss: 5536.1414
Epoch [  59/2600] -> Loss: 5565.6619
Epoch [  60/2600] -> Loss: 5561.5835
Epoch [  61/2600] -> Loss: 5556.5133
Epoch [  62/2600] -> Loss: 5540.1818
Epoch [  63/2600] -> Loss: 5547.1145
Epoch [  64/2600] -> Loss: 5499.0940
Epoch [  65/2600] -> Loss: 5520.8608
Epoch [  66/2600] -> Loss: 5541.9971
Epoch [  67/2600] -> Loss: 5551.3709
Epoch [  68/2600] -> Loss: 5529.8958
Epoch [  69/2600] -> Loss: 5526.2072
Epoch [  70/2600] -> Loss: 5490.8681
Epoch [  71/2600] -> Loss: 5507.8032
Epoch [  72/2600] -> Loss: 5488.8336
Epoch [  73/2600] -> Loss: 5528.5883
Epoch [  74/2600] -> Loss: 5453.6996
Epoch [  75/2600] -> Loss: 5456.9770
Epoch [  76/2600] -> Loss: 5466.6869
Epoch [  77/2600] -> Loss: 5541.8656
Epoch [  78/2600] -> Loss: 5436.8385
Epoch [  79/2600] -> Loss: 5535.4452
Epoch [  80/2600] -> Loss: 5492.3298
Epoch [  81/2600] -> Loss: 5481.4735
Epoch [  82/2600] -> Loss: 5454.6584
Epoch [  83/2600] -> Loss: 5476.5822
Epoch [  84/2600] -> Loss: 5455.0728
Epoch [  85/2600] -> Loss: 5510.8664
Epoch [  86/2600] -> Loss: 5467.3887
Epoch [  87/2600] -> Loss: 5433.3457
Epoch [  88/2600] -> Loss: 5454.0003
Epoch [  89/2600] -> Loss: 5462.5610
Epoch [  90/2600] -> Loss: 5447.5034
Epoch [  91/2600] -> Loss: 5474.9365
Epoch [  92/2600] -> Loss: 5414.6050
Epoch [  93/2600] -> Loss: 5464.7310
Epoch [  94/2600] -> Loss: 5454.1401
Epoch [  95/2600] -> Loss: 5422.8870
Epoch [  96/2600] -> Loss: 5444.9371
Epoch [  97/2600] -> Loss: 5410.0045
Epoch [  98/2600] -> Loss: 5423.7259
Epoch [  99/2600] -> Loss: 5425.6191
--------------------------------------------------
Model checkpoint saved as FFNN_100.pth
--------------------------------------------------
Epoch [ 100/2600] -> Loss: 5448.4435
Epoch [ 101/2600] -> Loss: 5412.5541
Epoch [ 102/2600] -> Loss: 5392.6609
Epoch [ 103/2600] -> Loss: 5395.9870
Epoch [ 104/2600] -> Loss: 5376.4974
Epoch [ 105/2600] -> Loss: 5364.8412
Epoch [ 106/2600] -> Loss: 5380.0620
Epoch [ 107/2600] -> Loss: 5427.5433
Epoch [ 108/2600] -> Loss: 5369.7706
Epoch [ 109/2600] -> Loss: 5392.4167
Epoch [ 110/2600] -> Loss: 5375.8018
Epoch [ 111/2600] -> Loss: 5331.7814
Epoch [ 112/2600] -> Loss: 5371.3666
Epoch [ 113/2600] -> Loss: 5374.2099
Epoch [ 114/2600] -> Loss: 5384.8342
Epoch [ 115/2600] -> Loss: 5398.3286
Epoch [ 116/2600] -> Loss: 5366.6928
Epoch [ 117/2600] -> Loss: 5339.1263
Epoch [ 118/2600] -> Loss: 5343.2946
Epoch [ 119/2600] -> Loss: 5354.2056
Epoch [ 120/2600] -> Loss: 5358.7724
Epoch [ 121/2600] -> Loss: 5353.6886
Epoch   122: reducing learning rate of group 0 to 2.5000e-04.
Epoch [ 122/2600] -> Loss: 5336.0940
Epoch [ 123/2600] -> Loss: 5355.0847
Epoch [ 124/2600] -> Loss: 5305.1850
Epoch [ 125/2600] -> Loss: 5335.1848
Epoch [ 126/2600] -> Loss: 5364.2673
Epoch [ 127/2600] -> Loss: 5334.5481
Epoch [ 128/2600] -> Loss: 5362.4853
Epoch [ 129/2600] -> Loss: 5362.6321
Epoch [ 130/2600] -> Loss: 5372.7331
Epoch [ 131/2600] -> Loss: 5342.1974
Epoch [ 132/2600] -> Loss: 5369.2891
Epoch [ 133/2600] -> Loss: 5365.9772
Epoch [ 134/2600] -> Loss: 5306.0687
Epoch   135: reducing learning rate of group 0 to 1.2500e-04.
Epoch [ 135/2600] -> Loss: 5329.6894
Epoch [ 136/2600] -> Loss: 5346.7475
Epoch [ 137/2600] -> Loss: 5371.9564
Epoch [ 138/2600] -> Loss: 5326.8076
Epoch [ 139/2600] -> Loss: 5358.1646
Epoch [ 140/2600] -> Loss: 5334.6696
Epoch [ 141/2600] -> Loss: 5310.5225
Epoch [ 142/2600] -> Loss: 5323.1229
Epoch [ 143/2600] -> Loss: 5353.0671
Epoch [ 144/2600] -> Loss: 5344.8367
Epoch [ 145/2600] -> Loss: 5329.5342
Epoch   146: reducing learning rate of group 0 to 6.2500e-05.
Epoch [ 146/2600] -> Loss: 5331.4122
Epoch [ 147/2600] -> Loss: 5333.8418
Epoch [ 148/2600] -> Loss: 5294.9168
Epoch [ 149/2600] -> Loss: 5352.4208
Epoch [ 150/2600] -> Loss: 5319.2515
Epoch [ 151/2600] -> Loss: 5302.1886
Epoch [ 152/2600] -> Loss: 5302.2618
Epoch [ 153/2600] -> Loss: 5303.1495
Epoch [ 154/2600] -> Loss: 5293.3351
Epoch [ 155/2600] -> Loss: 5317.1003
Epoch [ 156/2600] -> Loss: 5279.0756
Epoch [ 157/2600] -> Loss: 5284.0792
Epoch [ 158/2600] -> Loss: 5263.7988
Epoch [ 159/2600] -> Loss: 5303.7178
Epoch [ 160/2600] -> Loss: 5301.6233
Epoch [ 161/2600] -> Loss: 5329.2820
Epoch [ 162/2600] -> Loss: 5309.3813
Epoch [ 163/2600] -> Loss: 5296.1759
Epoch [ 164/2600] -> Loss: 5300.5695
Epoch [ 165/2600] -> Loss: 5355.4345
Epoch [ 166/2600] -> Loss: 5312.3258
Epoch [ 167/2600] -> Loss: 5306.0914
Epoch [ 168/2600] -> Loss: 5337.6567
Epoch   169: reducing learning rate of group 0 to 3.1250e-05.
Epoch [ 169/2600] -> Loss: 5328.0205
Epoch [ 170/2600] -> Loss: 5318.9991
Epoch [ 171/2600] -> Loss: 5246.3989
Epoch [ 172/2600] -> Loss: 5281.6594
Epoch [ 173/2600] -> Loss: 5296.8476
Epoch [ 174/2600] -> Loss: 5330.9841
Epoch [ 175/2600] -> Loss: 5310.5453
Epoch [ 176/2600] -> Loss: 5299.2142
Epoch [ 177/2600] -> Loss: 5304.2524
Epoch [ 178/2600] -> Loss: 5312.5549
Epoch [ 179/2600] -> Loss: 5333.0537
