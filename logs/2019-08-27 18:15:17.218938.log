--------------------------------------------------
Code running on device: cuda
[(array([0.       , 1.       , 0.5      , 0.8660254, 0.       , 0.       ]), 96.7), (array([0.       , 1.       , 0.8660254, 0.5      , 0.       , 0.       ]), 104.3), (array([0.000000e+00, 1.000000e+00, 1.000000e+00, 6.123234e-17,
       0.000000e+00, 0.000000e+00]), 116.7), (array([ 0.       ,  1.       ,  0.8660254, -0.5      ,  0.       ,
        0.       ]), 92.8), (array([ 0.       ,  1.       ,  0.5      , -0.8660254,  0.       ,
        0.       ]), 141.7), (array([ 0.0000000e+00,  1.0000000e+00,  1.2246468e-16, -1.0000000e+00,
        0.0000000e+00,  0.0000000e+00]), 139.2), (array([ 0.       ,  1.       , -0.5      , -0.8660254,  0.       ,
        0.       ]), 158.0), (array([ 0.       ,  1.       , -0.8660254, -0.5      ,  0.       ,
        0.       ]), 110.5), (array([ 0.0000000e+00,  1.0000000e+00, -1.0000000e+00, -1.8369702e-16,
        0.0000000e+00,  0.0000000e+00]), 126.5), (array([ 0.       ,  1.       , -0.8660254,  0.5      ,  0.       ,
        0.       ]), 125.8)]
--------------------------------------------------
Dataset source : SILSO, ISGI
File location :
    SSN - /home/extern/Documents/Research/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/data/ISGI/aa_1869-08-01_2017-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
Pre-trained model available, loading model weights
--------------------------------------------------
Training model with: num_epochs=450, start_lr=0.0005
Epoch [   1/450] -> Loss: 877.0640
Epoch [   2/450] -> Loss: 874.9028
Epoch [   3/450] -> Loss: 869.5863
Epoch [   4/450] -> Loss: 869.0880
Epoch [   5/450] -> Loss: 873.0091
Epoch [   6/450] -> Loss: 876.2466
Epoch [   7/450] -> Loss: 870.2167
Epoch [   8/450] -> Loss: 867.2226
Epoch [   9/450] -> Loss: 874.2861
Epoch [  10/450] -> Loss: 869.0576
Epoch [  11/450] -> Loss: 871.1623
Epoch [  12/450] -> Loss: 872.9687
Epoch [  13/450] -> Loss: 875.3394
Epoch [  14/450] -> Loss: 870.6256
Epoch [  15/450] -> Loss: 871.4908
Epoch [  16/450] -> Loss: 871.4281
Epoch [  17/450] -> Loss: 868.6290
Epoch [  18/450] -> Loss: 869.2209
Epoch    18: reducing learning rate of group 0 to 2.5000e-04.
Epoch [  19/450] -> Loss: 869.1176
Epoch [  20/450] -> Loss: 867.5797
Epoch [  21/450] -> Loss: 866.5702
Epoch [  22/450] -> Loss: 866.4414
Epoch [  23/450] -> Loss: 867.0179
Epoch [  24/450] -> Loss: 867.2147
Epoch [  25/450] -> Loss: 868.6092
Epoch [  26/450] -> Loss: 867.7746
Epoch [  27/450] -> Loss: 867.5225
Epoch [  28/450] -> Loss: 867.8441
Epoch [  29/450] -> Loss: 865.2006
Epoch [  30/450] -> Loss: 868.1084
Epoch [  31/450] -> Loss: 867.2608
Epoch [  32/450] -> Loss: 867.1779
Epoch [  33/450] -> Loss: 865.1567
Epoch [  34/450] -> Loss: 868.1754
Epoch [  35/450] -> Loss: 864.9742
Epoch [  36/450] -> Loss: 866.4902
Epoch [  37/450] -> Loss: 865.0408
Epoch [  38/450] -> Loss: 866.0750
Epoch [  39/450] -> Loss: 866.8546
Epoch [  40/450] -> Loss: 865.4100
Epoch [  41/450] -> Loss: 866.2231
Epoch [  42/450] -> Loss: 865.4414
Epoch [  43/450] -> Loss: 865.5339
Epoch [  44/450] -> Loss: 866.1015
Epoch [  45/450] -> Loss: 865.9791
Epoch [  46/450] -> Loss: 864.0664
Epoch [  47/450] -> Loss: 867.4519
Epoch [  48/450] -> Loss: 861.7172
Epoch [  49/450] -> Loss: 869.9231
--------------------------------------------------
Model checkpoint saved as FFNN_50.pth
--------------------------------------------------
Epoch [  50/450] -> Loss: 863.9638
Epoch [  51/450] -> Loss: 867.2482
Epoch [  52/450] -> Loss: 866.8904
Epoch [  53/450] -> Loss: 864.6652
Epoch [  54/450] -> Loss: 866.1770
Epoch [  55/450] -> Loss: 866.6214
Epoch [  56/450] -> Loss: 863.4892
Epoch [  57/450] -> Loss: 868.5094
Epoch [  58/450] -> Loss: 865.4642
Epoch    58: reducing learning rate of group 0 to 1.2500e-04.
Epoch [  59/450] -> Loss: 866.8524
Epoch [  60/450] -> Loss: 863.5567
Epoch [  61/450] -> Loss: 864.8834
Epoch [  62/450] -> Loss: 864.4082
Epoch [  63/450] -> Loss: 864.3237
Epoch [  64/450] -> Loss: 863.6018
Epoch [  65/450] -> Loss: 865.2473
Epoch [  66/450] -> Loss: 864.4241
Epoch [  67/450] -> Loss: 863.8736
Epoch [  68/450] -> Loss: 864.7333
Epoch [  69/450] -> Loss: 864.2303
Epoch    69: reducing learning rate of group 0 to 6.2500e-05.
Epoch [  70/450] -> Loss: 863.4658
Epoch [  71/450] -> Loss: 864.0297
Epoch [  72/450] -> Loss: 863.4583
Epoch [  73/450] -> Loss: 863.6421
Epoch [  74/450] -> Loss: 863.5425
Epoch [  75/450] -> Loss: 863.4145
Epoch [  76/450] -> Loss: 863.3322
Epoch [  77/450] -> Loss: 863.1256
