Usage: python3 SSN_predictor.py <path_to_ssn_datafile> <path_to_aa_datafile>
----------------------------------------------------------------
Code running on device: cuda
----------------------------------------------------------------
Data loaded from file locations :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
----------------------------------------------------------------
Solar cycle data loaded/saved as: cycle_data.pickle
----------------------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=3, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=3, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
----------------------------------------------------------------
Pre-trained model available, loading model weights
----------------------------------------------------------------
Training model with: num_epochs=1200, start_lr=0.0001
Epoch [   1/1200] -> Loss: 53.5831
Epoch [   2/1200] -> Loss: 53.7268
Epoch [   3/1200] -> Loss: 53.6116
Epoch [   4/1200] -> Loss: 53.2982
Epoch [   5/1200] -> Loss: 53.0671
Epoch [   6/1200] -> Loss: 53.4691
Epoch [   7/1200] -> Loss: 53.3370
Epoch [   8/1200] -> Loss: 53.3056
Epoch [   9/1200] -> Loss: 53.4104
Epoch [  10/1200] -> Loss: 53.2586
Epoch [  11/1200] -> Loss: 53.1616
Epoch [  12/1200] -> Loss: 53.3741
Epoch [  13/1200] -> Loss: 53.4120
Epoch [  14/1200] -> Loss: 53.2112
Epoch [  15/1200] -> Loss: 53.7936
Epoch    16: reducing learning rate of group 0 to 5.0000e-05.
Epoch [  16/1200] -> Loss: 53.2906
Epoch [  17/1200] -> Loss: 53.6222
Epoch [  18/1200] -> Loss: 53.4317
Epoch [  19/1200] -> Loss: 53.3800
Epoch [  20/1200] -> Loss: 53.7078
Epoch [  21/1200] -> Loss: 53.4537
Epoch [  22/1200] -> Loss: 53.2473
Epoch [  23/1200] -> Loss: 53.3658
Epoch [  24/1200] -> Loss: 53.3093
Epoch [  25/1200] -> Loss: 53.7930
Epoch [  26/1200] -> Loss: 53.1666
Epoch    27: reducing learning rate of group 0 to 2.5000e-05.
Epoch [  27/1200] -> Loss: 53.3484
Epoch [  28/1200] -> Loss: 53.0988
Epoch [  29/1200] -> Loss: 53.3301
Epoch [  30/1200] -> Loss: 53.3607
Epoch [  31/1200] -> Loss: 53.3214
Epoch [  32/1200] -> Loss: 53.2439
Epoch [  33/1200] -> Loss: 53.5657
Epoch [  34/1200] -> Loss: 53.3413
Epoch [  35/1200] -> Loss: 53.1867
Epoch [  36/1200] -> Loss: 53.0239
Epoch [  37/1200] -> Loss: 53.1265
Epoch [  38/1200] -> Loss: 53.3609
Epoch [  39/1200] -> Loss: 53.2048
Epoch [  40/1200] -> Loss: 53.4429
Epoch [  41/1200] -> Loss: 53.2586
Epoch [  42/1200] -> Loss: 53.2681
Epoch [  43/1200] -> Loss: 52.9649
Epoch [  44/1200] -> Loss: 53.2763
Epoch [  45/1200] -> Loss: 53.3261
Epoch [  46/1200] -> Loss: 53.2810
Epoch [  47/1200] -> Loss: 52.8763
Epoch [  48/1200] -> Loss: 53.2713
Epoch [  49/1200] -> Loss: 53.2271
Epoch [  50/1200] -> Loss: 53.2416
Epoch [  51/1200] -> Loss: 53.3477
Epoch [  52/1200] -> Loss: 53.2999
Epoch [  53/1200] -> Loss: 53.2628
Epoch [  54/1200] -> Loss: 53.3569
Epoch [  55/1200] -> Loss: 53.1262
Epoch [  56/1200] -> Loss: 52.7962
Epoch [  57/1200] -> Loss: 53.0480
Epoch [  58/1200] -> Loss: 53.0435
Epoch [  59/1200] -> Loss: 53.2449
Epoch [  60/1200] -> Loss: 53.0269
Epoch [  61/1200] -> Loss: 53.3756
Epoch [  62/1200] -> Loss: 53.4149
Epoch [  63/1200] -> Loss: 53.4791
Epoch [  64/1200] -> Loss: 53.1600
Epoch [  65/1200] -> Loss: 53.2266
Epoch [  66/1200] -> Loss: 53.0116
Epoch    67: reducing learning rate of group 0 to 1.2500e-05.
Epoch [  67/1200] -> Loss: 53.2013
Epoch [  68/1200] -> Loss: 53.1645
Epoch [  69/1200] -> Loss: 53.2506
Epoch [  70/1200] -> Loss: 53.0693
