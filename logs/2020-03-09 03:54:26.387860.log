--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
File location :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
No pre-trained models available, initializing model weights
--------------------------------------------------
Training model with: num_epochs=1800, start_lr=0.0005
Epoch [   1/1800] -> Loss: 13266.1665
Epoch [   2/1800] -> Loss: 11883.2030
Epoch [   3/1800] -> Loss: 9104.4216
Epoch [   4/1800] -> Loss: 5746.2191
Epoch [   5/1800] -> Loss: 3721.5146
Epoch [   6/1800] -> Loss: 3160.2667
Epoch [   7/1800] -> Loss: 3081.2303
Epoch [   8/1800] -> Loss: 3069.5419
Epoch [   9/1800] -> Loss: 3070.9732
Epoch [  10/1800] -> Loss: 3068.7902
Epoch [  11/1800] -> Loss: 3061.9501
Epoch [  12/1800] -> Loss: 3053.5813
Epoch [  13/1800] -> Loss: 3043.1101
Epoch [  14/1800] -> Loss: 3040.2730
Epoch [  15/1800] -> Loss: 3033.9315
Epoch [  16/1800] -> Loss: 3028.1648
Epoch [  17/1800] -> Loss: 3022.2011
Epoch [  18/1800] -> Loss: 3011.8929
Epoch [  19/1800] -> Loss: 3014.6463
Epoch [  20/1800] -> Loss: 3006.7430
Epoch [  21/1800] -> Loss: 3011.7860
Epoch [  22/1800] -> Loss: 2988.0111
Epoch [  23/1800] -> Loss: 2982.7377
Epoch [  24/1800] -> Loss: 2982.0363
Epoch [  25/1800] -> Loss: 2968.3182
Epoch [  26/1800] -> Loss: 2964.1733
Epoch [  27/1800] -> Loss: 2956.4075
Epoch [  28/1800] -> Loss: 2943.0874
Epoch [  29/1800] -> Loss: 2939.0700
Epoch [  30/1800] -> Loss: 2930.3808
Epoch [  31/1800] -> Loss: 2915.3420
Epoch [  32/1800] -> Loss: 2916.0229
Epoch [  33/1800] -> Loss: 2900.3455
Epoch [  34/1800] -> Loss: 2890.5106
Epoch [  35/1800] -> Loss: 2882.3558
Epoch [  36/1800] -> Loss: 2867.1444
Epoch [  37/1800] -> Loss: 2850.9163
Epoch [  38/1800] -> Loss: 2849.0259
Epoch [  39/1800] -> Loss: 2818.8145
Epoch [  40/1800] -> Loss: 2825.1444
Epoch [  41/1800] -> Loss: 2796.5621
Epoch [  42/1800] -> Loss: 2781.2728
Epoch [  43/1800] -> Loss: 2764.4282
Epoch [  44/1800] -> Loss: 2750.2752
Epoch [  45/1800] -> Loss: 2731.6591
Epoch [  46/1800] -> Loss: 2714.5259
Epoch [  47/1800] -> Loss: 2701.3420
Epoch [  48/1800] -> Loss: 2680.3820
Epoch [  49/1800] -> Loss: 2666.7745
Epoch [  50/1800] -> Loss: 2649.0006
Epoch [  51/1800] -> Loss: 2628.4921
Epoch [  52/1800] -> Loss: 2614.4289
Epoch [  53/1800] -> Loss: 2589.6158
Epoch [  54/1800] -> Loss: 2575.5763
Epoch [  55/1800] -> Loss: 2551.1603
Epoch [  56/1800] -> Loss: 2534.9479
Epoch [  57/1800] -> Loss: 2510.6813
Epoch [  58/1800] -> Loss: 2501.5936
Epoch [  59/1800] -> Loss: 2486.2651
Epoch [  60/1800] -> Loss: 2456.0735
Epoch [  61/1800] -> Loss: 2435.6485
Epoch [  62/1800] -> Loss: 2419.7409
Epoch [  63/1800] -> Loss: 2399.6889
Epoch [  64/1800] -> Loss: 2376.5178
Epoch [  65/1800] -> Loss: 2362.9675
Epoch [  66/1800] -> Loss: 2349.5102
Epoch [  67/1800] -> Loss: 2321.0072
Epoch [  68/1800] -> Loss: 2300.1852
Epoch [  69/1800] -> Loss: 2291.8165
Epoch [  70/1800] -> Loss: 2268.8680
Epoch [  71/1800] -> Loss: 2248.9798
Epoch [  72/1800] -> Loss: 2232.5075
Epoch [  73/1800] -> Loss: 2212.7494
Epoch [  74/1800] -> Loss: 2188.5911
Epoch [  75/1800] -> Loss: 2183.6399
Epoch [  76/1800] -> Loss: 2161.2841
Epoch [  77/1800] -> Loss: 2153.7079
Epoch [  78/1800] -> Loss: 2131.9818
Epoch [  79/1800] -> Loss: 2117.8694
Epoch [  80/1800] -> Loss: 2105.4125
Epoch [  81/1800] -> Loss: 2085.1084
Epoch [  82/1800] -> Loss: 2075.3846
Epoch [  83/1800] -> Loss: 2062.6761
Epoch [  84/1800] -> Loss: 2045.6302
Epoch [  85/1800] -> Loss: 2046.9526
Epoch [  86/1800] -> Loss: 2017.4761
Epoch [  87/1800] -> Loss: 2005.7615
Epoch [  88/1800] -> Loss: 1996.1630
Epoch [  89/1800] -> Loss: 1983.5421
Epoch [  90/1800] -> Loss: 1984.4403
Epoch [  91/1800] -> Loss: 1952.4245
Epoch [  92/1800] -> Loss: 1950.6358
Epoch [  93/1800] -> Loss: 1934.9456
Epoch [  94/1800] -> Loss: 1926.1633
Epoch [  95/1800] -> Loss: 1919.5224
Epoch [  96/1800] -> Loss: 1906.8397
Epoch [  97/1800] -> Loss: 1897.9953
Epoch [  98/1800] -> Loss: 1884.0289
Epoch [  99/1800] -> Loss: 1878.9740
--------------------------------------------------
Model checkpoint saved as FFNN_100.pth
--------------------------------------------------
Epoch [ 100/1800] -> Loss: 1878.4609
Epoch [ 101/1800] -> Loss: 1865.4253
Epoch [ 102/1800] -> Loss: 1845.3849
Epoch [ 103/1800] -> Loss: 1846.4027
Epoch [ 104/1800] -> Loss: 1838.7293
Epoch [ 105/1800] -> Loss: 1823.1100
Epoch [ 106/1800] -> Loss: 1825.1052
Epoch [ 107/1800] -> Loss: 1810.7621
Epoch [ 108/1800] -> Loss: 1808.2836
Epoch [ 109/1800] -> Loss: 1797.1158
Epoch [ 110/1800] -> Loss: 1786.4905
Epoch [ 111/1800] -> Loss: 1784.1732
Epoch [ 112/1800] -> Loss: 1776.2433
Epoch [ 113/1800] -> Loss: 1775.0578
Epoch [ 114/1800] -> Loss: 1764.1689
Epoch [ 115/1800] -> Loss: 1753.6185
Epoch [ 116/1800] -> Loss: 1752.4122
Epoch [ 117/1800] -> Loss: 1740.6963
Epoch [ 118/1800] -> Loss: 1735.8146
Epoch [ 119/1800] -> Loss: 1730.2708
Epoch [ 120/1800] -> Loss: 1736.3765
Epoch [ 121/1800] -> Loss: 1728.5479
Epoch [ 122/1800] -> Loss: 1706.2395
Epoch [ 123/1800] -> Loss: 1708.1250
Epoch [ 124/1800] -> Loss: 1701.7927
Epoch [ 125/1800] -> Loss: 1708.9759
Epoch [ 126/1800] -> Loss: 1691.6657
Epoch [ 127/1800] -> Loss: 1686.5329
Epoch [ 128/1800] -> Loss: 1682.2032
Epoch [ 129/1800] -> Loss: 1677.1331
Epoch [ 130/1800] -> Loss: 1677.6906
Epoch [ 131/1800] -> Loss: 1668.2240
Epoch [ 132/1800] -> Loss: 1672.1785
Epoch [ 133/1800] -> Loss: 1660.5448
Epoch [ 134/1800] -> Loss: 1654.2538
Epoch [ 135/1800] -> Loss: 1653.1560
Epoch [ 136/1800] -> Loss: 1650.0778
Epoch [ 137/1800] -> Loss: 1641.6347
Epoch [ 138/1800] -> Loss: 1637.0534
Epoch [ 139/1800] -> Loss: 1636.1363
Epoch [ 140/1800] -> Loss: 1633.6562
Epoch [ 141/1800] -> Loss: 1628.9227
Epoch [ 142/1800] -> Loss: 1631.6601
Epoch [ 143/1800] -> Loss: 1620.9746
Epoch [ 144/1800] -> Loss: 1617.6209
Epoch [ 145/1800] -> Loss: 1616.8795
Epoch [ 146/1800] -> Loss: 1616.7998
Epoch [ 147/1800] -> Loss: 1613.6111
Epoch [ 148/1800] -> Loss: 1605.9123
Epoch [ 149/1800] -> Loss: 1612.4548
Epoch [ 150/1800] -> Loss: 1602.1050
Epoch [ 151/1800] -> Loss: 1606.5376
Epoch [ 152/1800] -> Loss: 1598.8488
Epoch [ 153/1800] -> Loss: 1598.1434
Epoch [ 154/1800] -> Loss: 1604.6350
Epoch [ 155/1800] -> Loss: 1590.8169
Epoch [ 156/1800] -> Loss: 1599.2831
Epoch [ 157/1800] -> Loss: 1591.5629
Epoch [ 158/1800] -> Loss: 1588.3421
Epoch [ 159/1800] -> Loss: 1600.4751
Epoch [ 160/1800] -> Loss: 1585.7670
Epoch [ 161/1800] -> Loss: 1589.3721
Epoch [ 162/1800] -> Loss: 1582.5731
Epoch [ 163/1800] -> Loss: 1583.3012
Epoch [ 164/1800] -> Loss: 1586.5868
Epoch [ 165/1800] -> Loss: 1585.4468
Epoch [ 166/1800] -> Loss: 1573.0900
Epoch [ 167/1800] -> Loss: 1584.9353
Epoch [ 168/1800] -> Loss: 1575.6317
Epoch [ 169/1800] -> Loss: 1572.5669
Epoch [ 170/1800] -> Loss: 1573.6100
Epoch [ 171/1800] -> Loss: 1564.5479
Epoch [ 172/1800] -> Loss: 1572.6556
Epoch [ 173/1800] -> Loss: 1567.2974
Epoch [ 174/1800] -> Loss: 1577.2968
Epoch [ 175/1800] -> Loss: 1572.6288
Epoch [ 176/1800] -> Loss: 1572.2595
Epoch [ 177/1800] -> Loss: 1569.4402
Epoch [ 178/1800] -> Loss: 1570.8886
Epoch [ 179/1800] -> Loss: 1570.6510
Epoch [ 180/1800] -> Loss: 1567.5908
Epoch [ 181/1800] -> Loss: 1561.4500
Epoch [ 182/1800] -> Loss: 1563.9557
Epoch [ 183/1800] -> Loss: 1558.8204
Epoch [ 184/1800] -> Loss: 1558.1383
Epoch [ 185/1800] -> Loss: 1565.2280
Epoch [ 186/1800] -> Loss: 1564.5423
Epoch [ 187/1800] -> Loss: 1558.6464
Epoch [ 188/1800] -> Loss: 1574.2299
Epoch [ 189/1800] -> Loss: 1560.4340
Epoch [ 190/1800] -> Loss: 1553.3142
Epoch [ 191/1800] -> Loss: 1569.7859
Epoch [ 192/1800] -> Loss: 1565.7774
Epoch [ 193/1800] -> Loss: 1555.7108
Epoch [ 194/1800] -> Loss: 1558.4014
Epoch [ 195/1800] -> Loss: 1553.8991
Epoch [ 196/1800] -> Loss: 1562.2206
Epoch [ 197/1800] -> Loss: 1552.4497
Epoch [ 198/1800] -> Loss: 1564.6989
Epoch [ 199/1800] -> Loss: 1552.5754
--------------------------------------------------
Model checkpoint saved as FFNN_200.pth
--------------------------------------------------
Epoch [ 200/1800] -> Loss: 1553.6487
Epoch [ 201/1800] -> Loss: 1559.3693
Epoch [ 202/1800] -> Loss: 1557.9065
Epoch [ 203/1800] -> Loss: 1551.7347
Epoch [ 204/1800] -> Loss: 1559.1447
Epoch [ 205/1800] -> Loss: 1547.9009
Epoch [ 206/1800] -> Loss: 1551.1807
Epoch [ 207/1800] -> Loss: 1554.0583
Epoch [ 208/1800] -> Loss: 1545.6864
Epoch [ 209/1800] -> Loss: 1550.7837
Epoch [ 210/1800] -> Loss: 1549.3383
Epoch [ 211/1800] -> Loss: 1549.1621
Epoch [ 212/1800] -> Loss: 1559.8894
Epoch [ 213/1800] -> Loss: 1552.4068
Epoch [ 214/1800] -> Loss: 1552.1862
Epoch [ 215/1800] -> Loss: 1548.5525
Epoch [ 216/1800] -> Loss: 1542.8304
Epoch [ 217/1800] -> Loss: 1545.4722
Epoch [ 218/1800] -> Loss: 1548.4622
Epoch [ 219/1800] -> Loss: 1543.7520
Epoch [ 220/1800] -> Loss: 1537.7430
Epoch [ 221/1800] -> Loss: 1534.2775
Epoch [ 222/1800] -> Loss: 1548.0382
Epoch [ 223/1800] -> Loss: 1541.0876
Epoch [ 224/1800] -> Loss: 1546.9920
Epoch [ 225/1800] -> Loss: 1542.5716
Epoch [ 226/1800] -> Loss: 1545.0271
Epoch [ 227/1800] -> Loss: 1542.7778
Epoch [ 228/1800] -> Loss: 1545.6168
Epoch [ 229/1800] -> Loss: 1551.6730
Epoch [ 230/1800] -> Loss: 1534.1640
Epoch [ 231/1800] -> Loss: 1540.1301
Epoch   232: reducing learning rate of group 0 to 2.5000e-04.
Epoch [ 232/1800] -> Loss: 1538.8354
Epoch [ 233/1800] -> Loss: 1532.1380
Epoch [ 234/1800] -> Loss: 1538.0516
Epoch [ 235/1800] -> Loss: 1538.8749
Epoch [ 236/1800] -> Loss: 1533.7355
Epoch [ 237/1800] -> Loss: 1538.3191
Epoch [ 238/1800] -> Loss: 1533.5856
Epoch [ 239/1800] -> Loss: 1538.7191
Epoch [ 240/1800] -> Loss: 1545.1928
Epoch [ 241/1800] -> Loss: 1537.3915
Epoch [ 242/1800] -> Loss: 1537.2475
Epoch [ 243/1800] -> Loss: 1535.8823
Epoch   244: reducing learning rate of group 0 to 1.2500e-04.
Epoch [ 244/1800] -> Loss: 1537.4520
Epoch [ 245/1800] -> Loss: 1531.5118
Epoch [ 246/1800] -> Loss: 1531.1140
Epoch [ 247/1800] -> Loss: 1527.9589
Epoch [ 248/1800] -> Loss: 1534.6854
Epoch [ 249/1800] -> Loss: 1530.7660
Epoch [ 250/1800] -> Loss: 1533.2646
Epoch [ 251/1800] -> Loss: 1537.0388
Epoch [ 252/1800] -> Loss: 1532.3810
Epoch [ 253/1800] -> Loss: 1532.1885
Epoch [ 254/1800] -> Loss: 1532.1351
Epoch [ 255/1800] -> Loss: 1532.7626
Epoch [ 256/1800] -> Loss: 1531.6566
Epoch [ 257/1800] -> Loss: 1531.7365
Epoch   258: reducing learning rate of group 0 to 6.2500e-05.
Epoch [ 258/1800] -> Loss: 1530.8143
Epoch [ 259/1800] -> Loss: 1535.6678
Epoch [ 260/1800] -> Loss: 1527.5369
Epoch [ 261/1800] -> Loss: 1528.3603
Epoch [ 262/1800] -> Loss: 1527.8687
Epoch [ 263/1800] -> Loss: 1526.7630
Epoch [ 264/1800] -> Loss: 1531.2037
Epoch [ 265/1800] -> Loss: 1529.5869
Epoch [ 266/1800] -> Loss: 1527.9216
Epoch [ 267/1800] -> Loss: 1526.7385
Epoch [ 268/1800] -> Loss: 1527.2108
Epoch [ 269/1800] -> Loss: 1531.2721
Epoch [ 270/1800] -> Loss: 1526.7952
Epoch [ 271/1800] -> Loss: 1527.9116
Epoch [ 272/1800] -> Loss: 1531.8414
Epoch [ 273/1800] -> Loss: 1532.5457
Epoch   274: reducing learning rate of group 0 to 3.1250e-05.
Epoch [ 274/1800] -> Loss: 1532.0532
Epoch [ 275/1800] -> Loss: 1527.6500
Epoch [ 276/1800] -> Loss: 1528.2199
Epoch [ 277/1800] -> Loss: 1531.1309
Epoch [ 278/1800] -> Loss: 1528.4185
Epoch [ 279/1800] -> Loss: 1530.5197
Epoch [ 280/1800] -> Loss: 1527.3667
Epoch [ 281/1800] -> Loss: 1526.2783
Epoch [ 282/1800] -> Loss: 1530.5898
Epoch [ 283/1800] -> Loss: 1531.1443
Epoch [ 284/1800] -> Loss: 1529.1464
Epoch [ 285/1800] -> Loss: 1528.6640
Epoch [ 286/1800] -> Loss: 1528.6878
Epoch [ 287/1800] -> Loss: 1532.4334
Epoch [ 288/1800] -> Loss: 1528.0684
Epoch [ 289/1800] -> Loss: 1526.9737
Epoch [ 290/1800] -> Loss: 1528.1539
Epoch [ 291/1800] -> Loss: 1528.8435
Epoch [ 292/1800] -> Loss: 1526.0742
Epoch [ 293/1800] -> Loss: 1527.6300
Epoch [ 294/1800] -> Loss: 1527.2575
Epoch [ 295/1800] -> Loss: 1528.0090
Epoch [ 296/1800] -> Loss: 1524.9179
Epoch [ 297/1800] -> Loss: 1527.2188
Epoch [ 298/1800] -> Loss: 1526.4357
Epoch [ 299/1800] -> Loss: 1526.7186
--------------------------------------------------
Model checkpoint saved as FFNN_300.pth
--------------------------------------------------
Epoch [ 300/1800] -> Loss: 1526.6201
Epoch [ 301/1800] -> Loss: 1527.3929
Epoch [ 302/1800] -> Loss: 1527.9360
Epoch [ 303/1800] -> Loss: 1525.5218
Epoch [ 304/1800] -> Loss: 1529.4286
Epoch [ 305/1800] -> Loss: 1530.8894
Epoch [ 306/1800] -> Loss: 1525.4930
Epoch   307: reducing learning rate of group 0 to 1.5625e-05.
Epoch [ 307/1800] -> Loss: 1532.6601
Epoch [ 308/1800] -> Loss: 1525.2626
Epoch [ 309/1800] -> Loss: 1526.1900
Epoch [ 310/1800] -> Loss: 1524.8726
Epoch [ 311/1800] -> Loss: 1527.6907
Epoch [ 312/1800] -> Loss: 1526.2571
Epoch [ 313/1800] -> Loss: 1526.3225
Epoch [ 314/1800] -> Loss: 1524.8090
Epoch [ 315/1800] -> Loss: 1525.2671
Epoch [ 316/1800] -> Loss: 1526.1593
Epoch [ 317/1800] -> Loss: 1529.0530
Epoch   318: reducing learning rate of group 0 to 7.8125e-06.
Epoch [ 318/1800] -> Loss: 1526.9946
Epoch [ 319/1800] -> Loss: 1525.6497
Epoch [ 320/1800] -> Loss: 1525.2387
Epoch [ 321/1800] -> Loss: 1527.8923
Epoch [ 322/1800] -> Loss: 1527.9406
Epoch [ 323/1800] -> Loss: 1526.3298
Epoch [ 324/1800] -> Loss: 1525.7245
Epoch [ 325/1800] -> Loss: 1524.3234
Epoch [ 326/1800] -> Loss: 1531.6496
Epoch [ 327/1800] -> Loss: 1531.5780
Epoch [ 328/1800] -> Loss: 1525.5298
Epoch [ 329/1800] -> Loss: 1525.6583
Epoch [ 330/1800] -> Loss: 1527.4761
Epoch [ 331/1800] -> Loss: 1525.2314
Epoch [ 332/1800] -> Loss: 1526.3950
Epoch [ 333/1800] -> Loss: 1528.7888
Epoch [ 334/1800] -> Loss: 1527.6043
Epoch [ 335/1800] -> Loss: 1536.3253
Epoch   336: reducing learning rate of group 0 to 3.9063e-06.
Epoch [ 336/1800] -> Loss: 1527.3727
Epoch [ 337/1800] -> Loss: 1524.5637
Epoch [ 338/1800] -> Loss: 1526.4909
Epoch [ 339/1800] -> Loss: 1526.6141
Epoch [ 340/1800] -> Loss: 1526.3877
Epoch [ 341/1800] -> Loss: 1525.4171
Epoch [ 342/1800] -> Loss: 1529.4266
Epoch [ 343/1800] -> Loss: 1528.1754
Epoch [ 344/1800] -> Loss: 1524.2367
Epoch [ 345/1800] -> Loss: 1527.2540
Epoch [ 346/1800] -> Loss: 1528.7002
Epoch   347: reducing learning rate of group 0 to 1.9531e-06.
Epoch [ 347/1800] -> Loss: 1527.4756
Epoch [ 348/1800] -> Loss: 1530.9944
Epoch [ 349/1800] -> Loss: 1524.6441
Epoch [ 350/1800] -> Loss: 1526.3474
Epoch [ 351/1800] -> Loss: 1528.6005
Epoch [ 352/1800] -> Loss: 1524.8182
Epoch [ 353/1800] -> Loss: 1525.2407
Epoch [ 354/1800] -> Loss: 1527.2574
Epoch [ 355/1800] -> Loss: 1524.8584
Epoch [ 356/1800] -> Loss: 1527.5178
Epoch [ 357/1800] -> Loss: 1526.2225
Epoch   358: reducing learning rate of group 0 to 9.7656e-07.
Epoch [ 358/1800] -> Loss: 1524.8374
Epoch [ 359/1800] -> Loss: 1526.0879
Epoch [ 360/1800] -> Loss: 1526.2252
Epoch [ 361/1800] -> Loss: 1528.5135
Epoch [ 362/1800] -> Loss: 1524.8664
Epoch [ 363/1800] -> Loss: 1525.7859
Epoch [ 364/1800] -> Loss: 1525.9166
Epoch [ 365/1800] -> Loss: 1525.2407
Epoch [ 366/1800] -> Loss: 1525.8822
Epoch [ 367/1800] -> Loss: 1526.3094
Epoch [ 368/1800] -> Loss: 1527.3453
Epoch   369: reducing learning rate of group 0 to 4.8828e-07.
Epoch [ 369/1800] -> Loss: 1525.0137
Epoch [ 370/1800] -> Loss: 1524.0778
Epoch [ 371/1800] -> Loss: 1529.4003
Epoch [ 372/1800] -> Loss: 1525.7294
Epoch [ 373/1800] -> Loss: 1527.0744
Epoch [ 374/1800] -> Loss: 1524.7667
Epoch [ 375/1800] -> Loss: 1526.6186
Epoch [ 376/1800] -> Loss: 1524.6329
Epoch [ 377/1800] -> Loss: 1526.7036
Epoch [ 378/1800] -> Loss: 1527.8416
Epoch [ 379/1800] -> Loss: 1524.7644
Epoch [ 380/1800] -> Loss: 1525.7682
Epoch   381: reducing learning rate of group 0 to 2.4414e-07.
Epoch [ 381/1800] -> Loss: 1525.6962
Epoch [ 382/1800] -> Loss: 1526.4492
Epoch [ 383/1800] -> Loss: 1526.1940
Epoch [ 384/1800] -> Loss: 1525.5964
Epoch [ 385/1800] -> Loss: 1526.1909
Epoch [ 386/1800] -> Loss: 1524.9543
Epoch [ 387/1800] -> Loss: 1525.2696
Epoch [ 388/1800] -> Loss: 1527.3551
Epoch [ 389/1800] -> Loss: 1526.2004
Epoch [ 390/1800] -> Loss: 1528.2867
Epoch [ 391/1800] -> Loss: 1526.3822
Epoch   392: reducing learning rate of group 0 to 1.2207e-07.
Epoch [ 392/1800] -> Loss: 1527.1408
Epoch [ 393/1800] -> Loss: 1528.7297
Epoch [ 394/1800] -> Loss: 1525.2660
Epoch [ 395/1800] -> Loss: 1528.7031
Epoch [ 396/1800] -> Loss: 1524.7277
Epoch [ 397/1800] -> Loss: 1527.0841
Epoch [ 398/1800] -> Loss: 1536.5890
Epoch [ 399/1800] -> Loss: 1528.2314
--------------------------------------------------
Model checkpoint saved as FFNN_400.pth
--------------------------------------------------
Epoch [ 400/1800] -> Loss: 1525.8909
Epoch [ 401/1800] -> Loss: 1527.1515
Epoch [ 402/1800] -> Loss: 1527.3742
Epoch   403: reducing learning rate of group 0 to 6.1035e-08.
Epoch [ 403/1800] -> Loss: 1525.8436
Epoch [ 404/1800] -> Loss: 1527.8402
Epoch [ 405/1800] -> Loss: 1526.8433
Epoch [ 406/1800] -> Loss: 1527.8453
Epoch [ 407/1800] -> Loss: 1529.9583
Epoch [ 408/1800] -> Loss: 1525.0573
Epoch [ 409/1800] -> Loss: 1525.1376
Epoch [ 410/1800] -> Loss: 1525.9725
Epoch [ 411/1800] -> Loss: 1525.4249
Epoch [ 412/1800] -> Loss: 1526.9404
Epoch [ 413/1800] -> Loss: 1526.6514
Epoch   414: reducing learning rate of group 0 to 3.0518e-08.
Epoch [ 414/1800] -> Loss: 1524.8894
Epoch [ 415/1800] -> Loss: 1526.1382
Epoch [ 416/1800] -> Loss: 1527.0396
Epoch [ 417/1800] -> Loss: 1526.3905
Epoch [ 418/1800] -> Loss: 1524.4508
Epoch [ 419/1800] -> Loss: 1525.5774
Epoch [ 420/1800] -> Loss: 1524.1814
Epoch [ 421/1800] -> Loss: 1524.5532
Epoch [ 422/1800] -> Loss: 1524.9874
Epoch [ 423/1800] -> Loss: 1526.5284
Epoch [ 424/1800] -> Loss: 1525.7643
Epoch   425: reducing learning rate of group 0 to 1.5259e-08.
Epoch [ 425/1800] -> Loss: 1525.8992
Epoch [ 426/1800] -> Loss: 1526.7706
Epoch [ 427/1800] -> Loss: 1526.5815
Epoch [ 428/1800] -> Loss: 1524.6012
Epoch [ 429/1800] -> Loss: 1526.6275
Epoch [ 430/1800] -> Loss: 1525.4366
Epoch [ 431/1800] -> Loss: 1524.5740
Epoch [ 432/1800] -> Loss: 1525.6455
Epoch [ 433/1800] -> Loss: 1525.5702
Epoch [ 434/1800] -> Loss: 1525.3800
Epoch [ 435/1800] -> Loss: 1525.0815
Epoch [ 436/1800] -> Loss: 1525.7686
Epoch [ 437/1800] -> Loss: 1525.5388
Epoch [ 438/1800] -> Loss: 1525.8516
Epoch [ 439/1800] -> Loss: 1526.1877
Epoch [ 440/1800] -> Loss: 1526.3959
Epoch [ 441/1800] -> Loss: 1528.7417
Epoch [ 442/1800] -> Loss: 1527.4008
Epoch [ 443/1800] -> Loss: 1525.7224
Epoch [ 444/1800] -> Loss: 1529.1208
Epoch [ 445/1800] -> Loss: 1525.8397
Epoch [ 446/1800] -> Loss: 1525.6021
Epoch [ 447/1800] -> Loss: 1529.6191
Epoch [ 448/1800] -> Loss: 1526.1780
Epoch [ 449/1800] -> Loss: 1524.4707
Epoch [ 450/1800] -> Loss: 1528.7633
Epoch [ 451/1800] -> Loss: 1533.1908
Epoch [ 452/1800] -> Loss: 1526.7973
Epoch [ 453/1800] -> Loss: 1526.0160
Epoch [ 454/1800] -> Loss: 1529.8137
Epoch [ 455/1800] -> Loss: 1524.3616
Epoch [ 456/1800] -> Loss: 1528.4009
Epoch [ 457/1800] -> Loss: 1526.0238
Epoch [ 458/1800] -> Loss: 1526.1969
Epoch [ 459/1800] -> Loss: 1528.3662
Epoch [ 460/1800] -> Loss: 1524.5087
Epoch [ 461/1800] -> Loss: 1526.4133
Epoch [ 462/1800] -> Loss: 1524.4197
Epoch [ 463/1800] -> Loss: 1530.6108
Epoch [ 464/1800] -> Loss: 1524.3488
Epoch [ 465/1800] -> Loss: 1529.5622
Epoch [ 466/1800] -> Loss: 1526.4272
Epoch [ 467/1800] -> Loss: 1524.0584
Epoch [ 468/1800] -> Loss: 1525.0683
Epoch [ 469/1800] -> Loss: 1526.6082
Epoch [ 470/1800] -> Loss: 1525.4567
Epoch [ 471/1800] -> Loss: 1526.7279
Epoch [ 472/1800] -> Loss: 1527.4297
Epoch [ 473/1800] -> Loss: 1526.0758
Epoch [ 474/1800] -> Loss: 1525.8260
Epoch [ 475/1800] -> Loss: 1526.2936
Epoch [ 476/1800] -> Loss: 1527.1449
Epoch [ 477/1800] -> Loss: 1523.9318
Epoch [ 478/1800] -> Loss: 1528.7467
Epoch [ 479/1800] -> Loss: 1524.5900
Epoch [ 480/1800] -> Loss: 1525.1043
Epoch [ 481/1800] -> Loss: 1525.6894
Epoch [ 482/1800] -> Loss: 1526.2131
Epoch [ 483/1800] -> Loss: 1525.9514
Epoch [ 484/1800] -> Loss: 1526.3580
Epoch [ 485/1800] -> Loss: 1525.4400
Epoch [ 486/1800] -> Loss: 1526.9832
Epoch [ 487/1800] -> Loss: 1527.1301
Epoch [ 488/1800] -> Loss: 1526.0367
Epoch [ 489/1800] -> Loss: 1524.9065
Epoch [ 490/1800] -> Loss: 1523.6999
Epoch [ 491/1800] -> Loss: 1526.2763
Epoch [ 492/1800] -> Loss: 1526.3261
Epoch [ 493/1800] -> Loss: 1525.6865
Epoch [ 494/1800] -> Loss: 1524.4440
Epoch [ 495/1800] -> Loss: 1524.8248
Epoch [ 496/1800] -> Loss: 1527.5289
Epoch [ 497/1800] -> Loss: 1527.1218
Epoch [ 498/1800] -> Loss: 1524.1383
Epoch [ 499/1800] -> Loss: 1525.2832
--------------------------------------------------
Model checkpoint saved as FFNN_500.pth
--------------------------------------------------
Epoch [ 500/1800] -> Loss: 1525.2010
Epoch [ 501/1800] -> Loss: 1524.8202
Epoch [ 502/1800] -> Loss: 1525.0839
Epoch [ 503/1800] -> Loss: 1525.0331
Epoch [ 504/1800] -> Loss: 1527.9678
Epoch [ 505/1800] -> Loss: 1527.4409
Epoch [ 506/1800] -> Loss: 1529.4947
Epoch [ 507/1800] -> Loss: 1530.6952
Epoch [ 508/1800] -> Loss: 1525.1822
Epoch [ 509/1800] -> Loss: 1524.8063
Epoch [ 510/1800] -> Loss: 1529.6891
Epoch [ 511/1800] -> Loss: 1532.7108
Epoch [ 512/1800] -> Loss: 1524.9570
Epoch [ 513/1800] -> Loss: 1525.8790
Epoch [ 514/1800] -> Loss: 1525.0900
Epoch [ 515/1800] -> Loss: 1524.6529
Epoch [ 516/1800] -> Loss: 1525.0233
Epoch [ 517/1800] -> Loss: 1525.2412
Epoch [ 518/1800] -> Loss: 1531.1648
Epoch [ 519/1800] -> Loss: 1524.9742
Epoch [ 520/1800] -> Loss: 1525.2877
Epoch [ 521/1800] -> Loss: 1525.5051
Epoch [ 522/1800] -> Loss: 1526.9986
Epoch [ 523/1800] -> Loss: 1524.8730
Epoch [ 524/1800] -> Loss: 1526.7342
Epoch [ 525/1800] -> Loss: 1529.1264
Epoch [ 526/1800] -> Loss: 1532.0633
Epoch [ 527/1800] -> Loss: 1533.4727
Epoch [ 528/1800] -> Loss: 1529.2962
Epoch [ 529/1800] -> Loss: 1525.2695
Epoch [ 530/1800] -> Loss: 1528.0536
Epoch [ 531/1800] -> Loss: 1525.9626
Epoch [ 532/1800] -> Loss: 1525.8414
Epoch [ 533/1800] -> Loss: 1526.2711
Epoch [ 534/1800] -> Loss: 1527.7099
Epoch [ 535/1800] -> Loss: 1524.2619
Epoch [ 536/1800] -> Loss: 1525.7484
Epoch [ 537/1800] -> Loss: 1525.7410
Epoch [ 538/1800] -> Loss: 1525.2220
Epoch [ 539/1800] -> Loss: 1524.1278
Epoch [ 540/1800] -> Loss: 1525.7119
Epoch [ 541/1800] -> Loss: 1523.8126
Epoch [ 542/1800] -> Loss: 1529.9420
Epoch [ 543/1800] -> Loss: 1526.1894
Epoch [ 544/1800] -> Loss: 1528.8006
Epoch [ 545/1800] -> Loss: 1524.2280
Epoch [ 546/1800] -> Loss: 1526.2275
Epoch [ 547/1800] -> Loss: 1526.4423
Epoch [ 548/1800] -> Loss: 1524.7380
Epoch [ 549/1800] -> Loss: 1526.9825
Epoch [ 550/1800] -> Loss: 1525.9996
Epoch [ 551/1800] -> Loss: 1528.5300
Epoch [ 552/1800] -> Loss: 1523.6047
Epoch [ 553/1800] -> Loss: 1525.0034
Epoch [ 554/1800] -> Loss: 1527.6418
Epoch [ 555/1800] -> Loss: 1526.3770
Epoch [ 556/1800] -> Loss: 1526.3817
Epoch [ 557/1800] -> Loss: 1526.0147
Epoch [ 558/1800] -> Loss: 1527.9825
Epoch [ 559/1800] -> Loss: 1528.6992
Epoch [ 560/1800] -> Loss: 1524.1033
Epoch [ 561/1800] -> Loss: 1528.9415
Epoch [ 562/1800] -> Loss: 1525.9602
Epoch [ 563/1800] -> Loss: 1533.3264
Epoch [ 564/1800] -> Loss: 1525.0801
Epoch [ 565/1800] -> Loss: 1526.9742
Epoch [ 566/1800] -> Loss: 1524.6998
Epoch [ 567/1800] -> Loss: 1527.7511
Epoch [ 568/1800] -> Loss: 1528.0737
Epoch [ 569/1800] -> Loss: 1529.6560
Epoch [ 570/1800] -> Loss: 1525.9261
Epoch [ 571/1800] -> Loss: 1530.2607
Epoch [ 572/1800] -> Loss: 1523.5808
Epoch [ 573/1800] -> Loss: 1525.8743
Epoch [ 574/1800] -> Loss: 1525.4884
