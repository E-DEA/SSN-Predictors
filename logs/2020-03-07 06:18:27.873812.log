--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
File location :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
No pre-trained models available, initializing model weights
--------------------------------------------------
Training model with: num_epochs=1800, start_lr=0.0005
Epoch [   1/1800] -> Loss: 13266.1610
Epoch [   2/1800] -> Loss: 11883.2233
Epoch [   3/1800] -> Loss: 9104.4858
Epoch [   4/1800] -> Loss: 5746.3203
Epoch [   5/1800] -> Loss: 3721.6147
Epoch [   6/1800] -> Loss: 3160.3232
Epoch [   7/1800] -> Loss: 3081.2226
Epoch [   8/1800] -> Loss: 3069.4696
Epoch [   9/1800] -> Loss: 3070.8513
Epoch [  10/1800] -> Loss: 3068.6143
Epoch [  11/1800] -> Loss: 3061.7226
Epoch [  12/1800] -> Loss: 3053.3045
Epoch [  13/1800] -> Loss: 3042.7728
Epoch [  14/1800] -> Loss: 3039.8941
Epoch [  15/1800] -> Loss: 3033.5003
Epoch [  16/1800] -> Loss: 3027.6738
Epoch [  17/1800] -> Loss: 3021.6536
Epoch [  18/1800] -> Loss: 3011.2899
Epoch [  19/1800] -> Loss: 3013.9688
Epoch [  20/1800] -> Loss: 3005.9993
Epoch [  21/1800] -> Loss: 3010.9422
Epoch [  22/1800] -> Loss: 2987.0960
Epoch [  23/1800] -> Loss: 2981.7134
Epoch [  24/1800] -> Loss: 2980.9089
Epoch [  25/1800] -> Loss: 2967.0559
Epoch [  26/1800] -> Loss: 2962.7754
Epoch [  27/1800] -> Loss: 2954.8873
Epoch [  28/1800] -> Loss: 2941.4447
Epoch [  29/1800] -> Loss: 2937.1997
Epoch [  30/1800] -> Loss: 2928.3261
Epoch [  31/1800] -> Loss: 2913.0827
Epoch [  32/1800] -> Loss: 2913.5777
Epoch [  33/1800] -> Loss: 2897.6884
Epoch [  34/1800] -> Loss: 2887.6063
Epoch [  35/1800] -> Loss: 2879.2162
Epoch [  36/1800] -> Loss: 2863.7637
Epoch [  37/1800] -> Loss: 2847.2805
Epoch [  38/1800] -> Loss: 2845.1001
Epoch [  39/1800] -> Loss: 2814.6140
Epoch [  40/1800] -> Loss: 2820.7162
Epoch [  41/1800] -> Loss: 2791.7555
Epoch [  42/1800] -> Loss: 2776.2002
Epoch [  43/1800] -> Loss: 2759.0307
Epoch [  44/1800] -> Loss: 2744.5224
Epoch [  45/1800] -> Loss: 2725.5917
Epoch [  46/1800] -> Loss: 2708.1359
Epoch [  47/1800] -> Loss: 2694.5902
Epoch [  48/1800] -> Loss: 2673.3851
Epoch [  49/1800] -> Loss: 2659.3371
Epoch [  50/1800] -> Loss: 2641.2559
Epoch [  51/1800] -> Loss: 2620.4435
Epoch [  52/1800] -> Loss: 2605.9718
Epoch [  53/1800] -> Loss: 2580.7965
Epoch [  54/1800] -> Loss: 2566.4911
Epoch [  55/1800] -> Loss: 2541.7757
Epoch [  56/1800] -> Loss: 2525.2009
Epoch [  57/1800] -> Loss: 2500.6233
Epoch [  58/1800] -> Loss: 2491.2891
Epoch [  59/1800] -> Loss: 2475.5605
Epoch [  60/1800] -> Loss: 2445.0981
Epoch [  61/1800] -> Loss: 2424.3411
Epoch [  62/1800] -> Loss: 2408.1068
Epoch [  63/1800] -> Loss: 2387.7880
Epoch [  64/1800] -> Loss: 2364.1164
Epoch [  65/1800] -> Loss: 2350.6049
Epoch [  66/1800] -> Loss: 2336.9037
Epoch [  67/1800] -> Loss: 2307.9812
Epoch [  68/1800] -> Loss: 2286.7881
Epoch [  69/1800] -> Loss: 2278.6286
Epoch [  70/1800] -> Loss: 2255.2853
Epoch [  71/1800] -> Loss: 2235.2606
Epoch [  72/1800] -> Loss: 2218.6592
Epoch [  73/1800] -> Loss: 2198.7497
Epoch [  74/1800] -> Loss: 2174.5700
Epoch [  75/1800] -> Loss: 2169.6390
Epoch [  76/1800] -> Loss: 2147.1862
Epoch [  77/1800] -> Loss: 2139.1520
Epoch [  78/1800] -> Loss: 2117.9597
Epoch [  79/1800] -> Loss: 2103.9284
Epoch [  80/1800] -> Loss: 2091.5352
Epoch [  81/1800] -> Loss: 2071.0476
Epoch [  82/1800] -> Loss: 2061.4960
Epoch [  83/1800] -> Loss: 2048.8166
Epoch [  84/1800] -> Loss: 2032.0241
Epoch [  85/1800] -> Loss: 2033.3393
Epoch [  86/1800] -> Loss: 2003.9359
Epoch [  87/1800] -> Loss: 1992.3437
Epoch [  88/1800] -> Loss: 1982.9706
Epoch [  89/1800] -> Loss: 1970.5758
Epoch [  90/1800] -> Loss: 1971.4598
Epoch [  91/1800] -> Loss: 1939.8860
Epoch [  92/1800] -> Loss: 1938.2935
Epoch [  93/1800] -> Loss: 1922.8994
Epoch [  94/1800] -> Loss: 1914.3113
Epoch [  95/1800] -> Loss: 1908.0065
Epoch [  96/1800] -> Loss: 1895.6488
Epoch [  97/1800] -> Loss: 1887.1355
Epoch [  98/1800] -> Loss: 1873.4029
Epoch [  99/1800] -> Loss: 1868.6854
--------------------------------------------------
Model checkpoint saved as FFNN_100.pth
--------------------------------------------------
Epoch [ 100/1800] -> Loss: 1868.1586
Epoch [ 101/1800] -> Loss: 1855.3594
Epoch [ 102/1800] -> Loss: 1835.7805
Epoch [ 103/1800] -> Loss: 1836.6419
Epoch [ 104/1800] -> Loss: 1829.2840
Epoch [ 105/1800] -> Loss: 1813.6955
Epoch [ 106/1800] -> Loss: 1815.7474
Epoch [ 107/1800] -> Loss: 1801.4061
Epoch [ 108/1800] -> Loss: 1798.8647
Epoch [ 109/1800] -> Loss: 1787.7031
Epoch [ 110/1800] -> Loss: 1777.1600
Epoch [ 111/1800] -> Loss: 1774.6123
Epoch [ 112/1800] -> Loss: 1767.0721
Epoch [ 113/1800] -> Loss: 1765.6413
Epoch [ 114/1800] -> Loss: 1754.6215
Epoch [ 115/1800] -> Loss: 1744.0355
Epoch [ 116/1800] -> Loss: 1742.9432
Epoch [ 117/1800] -> Loss: 1730.8370
Epoch [ 118/1800] -> Loss: 1726.1320
Epoch [ 119/1800] -> Loss: 1720.5814
Epoch [ 120/1800] -> Loss: 1726.3721
Epoch [ 121/1800] -> Loss: 1718.9941
Epoch [ 122/1800] -> Loss: 1695.3320
Epoch [ 123/1800] -> Loss: 1698.0997
Epoch [ 124/1800] -> Loss: 1691.9444
Epoch [ 125/1800] -> Loss: 1699.0419
Epoch [ 126/1800] -> Loss: 1681.4136
Epoch [ 127/1800] -> Loss: 1676.1673
Epoch [ 128/1800] -> Loss: 1671.8078
Epoch [ 129/1800] -> Loss: 1666.8635
Epoch [ 130/1800] -> Loss: 1667.2847
Epoch [ 131/1800] -> Loss: 1657.6292
Epoch [ 132/1800] -> Loss: 1661.5254
Epoch [ 133/1800] -> Loss: 1649.9385
Epoch [ 134/1800] -> Loss: 1643.5799
Epoch [ 135/1800] -> Loss: 1642.4243
Epoch [ 136/1800] -> Loss: 1639.4002
Epoch [ 137/1800] -> Loss: 1630.9322
Epoch [ 138/1800] -> Loss: 1626.3523
Epoch [ 139/1800] -> Loss: 1625.2362
Epoch [ 140/1800] -> Loss: 1622.8219
Epoch [ 141/1800] -> Loss: 1618.0415
Epoch [ 142/1800] -> Loss: 1620.6573
Epoch [ 143/1800] -> Loss: 1610.0200
Epoch [ 144/1800] -> Loss: 1606.6424
Epoch [ 145/1800] -> Loss: 1605.6450
Epoch [ 146/1800] -> Loss: 1605.6477
Epoch [ 147/1800] -> Loss: 1602.3557
Epoch [ 148/1800] -> Loss: 1594.5306
Epoch [ 149/1800] -> Loss: 1601.0955
Epoch [ 150/1800] -> Loss: 1590.5315
Epoch [ 151/1800] -> Loss: 1594.4612
Epoch [ 152/1800] -> Loss: 1587.2242
Epoch [ 153/1800] -> Loss: 1586.4062
Epoch [ 154/1800] -> Loss: 1593.1960
Epoch [ 155/1800] -> Loss: 1578.9200
Epoch [ 156/1800] -> Loss: 1587.6651
Epoch [ 157/1800] -> Loss: 1579.8940
Epoch [ 158/1800] -> Loss: 1576.5498
Epoch [ 159/1800] -> Loss: 1588.9728
Epoch [ 160/1800] -> Loss: 1574.0847
Epoch [ 161/1800] -> Loss: 1577.8971
Epoch [ 162/1800] -> Loss: 1571.0705
Epoch [ 163/1800] -> Loss: 1571.7426
Epoch [ 164/1800] -> Loss: 1574.4046
Epoch [ 165/1800] -> Loss: 1574.6452
Epoch [ 166/1800] -> Loss: 1561.8038
Epoch [ 167/1800] -> Loss: 1573.8616
Epoch [ 168/1800] -> Loss: 1564.3950
Epoch [ 169/1800] -> Loss: 1561.3432
Epoch [ 170/1800] -> Loss: 1562.3748
Epoch [ 171/1800] -> Loss: 1553.2471
Epoch [ 172/1800] -> Loss: 1561.3450
Epoch [ 173/1800] -> Loss: 1556.2896
Epoch [ 174/1800] -> Loss: 1566.4920
Epoch [ 175/1800] -> Loss: 1561.6510
Epoch [ 176/1800] -> Loss: 1561.0251
Epoch [ 177/1800] -> Loss: 1558.1208
Epoch [ 178/1800] -> Loss: 1559.7482
Epoch [ 179/1800] -> Loss: 1559.3854
Epoch [ 180/1800] -> Loss: 1556.5368
Epoch [ 181/1800] -> Loss: 1550.2670
Epoch [ 182/1800] -> Loss: 1552.3838
Epoch [ 183/1800] -> Loss: 1547.7653
Epoch [ 184/1800] -> Loss: 1546.7943
Epoch [ 185/1800] -> Loss: 1554.3238
Epoch [ 186/1800] -> Loss: 1553.5919
Epoch [ 187/1800] -> Loss: 1547.9492
Epoch [ 188/1800] -> Loss: 1562.5870
Epoch [ 189/1800] -> Loss: 1548.9135
Epoch [ 190/1800] -> Loss: 1542.2352
Epoch [ 191/1800] -> Loss: 1558.6035
Epoch [ 192/1800] -> Loss: 1554.8156
Epoch [ 193/1800] -> Loss: 1544.6045
Epoch [ 194/1800] -> Loss: 1547.4377
Epoch [ 195/1800] -> Loss: 1542.8764
Epoch [ 196/1800] -> Loss: 1550.6276
Epoch [ 197/1800] -> Loss: 1541.4117
Epoch [ 198/1800] -> Loss: 1553.6069
Epoch [ 199/1800] -> Loss: 1540.7539
--------------------------------------------------
Model checkpoint saved as FFNN_200.pth
--------------------------------------------------
Epoch [ 200/1800] -> Loss: 1542.5639
Epoch [ 201/1800] -> Loss: 1548.3754
Epoch [ 202/1800] -> Loss: 1546.9663
Epoch [ 203/1800] -> Loss: 1540.3885
Epoch [ 204/1800] -> Loss: 1548.2807
Epoch [ 205/1800] -> Loss: 1536.9458
Epoch [ 206/1800] -> Loss: 1539.3902
Epoch [ 207/1800] -> Loss: 1542.3832
Epoch [ 208/1800] -> Loss: 1534.8411
Epoch [ 209/1800] -> Loss: 1538.4879
Epoch [ 210/1800] -> Loss: 1536.8756
Epoch [ 211/1800] -> Loss: 1536.6914
Epoch [ 212/1800] -> Loss: 1547.6022
Epoch [ 213/1800] -> Loss: 1540.3731
Epoch [ 214/1800] -> Loss: 1532.7073
Epoch [ 215/1800] -> Loss: 1538.0561
Epoch [ 216/1800] -> Loss: 1530.3326
Epoch [ 217/1800] -> Loss: 1534.8898
Epoch [ 218/1800] -> Loss: 1537.6960
Epoch [ 219/1800] -> Loss: 1533.0967
Epoch [ 220/1800] -> Loss: 1526.5235
Epoch [ 221/1800] -> Loss: 1523.4708
Epoch [ 222/1800] -> Loss: 1536.9063
Epoch [ 223/1800] -> Loss: 1529.9267
Epoch [ 224/1800] -> Loss: 1535.9176
Epoch [ 225/1800] -> Loss: 1531.9617
Epoch [ 226/1800] -> Loss: 1534.1469
Epoch [ 227/1800] -> Loss: 1531.7956
Epoch [ 228/1800] -> Loss: 1534.6378
Epoch [ 229/1800] -> Loss: 1541.5282
Epoch [ 230/1800] -> Loss: 1523.2139
Epoch [ 231/1800] -> Loss: 1528.8309
Epoch [ 232/1800] -> Loss: 1527.5548
Epoch [ 233/1800] -> Loss: 1524.4576
Epoch [ 234/1800] -> Loss: 1532.4286
Epoch [ 235/1800] -> Loss: 1534.2602
Epoch [ 236/1800] -> Loss: 1527.7258
Epoch [ 237/1800] -> Loss: 1527.7456
Epoch [ 238/1800] -> Loss: 1526.1917
Epoch [ 239/1800] -> Loss: 1533.7115
Epoch [ 240/1800] -> Loss: 1533.9476
Epoch   241: reducing learning rate of group 0 to 2.5000e-04.
Epoch [ 241/1800] -> Loss: 1535.5496
Epoch [ 242/1800] -> Loss: 1525.2618
Epoch [ 243/1800] -> Loss: 1523.5221
Epoch [ 244/1800] -> Loss: 1525.2978
Epoch [ 245/1800] -> Loss: 1521.0967
Epoch [ 246/1800] -> Loss: 1521.5839
Epoch [ 247/1800] -> Loss: 1515.2986
Epoch [ 248/1800] -> Loss: 1526.7304
Epoch [ 249/1800] -> Loss: 1520.8429
Epoch [ 250/1800] -> Loss: 1524.4921
Epoch [ 251/1800] -> Loss: 1522.5546
Epoch [ 252/1800] -> Loss: 1523.9018
Epoch [ 253/1800] -> Loss: 1521.8861
Epoch [ 254/1800] -> Loss: 1521.9203
Epoch [ 255/1800] -> Loss: 1525.1588
Epoch [ 256/1800] -> Loss: 1522.2038
Epoch [ 257/1800] -> Loss: 1523.5847
Epoch   258: reducing learning rate of group 0 to 1.2500e-04.
Epoch [ 258/1800] -> Loss: 1519.6661
Epoch [ 259/1800] -> Loss: 1523.9021
Epoch [ 260/1800] -> Loss: 1515.4874
Epoch [ 261/1800] -> Loss: 1516.5483
Epoch [ 262/1800] -> Loss: 1515.7794
Epoch [ 263/1800] -> Loss: 1513.2653
Epoch [ 264/1800] -> Loss: 1519.9475
Epoch [ 265/1800] -> Loss: 1518.2156
Epoch [ 266/1800] -> Loss: 1515.7645
Epoch [ 267/1800] -> Loss: 1514.2879
Epoch [ 268/1800] -> Loss: 1514.2389
Epoch [ 269/1800] -> Loss: 1520.1207
Epoch [ 270/1800] -> Loss: 1514.8392
Epoch [ 271/1800] -> Loss: 1516.6510
Epoch [ 272/1800] -> Loss: 1518.5455
Epoch [ 273/1800] -> Loss: 1520.5104
Epoch   274: reducing learning rate of group 0 to 6.2500e-05.
Epoch [ 274/1800] -> Loss: 1519.9828
Epoch [ 275/1800] -> Loss: 1514.7477
Epoch [ 276/1800] -> Loss: 1515.6706
Epoch [ 277/1800] -> Loss: 1517.9702
Epoch [ 278/1800] -> Loss: 1515.3089
Epoch [ 279/1800] -> Loss: 1517.9443
Epoch [ 280/1800] -> Loss: 1514.3544
Epoch [ 281/1800] -> Loss: 1513.5641
Epoch [ 282/1800] -> Loss: 1519.4933
Epoch [ 283/1800] -> Loss: 1518.0453
Epoch [ 284/1800] -> Loss: 1516.6574
Epoch   285: reducing learning rate of group 0 to 3.1250e-05.
Epoch [ 285/1800] -> Loss: 1516.2724
Epoch [ 286/1800] -> Loss: 1514.8576
Epoch [ 287/1800] -> Loss: 1518.5546
Epoch [ 288/1800] -> Loss: 1514.3750
Epoch [ 289/1800] -> Loss: 1513.2704
Epoch [ 290/1800] -> Loss: 1514.3211
Epoch [ 291/1800] -> Loss: 1515.1324
Epoch [ 292/1800] -> Loss: 1512.3726
Epoch [ 293/1800] -> Loss: 1513.9267
Epoch [ 294/1800] -> Loss: 1513.5054
Epoch [ 295/1800] -> Loss: 1514.2920
Epoch [ 296/1800] -> Loss: 1511.2038
Epoch [ 297/1800] -> Loss: 1513.4173
Epoch [ 298/1800] -> Loss: 1512.7662
Epoch [ 299/1800] -> Loss: 1512.9747
--------------------------------------------------
Model checkpoint saved as FFNN_300.pth
--------------------------------------------------
Epoch [ 300/1800] -> Loss: 1512.8948
Epoch [ 301/1800] -> Loss: 1513.5861
Epoch [ 302/1800] -> Loss: 1513.6471
Epoch [ 303/1800] -> Loss: 1511.7080
Epoch [ 304/1800] -> Loss: 1515.7160
Epoch [ 305/1800] -> Loss: 1517.1879
Epoch [ 306/1800] -> Loss: 1511.8237
Epoch   307: reducing learning rate of group 0 to 1.5625e-05.
Epoch [ 307/1800] -> Loss: 1518.9436
Epoch [ 308/1800] -> Loss: 1511.5312
Epoch [ 309/1800] -> Loss: 1512.4048
Epoch [ 310/1800] -> Loss: 1511.1286
Epoch [ 311/1800] -> Loss: 1513.7874
Epoch [ 312/1800] -> Loss: 1512.5223
Epoch [ 313/1800] -> Loss: 1512.6080
Epoch [ 314/1800] -> Loss: 1511.1508
Epoch [ 315/1800] -> Loss: 1511.6186
Epoch [ 316/1800] -> Loss: 1512.4423
Epoch [ 317/1800] -> Loss: 1515.3192
Epoch   318: reducing learning rate of group 0 to 7.8125e-06.
Epoch [ 318/1800] -> Loss: 1513.2646
Epoch [ 319/1800] -> Loss: 1511.8015
Epoch [ 320/1800] -> Loss: 1511.6170
Epoch [ 321/1800] -> Loss: 1514.1574
Epoch [ 322/1800] -> Loss: 1514.2040
Epoch [ 323/1800] -> Loss: 1512.6017
Epoch [ 324/1800] -> Loss: 1511.9871
Epoch [ 325/1800] -> Loss: 1510.5896
Epoch [ 326/1800] -> Loss: 1517.9228
Epoch [ 327/1800] -> Loss: 1517.8486
Epoch [ 328/1800] -> Loss: 1511.8087
Epoch [ 329/1800] -> Loss: 1511.8739
Epoch [ 330/1800] -> Loss: 1513.7467
Epoch [ 331/1800] -> Loss: 1511.5092
Epoch [ 332/1800] -> Loss: 1512.6874
Epoch [ 333/1800] -> Loss: 1515.0617
Epoch [ 334/1800] -> Loss: 1513.8715
Epoch [ 335/1800] -> Loss: 1522.6007
Epoch   336: reducing learning rate of group 0 to 3.9063e-06.
Epoch [ 336/1800] -> Loss: 1513.6185
Epoch [ 337/1800] -> Loss: 1510.9785
Epoch [ 338/1800] -> Loss: 1512.8234
Epoch [ 339/1800] -> Loss: 1512.8889
Epoch [ 340/1800] -> Loss: 1512.5242
Epoch [ 341/1800] -> Loss: 1511.7402
Epoch [ 342/1800] -> Loss: 1515.7285
Epoch [ 343/1800] -> Loss: 1514.4351
Epoch [ 344/1800] -> Loss: 1510.0245
Epoch [ 345/1800] -> Loss: 1513.5717
Epoch [ 346/1800] -> Loss: 1514.9753
Epoch [ 347/1800] -> Loss: 1513.7558
Epoch [ 348/1800] -> Loss: 1517.3180
Epoch [ 349/1800] -> Loss: 1510.9718
Epoch [ 350/1800] -> Loss: 1512.6479
Epoch [ 351/1800] -> Loss: 1514.9393
Epoch [ 352/1800] -> Loss: 1511.1352
Epoch [ 353/1800] -> Loss: 1511.6019
Epoch [ 354/1800] -> Loss: 1513.6234
Epoch   355: reducing learning rate of group 0 to 1.9531e-06.
Epoch [ 355/1800] -> Loss: 1511.1911
Epoch [ 356/1800] -> Loss: 1513.7937
Epoch [ 357/1800] -> Loss: 1512.5279
Epoch [ 358/1800] -> Loss: 1511.1080
Epoch [ 359/1800] -> Loss: 1512.4143
Epoch [ 360/1800] -> Loss: 1512.5628
Epoch [ 361/1800] -> Loss: 1514.7862
Epoch [ 362/1800] -> Loss: 1511.1644
Epoch [ 363/1800] -> Loss: 1512.0224
Epoch [ 364/1800] -> Loss: 1512.2391
Epoch [ 365/1800] -> Loss: 1511.5388
Epoch   366: reducing learning rate of group 0 to 9.7656e-07.
Epoch [ 366/1800] -> Loss: 1512.1791
Epoch [ 367/1800] -> Loss: 1512.5193
Epoch [ 368/1800] -> Loss: 1513.6599
Epoch [ 369/1800] -> Loss: 1511.2697
Epoch [ 370/1800] -> Loss: 1510.3485
Epoch [ 371/1800] -> Loss: 1515.6587
Epoch [ 372/1800] -> Loss: 1512.0247
Epoch [ 373/1800] -> Loss: 1513.3466
Epoch [ 374/1800] -> Loss: 1510.9578
Epoch [ 375/1800] -> Loss: 1512.9514
Epoch [ 376/1800] -> Loss: 1510.8925
Epoch   377: reducing learning rate of group 0 to 4.8828e-07.
Epoch [ 377/1800] -> Loss: 1512.9832
Epoch [ 378/1800] -> Loss: 1514.1010
Epoch [ 379/1800] -> Loss: 1511.0358
Epoch [ 380/1800] -> Loss: 1511.9890
Epoch [ 381/1800] -> Loss: 1511.9719
Epoch [ 382/1800] -> Loss: 1512.7479
Epoch [ 383/1800] -> Loss: 1512.4402
Epoch [ 384/1800] -> Loss: 1511.8625
Epoch [ 385/1800] -> Loss: 1512.4680
Epoch [ 386/1800] -> Loss: 1511.2805
Epoch [ 387/1800] -> Loss: 1511.5481
Epoch   388: reducing learning rate of group 0 to 2.4414e-07.
Epoch [ 388/1800] -> Loss: 1513.6548
Epoch [ 389/1800] -> Loss: 1512.5148
Epoch [ 390/1800] -> Loss: 1514.5679
Epoch [ 391/1800] -> Loss: 1512.6542
Epoch [ 392/1800] -> Loss: 1513.4201
Epoch [ 393/1800] -> Loss: 1514.8681
Epoch [ 394/1800] -> Loss: 1511.5246
Epoch [ 395/1800] -> Loss: 1515.0065
Epoch [ 396/1800] -> Loss: 1511.1693
Epoch [ 397/1800] -> Loss: 1513.3484
Epoch [ 398/1800] -> Loss: 1522.7569
Epoch   399: reducing learning rate of group 0 to 1.2207e-07.
Epoch [ 399/1800] -> Loss: 1514.5039
--------------------------------------------------
Model checkpoint saved as FFNN_400.pth
--------------------------------------------------
Epoch [ 400/1800] -> Loss: 1512.1540
Epoch [ 401/1800] -> Loss: 1513.4088
Epoch [ 402/1800] -> Loss: 1513.6581
Epoch [ 403/1800] -> Loss: 1512.0954
Epoch [ 404/1800] -> Loss: 1514.1343
Epoch [ 405/1800] -> Loss: 1513.0318
Epoch [ 406/1800] -> Loss: 1514.1042
Epoch [ 407/1800] -> Loss: 1516.0567
Epoch [ 408/1800] -> Loss: 1511.3589
Epoch [ 409/1800] -> Loss: 1511.4011
Epoch   410: reducing learning rate of group 0 to 6.1035e-08.
Epoch [ 410/1800] -> Loss: 1512.2482
Epoch [ 411/1800] -> Loss: 1511.6868
Epoch [ 412/1800] -> Loss: 1513.1824
Epoch [ 413/1800] -> Loss: 1512.8922
Epoch [ 414/1800] -> Loss: 1511.0854
Epoch [ 415/1800] -> Loss: 1512.4169
Epoch [ 416/1800] -> Loss: 1513.3396
Epoch [ 417/1800] -> Loss: 1512.6457
Epoch [ 418/1800] -> Loss: 1510.7112
Epoch [ 419/1800] -> Loss: 1511.7955
Epoch [ 420/1800] -> Loss: 1510.4261
Epoch   421: reducing learning rate of group 0 to 3.0518e-08.
Epoch [ 421/1800] -> Loss: 1510.8246
Epoch [ 422/1800] -> Loss: 1511.2716
Epoch [ 423/1800] -> Loss: 1512.8027
Epoch [ 424/1800] -> Loss: 1512.0209
Epoch [ 425/1800] -> Loss: 1512.1551
Epoch [ 426/1800] -> Loss: 1513.0077
Epoch [ 427/1800] -> Loss: 1512.9481
Epoch [ 428/1800] -> Loss: 1510.8657
Epoch [ 429/1800] -> Loss: 1512.8858
Epoch [ 430/1800] -> Loss: 1511.7066
Epoch [ 431/1800] -> Loss: 1510.8519
Epoch   432: reducing learning rate of group 0 to 1.5259e-08.
Epoch [ 432/1800] -> Loss: 1511.9135
Epoch [ 433/1800] -> Loss: 1511.8344
Epoch [ 434/1800] -> Loss: 1511.6069
Epoch [ 435/1800] -> Loss: 1511.3485
Epoch [ 436/1800] -> Loss: 1512.0162
Epoch [ 437/1800] -> Loss: 1511.8001
Epoch [ 438/1800] -> Loss: 1512.1150
Epoch [ 439/1800] -> Loss: 1512.4735
Epoch [ 440/1800] -> Loss: 1512.7541
Epoch [ 441/1800] -> Loss: 1515.0028
