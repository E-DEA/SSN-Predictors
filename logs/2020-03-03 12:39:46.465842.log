--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
File location :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): Linear(in_features=6, out_features=6, bias=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
No pre-trained models available, initializing model weights
--------------------------------------------------
Training model with: num_epochs=2600, start_lr=0.0005
Epoch [   1/2600] -> Loss: 26441.0142
Epoch [   2/2600] -> Loss: 24570.7734
Epoch [   3/2600] -> Loss: 23067.7976
Epoch [   4/2600] -> Loss: 21463.0994
Epoch [   5/2600] -> Loss: 20065.0680
Epoch [   6/2600] -> Loss: 18746.0597
Epoch [   7/2600] -> Loss: 17428.0167
Epoch [   8/2600] -> Loss: 16266.1736
Epoch [   9/2600] -> Loss: 15111.5255
Epoch [  10/2600] -> Loss: 14145.6798
Epoch [  11/2600] -> Loss: 13094.8281
Epoch [  12/2600] -> Loss: 12130.4141
Epoch [  13/2600] -> Loss: 11388.7527
Epoch [  14/2600] -> Loss: 10495.7426
Epoch [  15/2600] -> Loss: 9821.4796
Epoch [  16/2600] -> Loss: 9121.2176
Epoch [  17/2600] -> Loss: 8593.2349
Epoch [  18/2600] -> Loss: 8135.7716
Epoch [  19/2600] -> Loss: 7717.7561
Epoch [  20/2600] -> Loss: 7330.1109
Epoch [  21/2600] -> Loss: 7003.7785
Epoch [  22/2600] -> Loss: 6666.5584
Epoch [  23/2600] -> Loss: 6326.8864
Epoch [  24/2600] -> Loss: 5975.6563
Epoch [  25/2600] -> Loss: 5732.9697
Epoch [  26/2600] -> Loss: 5449.4600
Epoch [  27/2600] -> Loss: 5172.0992
Epoch [  28/2600] -> Loss: 4946.2412
Epoch [  29/2600] -> Loss: 4708.0732
Epoch [  30/2600] -> Loss: 4491.1372
Epoch [  31/2600] -> Loss: 4295.2709
Epoch [  32/2600] -> Loss: 4150.2639
Epoch [  33/2600] -> Loss: 3981.3249
Epoch [  34/2600] -> Loss: 3854.5194
Epoch [  35/2600] -> Loss: 3754.1363
Epoch [  36/2600] -> Loss: 3597.6984
Epoch [  37/2600] -> Loss: 3519.1910
Epoch [  38/2600] -> Loss: 3424.3211
Epoch [  39/2600] -> Loss: 3359.4294
Epoch [  40/2600] -> Loss: 3302.8482
Epoch [  41/2600] -> Loss: 3232.8240
Epoch [  42/2600] -> Loss: 3189.9156
Epoch [  43/2600] -> Loss: 3150.4968
Epoch [  44/2600] -> Loss: 3121.5006
Epoch [  45/2600] -> Loss: 3081.3839
Epoch [  46/2600] -> Loss: 3057.4635
Epoch [  47/2600] -> Loss: 3049.2113
Epoch [  48/2600] -> Loss: 3033.0023
Epoch [  49/2600] -> Loss: 3028.2173
Epoch [  50/2600] -> Loss: 3014.7844
Epoch [  51/2600] -> Loss: 3017.5929
Epoch [  52/2600] -> Loss: 3006.5659
Epoch [  53/2600] -> Loss: 3017.7899
Epoch [  54/2600] -> Loss: 2984.3984
Epoch [  55/2600] -> Loss: 2995.5967
Epoch [  56/2600] -> Loss: 3002.5702
Epoch [  57/2600] -> Loss: 3000.1107
Epoch [  58/2600] -> Loss: 2989.2706
Epoch [  59/2600] -> Loss: 2976.9246
Epoch [  60/2600] -> Loss: 2989.3188
Epoch [  61/2600] -> Loss: 2979.9903
Epoch [  62/2600] -> Loss: 2979.3400
Epoch [  63/2600] -> Loss: 2989.3036
Epoch [  64/2600] -> Loss: 2975.3344
Epoch [  65/2600] -> Loss: 2987.8406
Epoch [  66/2600] -> Loss: 2983.8584
Epoch [  67/2600] -> Loss: 2984.6254
Epoch [  68/2600] -> Loss: 2981.7879
Epoch [  69/2600] -> Loss: 2973.2944
Epoch [  70/2600] -> Loss: 2975.0491
Epoch [  71/2600] -> Loss: 2987.3170
Epoch [  72/2600] -> Loss: 2983.5162
Epoch [  73/2600] -> Loss: 2980.7617
Epoch [  74/2600] -> Loss: 2960.2718
Epoch [  75/2600] -> Loss: 2974.8592
Epoch [  76/2600] -> Loss: 2972.6817
Epoch [  77/2600] -> Loss: 2978.4049
Epoch [  78/2600] -> Loss: 2966.0216
Epoch [  79/2600] -> Loss: 2957.9598
Epoch [  80/2600] -> Loss: 2962.1740
Epoch [  81/2600] -> Loss: 2966.6724
Epoch [  82/2600] -> Loss: 2966.2330
Epoch [  83/2600] -> Loss: 2982.1614
Epoch [  84/2600] -> Loss: 2958.6754
Epoch [  85/2600] -> Loss: 2966.7649
Epoch [  86/2600] -> Loss: 2952.9378
Epoch [  87/2600] -> Loss: 2966.9551
Epoch [  88/2600] -> Loss: 2982.5068
Epoch [  89/2600] -> Loss: 2968.6602
Epoch [  90/2600] -> Loss: 2950.0280
Epoch [  91/2600] -> Loss: 2968.4762
Epoch [  92/2600] -> Loss: 2946.5819
Epoch [  93/2600] -> Loss: 2958.5613
Epoch [  94/2600] -> Loss: 2975.3460
Epoch [  95/2600] -> Loss: 2969.5403
Epoch [  96/2600] -> Loss: 2950.3253
Epoch [  97/2600] -> Loss: 2960.4622
Epoch [  98/2600] -> Loss: 2959.4593
Epoch [  99/2600] -> Loss: 2965.3851
Epoch [ 100/2600] -> Loss: 2950.8240
Epoch [ 101/2600] -> Loss: 2943.3914
Epoch [ 102/2600] -> Loss: 2940.8827
Epoch [ 103/2600] -> Loss: 2944.4660
Epoch [ 104/2600] -> Loss: 2962.8497
Epoch [ 105/2600] -> Loss: 2952.8051
Epoch [ 106/2600] -> Loss: 2947.6182
Epoch [ 107/2600] -> Loss: 2974.2959
Epoch [ 108/2600] -> Loss: 2952.6620
Epoch [ 109/2600] -> Loss: 2955.3617
Epoch [ 110/2600] -> Loss: 2952.6967
Epoch [ 111/2600] -> Loss: 2966.3609
Epoch [ 112/2600] -> Loss: 2955.3349
Epoch   113: reducing learning rate of group 0 to 2.5000e-04.
Epoch [ 113/2600] -> Loss: 2954.1065
Epoch [ 114/2600] -> Loss: 2938.4894
Epoch [ 115/2600] -> Loss: 2938.1443
Epoch [ 116/2600] -> Loss: 2953.6199
Epoch [ 117/2600] -> Loss: 2946.7335
Epoch [ 118/2600] -> Loss: 2930.2627
Epoch [ 119/2600] -> Loss: 2947.9058
Epoch [ 120/2600] -> Loss: 2944.4966
Epoch [ 121/2600] -> Loss: 2940.9697
Epoch [ 122/2600] -> Loss: 2964.7955
Epoch [ 123/2600] -> Loss: 2949.7983
Epoch [ 124/2600] -> Loss: 2965.4693
Epoch [ 125/2600] -> Loss: 2964.5766
Epoch [ 126/2600] -> Loss: 2944.0507
Epoch [ 127/2600] -> Loss: 2943.8626
Epoch [ 128/2600] -> Loss: 2944.9699
Epoch   129: reducing learning rate of group 0 to 1.2500e-04.
Epoch [ 129/2600] -> Loss: 2951.8683
Epoch [ 130/2600] -> Loss: 2948.0456
Epoch [ 131/2600] -> Loss: 2937.2004
Epoch [ 132/2600] -> Loss: 2938.7833
Epoch [ 133/2600] -> Loss: 2930.0718
Epoch [ 134/2600] -> Loss: 2940.7498
Epoch [ 135/2600] -> Loss: 2932.4731
Epoch [ 136/2600] -> Loss: 2949.4700
Epoch [ 137/2600] -> Loss: 2926.9279
Epoch [ 138/2600] -> Loss: 2943.4735
Epoch [ 139/2600] -> Loss: 2946.6057
Epoch [ 140/2600] -> Loss: 2949.2304
Epoch [ 141/2600] -> Loss: 2934.6297
Epoch [ 142/2600] -> Loss: 2946.1729
Epoch [ 143/2600] -> Loss: 2945.3715
Epoch [ 144/2600] -> Loss: 2944.7022
Epoch [ 145/2600] -> Loss: 2941.7352
Epoch [ 146/2600] -> Loss: 2943.7282
Epoch [ 147/2600] -> Loss: 2950.1888
Epoch   148: reducing learning rate of group 0 to 6.2500e-05.
Epoch [ 148/2600] -> Loss: 2937.9405
Epoch [ 149/2600] -> Loss: 2941.5288
Epoch [ 150/2600] -> Loss: 2933.5766
Epoch [ 151/2600] -> Loss: 2934.8613
Epoch [ 152/2600] -> Loss: 2936.4164
Epoch [ 153/2600] -> Loss: 2928.9631
Epoch [ 154/2600] -> Loss: 2954.4835
Epoch [ 155/2600] -> Loss: 2937.2882
Epoch [ 156/2600] -> Loss: 2940.2028
Epoch [ 157/2600] -> Loss: 2944.2395
Epoch [ 158/2600] -> Loss: 2928.8987
Epoch   159: reducing learning rate of group 0 to 3.1250e-05.
Epoch [ 159/2600] -> Loss: 2933.3897
Epoch [ 160/2600] -> Loss: 2930.8122
Epoch [ 161/2600] -> Loss: 2924.4908
Epoch [ 162/2600] -> Loss: 2946.1072
Epoch [ 163/2600] -> Loss: 2920.0383
Epoch [ 164/2600] -> Loss: 2931.3683
Epoch [ 165/2600] -> Loss: 2954.9857
Epoch [ 166/2600] -> Loss: 2955.4070
Epoch [ 167/2600] -> Loss: 2935.7992
Epoch [ 168/2600] -> Loss: 2922.3028
Epoch [ 169/2600] -> Loss: 2928.1626
Epoch [ 170/2600] -> Loss: 2935.5999
Epoch [ 171/2600] -> Loss: 2949.1637
Epoch [ 172/2600] -> Loss: 2937.5504
Epoch [ 173/2600] -> Loss: 2924.0350
Epoch   174: reducing learning rate of group 0 to 1.5625e-05.
Epoch [ 174/2600] -> Loss: 2935.4850
Epoch [ 175/2600] -> Loss: 2939.1212
Epoch [ 176/2600] -> Loss: 2935.9217
Epoch [ 177/2600] -> Loss: 2936.3902
Epoch [ 178/2600] -> Loss: 2945.3265
Epoch [ 179/2600] -> Loss: 2935.7739
Epoch [ 180/2600] -> Loss: 2952.4550
Epoch [ 181/2600] -> Loss: 2927.4337
Epoch [ 182/2600] -> Loss: 2934.6693
Epoch [ 183/2600] -> Loss: 2945.6301
Epoch [ 184/2600] -> Loss: 2937.2651
Epoch   185: reducing learning rate of group 0 to 7.8125e-06.
Epoch [ 185/2600] -> Loss: 2926.2835
Epoch [ 186/2600] -> Loss: 2963.0017
Epoch [ 187/2600] -> Loss: 2942.7427
Epoch [ 188/2600] -> Loss: 2936.2563
Epoch [ 189/2600] -> Loss: 2948.9395
Epoch [ 190/2600] -> Loss: 2954.6514
Epoch [ 191/2600] -> Loss: 2934.6436
Epoch [ 192/2600] -> Loss: 2940.3449
Epoch [ 193/2600] -> Loss: 2929.8716
Epoch [ 194/2600] -> Loss: 2950.4788
Epoch [ 195/2600] -> Loss: 2940.4457
Epoch   196: reducing learning rate of group 0 to 3.9063e-06.
Epoch [ 196/2600] -> Loss: 2942.6361
Epoch [ 197/2600] -> Loss: 2944.7466
Epoch [ 198/2600] -> Loss: 2927.4431
Epoch [ 199/2600] -> Loss: 2927.6409
Epoch [ 200/2600] -> Loss: 2958.4444
Epoch [ 201/2600] -> Loss: 2948.9484
Epoch [ 202/2600] -> Loss: 2936.2480
Epoch [ 203/2600] -> Loss: 2918.8894
Epoch [ 204/2600] -> Loss: 2929.3113
Epoch [ 205/2600] -> Loss: 2943.8538
Epoch [ 206/2600] -> Loss: 2918.2895
Epoch [ 207/2600] -> Loss: 2934.5257
Epoch [ 208/2600] -> Loss: 2927.3248
Epoch [ 209/2600] -> Loss: 2934.1478
Epoch [ 210/2600] -> Loss: 2939.5341
Epoch [ 211/2600] -> Loss: 2940.0835
Epoch [ 212/2600] -> Loss: 2929.9422
Epoch [ 213/2600] -> Loss: 2939.4459
Epoch [ 214/2600] -> Loss: 2936.6455
Epoch [ 215/2600] -> Loss: 2934.9692
Epoch [ 216/2600] -> Loss: 2929.4082
Epoch   217: reducing learning rate of group 0 to 1.9531e-06.
Epoch [ 217/2600] -> Loss: 2929.5070
Epoch [ 218/2600] -> Loss: 2940.9632
Epoch [ 219/2600] -> Loss: 2942.9028
Epoch [ 220/2600] -> Loss: 2939.5570
Epoch [ 221/2600] -> Loss: 2928.4931
Epoch [ 222/2600] -> Loss: 2923.4832
Epoch [ 223/2600] -> Loss: 2928.2903
Epoch [ 224/2600] -> Loss: 2933.5868
Epoch [ 225/2600] -> Loss: 2928.5987
Epoch [ 226/2600] -> Loss: 2969.6545
Epoch [ 227/2600] -> Loss: 2950.7002
Epoch   228: reducing learning rate of group 0 to 9.7656e-07.
Epoch [ 228/2600] -> Loss: 2944.9987
Epoch [ 229/2600] -> Loss: 2939.7472
Epoch [ 230/2600] -> Loss: 2970.4648
Epoch [ 231/2600] -> Loss: 2927.3587
Epoch [ 232/2600] -> Loss: 2922.3950
Epoch [ 233/2600] -> Loss: 2950.2473
Epoch [ 234/2600] -> Loss: 2943.9177
Epoch [ 235/2600] -> Loss: 2945.8791
Epoch [ 236/2600] -> Loss: 2917.2053
Epoch [ 237/2600] -> Loss: 2935.5325
Epoch [ 238/2600] -> Loss: 2932.3878
Epoch [ 239/2600] -> Loss: 2925.9613
Epoch [ 240/2600] -> Loss: 2921.0651
Epoch [ 241/2600] -> Loss: 2932.8122
Epoch [ 242/2600] -> Loss: 2933.2091
Epoch [ 243/2600] -> Loss: 2925.5869
Epoch [ 244/2600] -> Loss: 2932.3838
Epoch [ 245/2600] -> Loss: 2956.5713
Epoch [ 246/2600] -> Loss: 2933.7954
Epoch   247: reducing learning rate of group 0 to 4.8828e-07.
Epoch [ 247/2600] -> Loss: 2941.2716
Epoch [ 248/2600] -> Loss: 2947.3557
Epoch [ 249/2600] -> Loss: 2940.0448
Epoch [ 250/2600] -> Loss: 2922.8309
Epoch [ 251/2600] -> Loss: 2941.2264
Epoch [ 252/2600] -> Loss: 2933.9856
Epoch [ 253/2600] -> Loss: 2936.5491
Epoch [ 254/2600] -> Loss: 2941.8125
Epoch [ 255/2600] -> Loss: 2935.3631
Epoch [ 256/2600] -> Loss: 2954.6500
Epoch [ 257/2600] -> Loss: 2941.8934
Epoch   258: reducing learning rate of group 0 to 2.4414e-07.
Epoch [ 258/2600] -> Loss: 2937.8827
Epoch [ 259/2600] -> Loss: 2948.4098
Epoch [ 260/2600] -> Loss: 2928.3536
Epoch [ 261/2600] -> Loss: 2937.1571
Epoch [ 262/2600] -> Loss: 2923.3764
Epoch [ 263/2600] -> Loss: 2941.0397
Epoch [ 264/2600] -> Loss: 2940.8635
Epoch [ 265/2600] -> Loss: 2938.4175
Epoch [ 266/2600] -> Loss: 2945.3530
Epoch [ 267/2600] -> Loss: 2944.2809
Epoch [ 268/2600] -> Loss: 2940.4279
Epoch   269: reducing learning rate of group 0 to 1.2207e-07.
Epoch [ 269/2600] -> Loss: 2950.8598
Epoch [ 270/2600] -> Loss: 2948.0746
Epoch [ 271/2600] -> Loss: 2934.3296
Epoch [ 272/2600] -> Loss: 2937.0060
Epoch [ 273/2600] -> Loss: 2941.1220
Epoch [ 274/2600] -> Loss: 2934.6286
Epoch [ 275/2600] -> Loss: 2930.0630
Epoch [ 276/2600] -> Loss: 2935.8546
Epoch [ 277/2600] -> Loss: 2943.2509
Epoch [ 278/2600] -> Loss: 2918.8154
Epoch [ 279/2600] -> Loss: 2939.4697
Epoch   280: reducing learning rate of group 0 to 6.1035e-08.
Epoch [ 280/2600] -> Loss: 2930.0873
Epoch [ 281/2600] -> Loss: 2934.0219
Epoch [ 282/2600] -> Loss: 2946.1362
Epoch [ 283/2600] -> Loss: 2930.6681
Epoch [ 284/2600] -> Loss: 2960.4999
Epoch [ 285/2600] -> Loss: 2930.2405
Epoch [ 286/2600] -> Loss: 2943.4808
Epoch [ 287/2600] -> Loss: 2932.3918
Epoch [ 288/2600] -> Loss: 2952.6788
Epoch [ 289/2600] -> Loss: 2937.7998
Epoch [ 290/2600] -> Loss: 2920.1869
Epoch   291: reducing learning rate of group 0 to 3.0518e-08.
Epoch [ 291/2600] -> Loss: 2944.4702
Epoch [ 292/2600] -> Loss: 2942.5078
Epoch [ 293/2600] -> Loss: 2947.2219
Epoch [ 294/2600] -> Loss: 2951.8628
Epoch [ 295/2600] -> Loss: 2932.5264
Epoch [ 296/2600] -> Loss: 2933.0994
Epoch [ 297/2600] -> Loss: 2943.5957
Epoch [ 298/2600] -> Loss: 2923.4826
Epoch [ 299/2600] -> Loss: 2935.8390
Epoch [ 300/2600] -> Loss: 2931.2673
Epoch [ 301/2600] -> Loss: 2933.4343
Epoch   302: reducing learning rate of group 0 to 1.5259e-08.
Epoch [ 302/2600] -> Loss: 2937.9325
Epoch [ 303/2600] -> Loss: 2940.2873
Epoch [ 304/2600] -> Loss: 2941.3573
Epoch [ 305/2600] -> Loss: 2958.7282
Epoch [ 306/2600] -> Loss: 2928.6535
Epoch [ 307/2600] -> Loss: 2933.7150
Epoch [ 308/2600] -> Loss: 2948.5776
Epoch [ 309/2600] -> Loss: 2958.7606
Epoch [ 310/2600] -> Loss: 2935.4217
Epoch [ 311/2600] -> Loss: 2936.1846
Epoch [ 312/2600] -> Loss: 2925.9905
Epoch [ 313/2600] -> Loss: 2944.3426
Epoch [ 314/2600] -> Loss: 2939.2437
Epoch [ 315/2600] -> Loss: 2934.9822
Epoch [ 316/2600] -> Loss: 2940.3277
Epoch [ 317/2600] -> Loss: 2936.1220
Epoch [ 318/2600] -> Loss: 2938.5719
Epoch [ 319/2600] -> Loss: 2932.1073
Epoch [ 320/2600] -> Loss: 2958.0504
Epoch [ 321/2600] -> Loss: 2936.7597
Epoch [ 322/2600] -> Loss: 2936.5265
Epoch [ 323/2600] -> Loss: 2924.4990
Epoch [ 324/2600] -> Loss: 2955.2208
Epoch [ 325/2600] -> Loss: 2917.5234
Epoch [ 326/2600] -> Loss: 2953.7191
Epoch [ 327/2600] -> Loss: 2941.7114
Epoch [ 328/2600] -> Loss: 2942.0720
Epoch [ 329/2600] -> Loss: 2923.0560
Epoch [ 330/2600] -> Loss: 2931.4254
Epoch [ 331/2600] -> Loss: 2937.7218
Epoch [ 332/2600] -> Loss: 2935.5506
Epoch [ 333/2600] -> Loss: 2941.2557
Epoch [ 334/2600] -> Loss: 2941.7343
Epoch [ 335/2600] -> Loss: 2956.6600
Epoch [ 336/2600] -> Loss: 2933.1521
Epoch [ 337/2600] -> Loss: 2942.2701
Epoch [ 338/2600] -> Loss: 2930.5591
Epoch [ 339/2600] -> Loss: 2950.4071
Epoch [ 340/2600] -> Loss: 2933.3067
Epoch [ 341/2600] -> Loss: 2929.7670
Epoch [ 342/2600] -> Loss: 2954.5133
Epoch [ 343/2600] -> Loss: 2938.4486
Epoch [ 344/2600] -> Loss: 2936.3208
Epoch [ 345/2600] -> Loss: 2928.7721
Epoch [ 346/2600] -> Loss: 2943.6220
Epoch [ 347/2600] -> Loss: 2946.6984
Epoch [ 348/2600] -> Loss: 2934.2447
Epoch [ 349/2600] -> Loss: 2945.5044
Epoch [ 350/2600] -> Loss: 2937.0227
Epoch [ 351/2600] -> Loss: 2933.9454
Epoch [ 352/2600] -> Loss: 2928.8356
Epoch [ 353/2600] -> Loss: 2937.1470
Epoch [ 354/2600] -> Loss: 2952.6043
Epoch [ 355/2600] -> Loss: 2938.1654
Epoch [ 356/2600] -> Loss: 2940.1082
Epoch [ 357/2600] -> Loss: 2938.2407
Epoch [ 358/2600] -> Loss: 2947.1862
Epoch [ 359/2600] -> Loss: 2930.0328
Epoch [ 360/2600] -> Loss: 2938.1365
Epoch [ 361/2600] -> Loss: 2934.0532
Epoch [ 362/2600] -> Loss: 2935.8222
Epoch [ 363/2600] -> Loss: 2949.6281
Epoch [ 364/2600] -> Loss: 2944.6815
Epoch [ 365/2600] -> Loss: 2917.6291
Epoch [ 366/2600] -> Loss: 2923.1528
Epoch [ 367/2600] -> Loss: 2933.4416
Epoch [ 368/2600] -> Loss: 2938.1281
Epoch [ 369/2600] -> Loss: 2929.8089
Epoch [ 370/2600] -> Loss: 2941.4242
Epoch [ 371/2600] -> Loss: 2939.4628
Epoch [ 372/2600] -> Loss: 2944.3281
Epoch [ 373/2600] -> Loss: 2952.4011
Epoch [ 374/2600] -> Loss: 2935.1701
Epoch [ 375/2600] -> Loss: 2936.1156
Epoch [ 376/2600] -> Loss: 2953.1739
Epoch [ 377/2600] -> Loss: 2929.0200
Epoch [ 378/2600] -> Loss: 2949.9941
Epoch [ 379/2600] -> Loss: 2944.8825
Epoch [ 380/2600] -> Loss: 2934.8526
Epoch [ 381/2600] -> Loss: 2933.8789
Epoch [ 382/2600] -> Loss: 2929.8445
Epoch [ 383/2600] -> Loss: 2936.3558
Epoch [ 384/2600] -> Loss: 2932.9062
Epoch [ 385/2600] -> Loss: 2929.1771
Epoch [ 386/2600] -> Loss: 2942.6076
Epoch [ 387/2600] -> Loss: 2946.8825
Epoch [ 388/2600] -> Loss: 2947.0795
Epoch [ 389/2600] -> Loss: 2930.3643
Epoch [ 390/2600] -> Loss: 2934.9965
Epoch [ 391/2600] -> Loss: 2927.7479
Epoch [ 392/2600] -> Loss: 2942.9765
