--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
File location :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
Pre-trained model available, loading model weights
--------------------------------------------------
Training model with: num_epochs=1800, start_lr=0.0005
Epoch [   1/1800] -> Loss: 1571.8879
Epoch [   2/1800] -> Loss: 1558.5108
Epoch [   3/1800] -> Loss: 1562.0776
Epoch [   4/1800] -> Loss: 1562.5758
Epoch [   5/1800] -> Loss: 1562.6736
Epoch [   6/1800] -> Loss: 1562.7031
Epoch [   7/1800] -> Loss: 1562.7140
Epoch [   8/1800] -> Loss: 1562.7161
Epoch [   9/1800] -> Loss: 1562.7133
Epoch [  10/1800] -> Loss: 1562.7076
Epoch [  11/1800] -> Loss: 1562.7000
Epoch [  12/1800] -> Loss: 1562.6906
Epoch    13: reducing learning rate of group 0 to 2.5000e-04.
Epoch [  13/1800] -> Loss: 1562.6809
Epoch [  14/1800] -> Loss: 1534.5487
Epoch [  15/1800] -> Loss: 1530.6714
Epoch [  16/1800] -> Loss: 1529.5471
Epoch [  17/1800] -> Loss: 1529.2690
Epoch [  18/1800] -> Loss: 1529.2093
Epoch [  19/1800] -> Loss: 1529.2601
Epoch [  20/1800] -> Loss: 1529.2534
Epoch [  21/1800] -> Loss: 1529.2905
Epoch [  22/1800] -> Loss: 1529.3276
Epoch [  23/1800] -> Loss: 1529.3820
Epoch [  24/1800] -> Loss: 1529.3628
Epoch [  25/1800] -> Loss: 1529.4245
Epoch [  26/1800] -> Loss: 1529.4026
Epoch [  27/1800] -> Loss: 1529.4601
Epoch    28: reducing learning rate of group 0 to 1.2500e-04.
Epoch [  28/1800] -> Loss: 1529.4321
Epoch [  29/1800] -> Loss: 1513.7315
Epoch [  30/1800] -> Loss: 1512.6860
Epoch [  31/1800] -> Loss: 1512.1266
Epoch [  32/1800] -> Loss: 1511.8435
Epoch [  33/1800] -> Loss: 1511.6691
Epoch [  34/1800] -> Loss: 1511.5916
Epoch [  35/1800] -> Loss: 1511.5502
Epoch [  36/1800] -> Loss: 1511.5282
Epoch [  37/1800] -> Loss: 1511.5182
Epoch [  38/1800] -> Loss: 1511.5142
Epoch [  39/1800] -> Loss: 1511.5139
Epoch [  40/1800] -> Loss: 1511.5155
Epoch [  41/1800] -> Loss: 1511.5173
Epoch [  42/1800] -> Loss: 1511.5195
Epoch [  43/1800] -> Loss: 1511.5217
Epoch [  44/1800] -> Loss: 1511.5233
Epoch [  45/1800] -> Loss: 1511.5236
Epoch [  46/1800] -> Loss: 1511.5240
Epoch [  47/1800] -> Loss: 1511.5236
Epoch [  48/1800] -> Loss: 1511.5228
Epoch    49: reducing learning rate of group 0 to 6.2500e-05.
Epoch [  49/1800] -> Loss: 1511.5219
--------------------------------------------------
Model checkpoint saved as FFNN_50.pth
--------------------------------------------------
Epoch [  50/1800] -> Loss: 1502.8952
Epoch [  51/1800] -> Loss: 1502.7116
Epoch [  52/1800] -> Loss: 1502.5818
Epoch [  53/1800] -> Loss: 1502.4813
Epoch [  54/1800] -> Loss: 1502.4190
Epoch [  55/1800] -> Loss: 1502.3723
Epoch [  56/1800] -> Loss: 1502.3383
Epoch [  57/1800] -> Loss: 1502.3123
Epoch [  58/1800] -> Loss: 1502.2925
Epoch [  59/1800] -> Loss: 1502.2766
Epoch [  60/1800] -> Loss: 1502.2647
Epoch [  61/1800] -> Loss: 1502.2544
Epoch [  62/1800] -> Loss: 1502.2465
Epoch [  63/1800] -> Loss: 1502.2400
Epoch [  64/1800] -> Loss: 1502.2347
Epoch [  65/1800] -> Loss: 1502.2297
Epoch [  66/1800] -> Loss: 1502.2256
Epoch [  67/1800] -> Loss: 1502.2222
Epoch    68: reducing learning rate of group 0 to 3.1250e-05.
Epoch [  68/1800] -> Loss: 1502.2188
Epoch [  69/1800] -> Loss: 1497.6854
Epoch [  70/1800] -> Loss: 1497.6569
Epoch [  71/1800] -> Loss: 1497.6264
Epoch [  72/1800] -> Loss: 1497.6090
Epoch [  73/1800] -> Loss: 1497.5933
Epoch [  74/1800] -> Loss: 1497.5810
Epoch [  75/1800] -> Loss: 1497.5708
Epoch [  76/1800] -> Loss: 1497.5614
Epoch [  77/1800] -> Loss: 1497.5531
Epoch [  78/1800] -> Loss: 1497.5461
Epoch [  79/1800] -> Loss: 1497.5397
Epoch [  80/1800] -> Loss: 1497.5341
Epoch [  81/1800] -> Loss: 1497.5286
Epoch [  82/1800] -> Loss: 1497.5241
Epoch [  83/1800] -> Loss: 1497.5195
Epoch [  84/1800] -> Loss: 1497.5156
Epoch [  85/1800] -> Loss: 1497.5120
Epoch [  86/1800] -> Loss: 1497.5087
Epoch [  87/1800] -> Loss: 1497.5051
Epoch [  88/1800] -> Loss: 1497.5020
Epoch [  89/1800] -> Loss: 1497.4991
Epoch [  90/1800] -> Loss: 1497.4964
Epoch    91: reducing learning rate of group 0 to 1.5625e-05.
Epoch [  91/1800] -> Loss: 1497.4935
Epoch [  92/1800] -> Loss: 1495.1654
Epoch [  93/1800] -> Loss: 1495.1599
Epoch [  94/1800] -> Loss: 1495.1546
Epoch [  95/1800] -> Loss: 1495.1493
Epoch [  96/1800] -> Loss: 1495.1453
Epoch [  97/1800] -> Loss: 1495.1416
Epoch [  98/1800] -> Loss: 1495.1380
Epoch [  99/1800] -> Loss: 1495.1347
--------------------------------------------------
Model checkpoint saved as FFNN_100.pth
--------------------------------------------------
Epoch [ 100/1800] -> Loss: 1495.1318
Epoch [ 101/1800] -> Loss: 1495.1289
Epoch [ 102/1800] -> Loss: 1495.1262
Epoch   103: reducing learning rate of group 0 to 7.8125e-06.
Epoch [ 103/1800] -> Loss: 1495.1235
Epoch [ 104/1800] -> Loss: 1493.9413
Epoch [ 105/1800] -> Loss: 1493.9396
Epoch [ 106/1800] -> Loss: 1493.9378
Epoch [ 107/1800] -> Loss: 1493.9359
Epoch [ 108/1800] -> Loss: 1493.9342
Epoch [ 109/1800] -> Loss: 1493.9327
Epoch [ 110/1800] -> Loss: 1493.9311
Epoch [ 111/1800] -> Loss: 1493.9294
Epoch [ 112/1800] -> Loss: 1493.9282
Epoch [ 113/1800] -> Loss: 1493.9266
Epoch [ 114/1800] -> Loss: 1493.9253
Epoch   115: reducing learning rate of group 0 to 3.9063e-06.
Epoch [ 115/1800] -> Loss: 1493.9241
Epoch [ 116/1800] -> Loss: 1493.3287
Epoch [ 117/1800] -> Loss: 1493.3278
Epoch [ 118/1800] -> Loss: 1493.3271
Epoch [ 119/1800] -> Loss: 1493.3264
Epoch [ 120/1800] -> Loss: 1493.3256
Epoch [ 121/1800] -> Loss: 1493.3249
Epoch [ 122/1800] -> Loss: 1493.3241
Epoch [ 123/1800] -> Loss: 1493.3234
Epoch [ 124/1800] -> Loss: 1493.3228
Epoch [ 125/1800] -> Loss: 1493.3223
Epoch [ 126/1800] -> Loss: 1493.3214
Epoch   127: reducing learning rate of group 0 to 1.9531e-06.
Epoch [ 127/1800] -> Loss: 1493.3208
Epoch [ 128/1800] -> Loss: 1493.0235
Epoch [ 129/1800] -> Loss: 1493.0231
Epoch [ 130/1800] -> Loss: 1493.0227
Epoch [ 131/1800] -> Loss: 1493.0224
Epoch [ 132/1800] -> Loss: 1493.0220
Epoch [ 133/1800] -> Loss: 1493.0216
Epoch [ 134/1800] -> Loss: 1493.0213
Epoch [ 135/1800] -> Loss: 1493.0209
Epoch [ 136/1800] -> Loss: 1493.0204
Epoch [ 137/1800] -> Loss: 1493.0200
Epoch [ 138/1800] -> Loss: 1493.0197
Epoch   139: reducing learning rate of group 0 to 9.7656e-07.
Epoch [ 139/1800] -> Loss: 1493.0194
Epoch [ 140/1800] -> Loss: 1492.8680
Epoch [ 141/1800] -> Loss: 1492.8679
Epoch [ 142/1800] -> Loss: 1492.8678
Epoch [ 143/1800] -> Loss: 1492.8676
Epoch [ 144/1800] -> Loss: 1492.8674
Epoch [ 145/1800] -> Loss: 1492.8673
Epoch [ 146/1800] -> Loss: 1492.8671
Epoch [ 147/1800] -> Loss: 1492.8670
Epoch [ 148/1800] -> Loss: 1492.8669
Epoch [ 149/1800] -> Loss: 1492.8668
--------------------------------------------------
Model checkpoint saved as FFNN_150.pth
--------------------------------------------------
Epoch [ 150/1800] -> Loss: 1492.8666
