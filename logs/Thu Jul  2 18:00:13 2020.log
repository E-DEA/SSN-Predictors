Usage: python3 SSN_predictor.py <path_to_ssn_datafile> <path_to_aa_datafile>
----------------------------------------------------------------
Code running on device: cuda
----------------------------------------------------------------
Data loaded from file locations :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
----------------------------------------------------------------
Solar cycle data loaded/saved as: cycle_data.pickle
----------------------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=3, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=3, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
----------------------------------------------------------------
No pre-trained models available, initializing model weights
----------------------------------------------------------------
Training model with: num_epochs=1200, start_lr=0.00025
Epoch [   1/1200] -> Loss: 121.2373
Epoch [   2/1200] -> Loss: 114.9442
Epoch [   3/1200] -> Loss: 100.7218
Epoch [   4/1200] -> Loss: 85.2201
Epoch [   5/1200] -> Loss: 68.3069
Epoch [   6/1200] -> Loss: 56.4720
Epoch [   7/1200] -> Loss: 52.8361
Epoch [   8/1200] -> Loss: 52.0616
Epoch [   9/1200] -> Loss: 51.9158
Epoch [  10/1200] -> Loss: 51.8933
Epoch [  11/1200] -> Loss: 51.6817
Epoch [  12/1200] -> Loss: 52.0870
Epoch [  13/1200] -> Loss: 51.6640
Epoch [  14/1200] -> Loss: 51.7592
Epoch [  15/1200] -> Loss: 52.1211
Epoch [  16/1200] -> Loss: 51.4605
Epoch [  17/1200] -> Loss: 52.1124
Epoch [  18/1200] -> Loss: 51.5053
Epoch [  19/1200] -> Loss: 51.6628
Epoch [  20/1200] -> Loss: 51.6724
Epoch [  21/1200] -> Loss: 51.7088
Epoch [  22/1200] -> Loss: 51.6158
Epoch [  23/1200] -> Loss: 51.4094
Epoch [  24/1200] -> Loss: 51.1657
Epoch [  25/1200] -> Loss: 51.5664
Epoch [  26/1200] -> Loss: 51.2418
Epoch [  27/1200] -> Loss: 51.2219
Epoch [  28/1200] -> Loss: 50.7674
Epoch [  29/1200] -> Loss: 51.2213
Epoch [  30/1200] -> Loss: 51.0023
Epoch [  31/1200] -> Loss: 51.0657
Epoch [  32/1200] -> Loss: 50.9267
Epoch [  33/1200] -> Loss: 51.0756
Epoch [  34/1200] -> Loss: 50.8768
Epoch [  35/1200] -> Loss: 50.5159
Epoch [  36/1200] -> Loss: 50.6677
Epoch [  37/1200] -> Loss: 50.6103
Epoch [  38/1200] -> Loss: 50.6844
Epoch [  39/1200] -> Loss: 50.7378
Epoch [  40/1200] -> Loss: 50.8732
Epoch [  41/1200] -> Loss: 50.6029
Epoch [  42/1200] -> Loss: 50.6887
Epoch [  43/1200] -> Loss: 50.2098
Epoch [  44/1200] -> Loss: 50.5918
Epoch [  45/1200] -> Loss: 50.2459
Epoch [  46/1200] -> Loss: 50.5266
Epoch [  47/1200] -> Loss: 49.9952
Epoch [  48/1200] -> Loss: 50.3125
Epoch [  49/1200] -> Loss: 50.3069
Epoch [  50/1200] -> Loss: 50.2885
Epoch [  51/1200] -> Loss: 50.0873
Epoch [  52/1200] -> Loss: 50.1543
Epoch [  53/1200] -> Loss: 50.1560
Epoch [  54/1200] -> Loss: 50.2098
Epoch [  55/1200] -> Loss: 49.7721
Epoch [  56/1200] -> Loss: 49.5811
Epoch [  57/1200] -> Loss: 49.8518
Epoch [  58/1200] -> Loss: 49.4328
Epoch [  59/1200] -> Loss: 49.7653
Epoch [  60/1200] -> Loss: 49.4343
Epoch [  61/1200] -> Loss: 49.8472
Epoch [  62/1200] -> Loss: 49.7081
Epoch [  63/1200] -> Loss: 49.9205
Epoch [  64/1200] -> Loss: 49.4775
Epoch [  65/1200] -> Loss: 49.7918
Epoch [  66/1200] -> Loss: 49.3445
Epoch [  67/1200] -> Loss: 49.4219
Epoch [  68/1200] -> Loss: 49.5166
Epoch [  69/1200] -> Loss: 49.2373
Epoch [  70/1200] -> Loss: 49.2488
Epoch [  71/1200] -> Loss: 48.5280
Epoch [  72/1200] -> Loss: 49.1182
Epoch [  73/1200] -> Loss: 49.3322
Epoch [  74/1200] -> Loss: 48.8366
Epoch [  75/1200] -> Loss: 48.8249
Epoch [  76/1200] -> Loss: 49.0918
Epoch [  77/1200] -> Loss: 48.6429
Epoch [  78/1200] -> Loss: 48.9677
Epoch [  79/1200] -> Loss: 48.6072
Epoch [  80/1200] -> Loss: 48.7881
Epoch [  81/1200] -> Loss: 48.6766
Epoch    82: reducing learning rate of group 0 to 1.2500e-04.
Epoch [  82/1200] -> Loss: 48.6816
Epoch [  83/1200] -> Loss: 48.5952
Epoch [  84/1200] -> Loss: 48.6550
Epoch [  85/1200] -> Loss: 48.5099
Epoch [  86/1200] -> Loss: 48.5790
Epoch [  87/1200] -> Loss: 48.6587
Epoch [  88/1200] -> Loss: 48.2931
Epoch [  89/1200] -> Loss: 48.6197
Epoch [  90/1200] -> Loss: 48.5401
Epoch [  91/1200] -> Loss: 48.4113
Epoch [  92/1200] -> Loss: 48.4962
Epoch [  93/1200] -> Loss: 48.0715
Epoch [  94/1200] -> Loss: 48.2378
Epoch [  95/1200] -> Loss: 48.4307
Epoch [  96/1200] -> Loss: 48.3461
Epoch [  97/1200] -> Loss: 48.6010
Epoch [  98/1200] -> Loss: 47.8830
Epoch [  99/1200] -> Loss: 48.2106
----------------------------------------------------------------
Model checkpoint saved as FFNN_100.pth
----------------------------------------------------------------
Epoch [ 100/1200] -> Loss: 48.4260
Epoch [ 101/1200] -> Loss: 48.4269
Epoch [ 102/1200] -> Loss: 48.3291
Epoch [ 103/1200] -> Loss: 48.3328
Epoch [ 104/1200] -> Loss: 48.2414
Epoch [ 105/1200] -> Loss: 47.9096
Epoch [ 106/1200] -> Loss: 48.3459
Epoch [ 107/1200] -> Loss: 48.5196
Epoch [ 108/1200] -> Loss: 47.8823
Epoch   109: reducing learning rate of group 0 to 6.2500e-05.
Epoch [ 109/1200] -> Loss: 48.1724
Epoch [ 110/1200] -> Loss: 48.1478
Epoch [ 111/1200] -> Loss: 47.9513
Epoch [ 112/1200] -> Loss: 47.9507
Epoch [ 113/1200] -> Loss: 48.1396
Epoch [ 114/1200] -> Loss: 48.0853
Epoch [ 115/1200] -> Loss: 47.7004
Epoch [ 116/1200] -> Loss: 48.0151
Epoch [ 117/1200] -> Loss: 47.8033
Epoch [ 118/1200] -> Loss: 48.0544
Epoch [ 119/1200] -> Loss: 47.8174
Epoch [ 120/1200] -> Loss: 47.9886
Epoch [ 121/1200] -> Loss: 47.3478
Epoch [ 122/1200] -> Loss: 47.8405
Epoch [ 123/1200] -> Loss: 47.7576
Epoch [ 124/1200] -> Loss: 47.7602
Epoch [ 125/1200] -> Loss: 47.8096
Epoch [ 126/1200] -> Loss: 48.2642
Epoch [ 127/1200] -> Loss: 47.5202
Epoch [ 128/1200] -> Loss: 47.4829
Epoch [ 129/1200] -> Loss: 47.7174
Epoch [ 130/1200] -> Loss: 47.6322
Epoch [ 131/1200] -> Loss: 47.7412
Epoch   132: reducing learning rate of group 0 to 3.1250e-05.
Epoch [ 132/1200] -> Loss: 47.7635
Epoch [ 133/1200] -> Loss: 47.6971
Epoch [ 134/1200] -> Loss: 47.6421
Epoch [ 135/1200] -> Loss: 47.4010
Epoch [ 136/1200] -> Loss: 46.7800
Epoch [ 137/1200] -> Loss: 47.7919
Epoch [ 138/1200] -> Loss: 47.2957
Epoch [ 139/1200] -> Loss: 47.2053
Epoch [ 140/1200] -> Loss: 47.5834
Epoch [ 141/1200] -> Loss: 47.2257
Epoch [ 142/1200] -> Loss: 47.4061
Epoch [ 143/1200] -> Loss: 47.6856
Epoch [ 144/1200] -> Loss: 47.5465
