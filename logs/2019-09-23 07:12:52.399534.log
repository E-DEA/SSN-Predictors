--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
Dataset source : NOAA, ISGI
File location :
    SSN - /home/extern/Documents/Research/data/NOAA/table_international-sunspot-numbers_monthly.txt
    AA - /home/extern/Documents/Research/data/ISGI/aa_1869-08-01_2017-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
Pre-trained model available, loading model weights
--------------------------------------------------
Training model with: num_epochs=450, start_lr=0.0005
Epoch [   1/450] -> Loss: 2065.8694
Epoch [   2/450] -> Loss: 1887.6017
Epoch [   3/450] -> Loss: 1834.5038
Epoch [   4/450] -> Loss: 1793.5131
Epoch [   5/450] -> Loss: 1759.8486
Epoch [   6/450] -> Loss: 1730.3676
Epoch [   7/450] -> Loss: 1702.5577
Epoch [   8/450] -> Loss: 1677.7437
Epoch [   9/450] -> Loss: 1656.7511
Epoch [  10/450] -> Loss: 1634.9884
Epoch [  11/450] -> Loss: 1614.3337
Epoch [  12/450] -> Loss: 1595.6815
Epoch [  13/450] -> Loss: 1575.5152
Epoch [  14/450] -> Loss: 1554.7577
Epoch [  15/450] -> Loss: 1538.5329
Epoch [  16/450] -> Loss: 1518.1023
Epoch [  17/450] -> Loss: 1503.1822
Epoch [  18/450] -> Loss: 1485.5396
Epoch [  19/450] -> Loss: 1467.6549
Epoch [  20/450] -> Loss: 1451.9956
Epoch [  21/450] -> Loss: 1434.8083
Epoch [  22/450] -> Loss: 1420.9879
Epoch [  23/450] -> Loss: 1406.2590
Epoch [  24/450] -> Loss: 1393.5136
Epoch [  25/450] -> Loss: 1379.6321
Epoch [  26/450] -> Loss: 1366.5365
Epoch [  27/450] -> Loss: 1356.9713
Epoch [  28/450] -> Loss: 1345.1628
Epoch [  29/450] -> Loss: 1336.4390
Epoch [  30/450] -> Loss: 1328.8409
Epoch [  31/450] -> Loss: 1318.6798
Epoch [  32/450] -> Loss: 1313.3492
Epoch [  33/450] -> Loss: 1306.9753
Epoch [  34/450] -> Loss: 1300.2059
Epoch [  35/450] -> Loss: 1295.3000
Epoch [  36/450] -> Loss: 1290.0253
Epoch [  37/450] -> Loss: 1284.4144
Epoch [  38/450] -> Loss: 1281.2796
Epoch [  39/450] -> Loss: 1276.9009
Epoch [  40/450] -> Loss: 1271.9141
Epoch [  41/450] -> Loss: 1267.3807
Epoch [  42/450] -> Loss: 1265.5822
Epoch [  43/450] -> Loss: 1262.1494
Epoch [  44/450] -> Loss: 1258.3991
Epoch [  45/450] -> Loss: 1255.0997
Epoch [  46/450] -> Loss: 1253.0686
Epoch [  47/450] -> Loss: 1250.5727
Epoch [  48/450] -> Loss: 1249.5958
Epoch [  49/450] -> Loss: 1246.0429
--------------------------------------------------
Model checkpoint saved as FFNN_50.pth
--------------------------------------------------
Epoch [  50/450] -> Loss: 1242.2402
Epoch [  51/450] -> Loss: 1241.4413
Epoch [  52/450] -> Loss: 1240.9180
Epoch [  53/450] -> Loss: 1237.9390
Epoch [  54/450] -> Loss: 1237.6620
Epoch [  55/450] -> Loss: 1234.5083
Epoch [  56/450] -> Loss: 1232.1893
Epoch [  57/450] -> Loss: 1231.9178
Epoch [  58/450] -> Loss: 1232.1630
Epoch [  59/450] -> Loss: 1229.5182
Epoch [  60/450] -> Loss: 1230.5820
Epoch [  61/450] -> Loss: 1229.4546
Epoch [  62/450] -> Loss: 1225.7839
Epoch [  63/450] -> Loss: 1225.8593
Epoch [  64/450] -> Loss: 1224.7427
Epoch [  65/450] -> Loss: 1225.7470
Epoch [  66/450] -> Loss: 1222.8332
Epoch [  67/450] -> Loss: 1220.5820
Epoch [  68/450] -> Loss: 1222.7171
Epoch [  69/450] -> Loss: 1222.8568
Epoch [  70/450] -> Loss: 1220.1610
Epoch [  71/450] -> Loss: 1219.2505
Epoch [  72/450] -> Loss: 1217.9967
Epoch [  73/450] -> Loss: 1217.5138
Epoch [  74/450] -> Loss: 1218.1204
Epoch [  75/450] -> Loss: 1217.7917
Epoch [  76/450] -> Loss: 1217.7825
Epoch [  77/450] -> Loss: 1215.1192
Epoch [  78/450] -> Loss: 1213.3532
Epoch [  79/450] -> Loss: 1213.1038
Epoch [  80/450] -> Loss: 1214.0276
Epoch [  81/450] -> Loss: 1214.4366
Epoch [  82/450] -> Loss: 1213.3389
Epoch [  83/450] -> Loss: 1212.1768
Epoch [  84/450] -> Loss: 1211.7147
Epoch [  85/450] -> Loss: 1210.1099
Epoch [  86/450] -> Loss: 1209.4726
Epoch [  87/450] -> Loss: 1210.8985
Epoch [  88/450] -> Loss: 1209.0449
Epoch [  89/450] -> Loss: 1207.9121
Epoch [  90/450] -> Loss: 1207.9771
Epoch [  91/450] -> Loss: 1207.5367
Epoch [  92/450] -> Loss: 1204.2126
Epoch [  93/450] -> Loss: 1208.2443
Epoch [  94/450] -> Loss: 1205.3541
Epoch [  95/450] -> Loss: 1204.3310
Epoch [  96/450] -> Loss: 1205.8022
Epoch [  97/450] -> Loss: 1204.2444
Epoch [  98/450] -> Loss: 1205.9072
Epoch [  99/450] -> Loss: 1201.6258
--------------------------------------------------
Model checkpoint saved as FFNN_100.pth
--------------------------------------------------
Epoch [ 100/450] -> Loss: 1205.7219
Epoch [ 101/450] -> Loss: 1204.3217
Epoch [ 102/450] -> Loss: 1203.3640
Epoch [ 103/450] -> Loss: 1200.3424
Epoch [ 104/450] -> Loss: 1201.6844
Epoch [ 105/450] -> Loss: 1201.8366
Epoch [ 106/450] -> Loss: 1201.0708
Epoch [ 107/450] -> Loss: 1202.0646
Epoch [ 108/450] -> Loss: 1200.4341
Epoch [ 109/450] -> Loss: 1199.5493
Epoch [ 110/450] -> Loss: 1198.1614
Epoch [ 111/450] -> Loss: 1198.8582
Epoch [ 112/450] -> Loss: 1199.1065
Epoch [ 113/450] -> Loss: 1197.6283
Epoch [ 114/450] -> Loss: 1197.3435
Epoch [ 115/450] -> Loss: 1196.6914
Epoch [ 116/450] -> Loss: 1196.1597
Epoch [ 117/450] -> Loss: 1197.5701
Epoch [ 118/450] -> Loss: 1194.3876
Epoch [ 119/450] -> Loss: 1194.9374
Epoch [ 120/450] -> Loss: 1196.5616
Epoch [ 121/450] -> Loss: 1195.6771
Epoch [ 122/450] -> Loss: 1195.2529
Epoch [ 123/450] -> Loss: 1193.5719
Epoch [ 124/450] -> Loss: 1193.4291
Epoch [ 125/450] -> Loss: 1193.8211
Epoch [ 126/450] -> Loss: 1193.3017
Epoch [ 127/450] -> Loss: 1194.5622
Epoch [ 128/450] -> Loss: 1193.5890
Epoch [ 129/450] -> Loss: 1190.4538
Epoch [ 130/450] -> Loss: 1191.6981
Epoch [ 131/450] -> Loss: 1188.2267
Epoch [ 132/450] -> Loss: 1192.6397
Epoch [ 133/450] -> Loss: 1191.7329
Epoch [ 134/450] -> Loss: 1190.2458
Epoch [ 135/450] -> Loss: 1189.5924
Epoch [ 136/450] -> Loss: 1187.3851
Epoch [ 137/450] -> Loss: 1189.4346
Epoch [ 138/450] -> Loss: 1189.6949
Epoch [ 139/450] -> Loss: 1189.3921
Epoch [ 140/450] -> Loss: 1190.6577
Epoch [ 141/450] -> Loss: 1189.5435
Epoch [ 142/450] -> Loss: 1189.2396
Epoch [ 143/450] -> Loss: 1186.3622
Epoch [ 144/450] -> Loss: 1187.5284
Epoch [ 145/450] -> Loss: 1187.8013
Epoch [ 146/450] -> Loss: 1187.4524
Epoch [ 147/450] -> Loss: 1186.9146
Epoch [ 148/450] -> Loss: 1184.7708
Epoch [ 149/450] -> Loss: 1186.7661
--------------------------------------------------
Model checkpoint saved as FFNN_150.pth
--------------------------------------------------
Epoch [ 150/450] -> Loss: 1185.5363
Epoch [ 151/450] -> Loss: 1187.2119
Epoch [ 152/450] -> Loss: 1186.3595
Epoch [ 153/450] -> Loss: 1184.5504
Epoch [ 154/450] -> Loss: 1185.4334
Epoch [ 155/450] -> Loss: 1184.2211
Epoch [ 156/450] -> Loss: 1186.6014
Epoch [ 157/450] -> Loss: 1184.3177
Epoch [ 158/450] -> Loss: 1184.9899
Epoch [ 159/450] -> Loss: 1186.0718
Epoch [ 160/450] -> Loss: 1182.3887
Epoch [ 161/450] -> Loss: 1184.4797
Epoch [ 162/450] -> Loss: 1180.4855
Epoch [ 163/450] -> Loss: 1183.0349
Epoch [ 164/450] -> Loss: 1182.0033
Epoch [ 165/450] -> Loss: 1182.4369
Epoch [ 166/450] -> Loss: 1183.8118
Epoch [ 167/450] -> Loss: 1182.8169
Epoch [ 168/450] -> Loss: 1180.3654
Epoch [ 169/450] -> Loss: 1179.5472
Epoch [ 170/450] -> Loss: 1182.3958
Epoch [ 171/450] -> Loss: 1181.7156
Epoch [ 172/450] -> Loss: 1182.4362
Epoch [ 173/450] -> Loss: 1179.4882
Epoch [ 174/450] -> Loss: 1179.4100
Epoch [ 175/450] -> Loss: 1180.5467
Epoch [ 176/450] -> Loss: 1178.3524
Epoch [ 177/450] -> Loss: 1179.2283
Epoch [ 178/450] -> Loss: 1181.1604
Epoch [ 179/450] -> Loss: 1179.2815
Epoch [ 180/450] -> Loss: 1181.3171
Epoch [ 181/450] -> Loss: 1178.9380
Epoch [ 182/450] -> Loss: 1179.7035
Epoch [ 183/450] -> Loss: 1179.1200
Epoch [ 184/450] -> Loss: 1178.4936
Epoch [ 185/450] -> Loss: 1177.6400
Epoch [ 186/450] -> Loss: 1180.0793
Epoch [ 187/450] -> Loss: 1178.4752
Epoch [ 188/450] -> Loss: 1177.9759
Epoch [ 189/450] -> Loss: 1177.2166
Epoch [ 190/450] -> Loss: 1177.4210
Epoch [ 191/450] -> Loss: 1175.8126
Epoch [ 192/450] -> Loss: 1177.3377
Epoch [ 193/450] -> Loss: 1176.5544
Epoch [ 194/450] -> Loss: 1176.5234
Epoch [ 195/450] -> Loss: 1176.0716
Epoch [ 196/450] -> Loss: 1176.5014
Epoch [ 197/450] -> Loss: 1175.6998
Epoch [ 198/450] -> Loss: 1174.8013
Epoch [ 199/450] -> Loss: 1175.6340
--------------------------------------------------
Model checkpoint saved as FFNN_200.pth
--------------------------------------------------
Epoch [ 200/450] -> Loss: 1174.4203
Epoch [ 201/450] -> Loss: 1175.1001
Epoch [ 202/450] -> Loss: 1172.2737
Epoch [ 203/450] -> Loss: 1178.3086
Epoch [ 204/450] -> Loss: 1171.1989
Epoch [ 205/450] -> Loss: 1173.5634
Epoch [ 206/450] -> Loss: 1174.8206
Epoch [ 207/450] -> Loss: 1173.1154
Epoch [ 208/450] -> Loss: 1174.9239
Epoch [ 209/450] -> Loss: 1173.1617
Epoch [ 210/450] -> Loss: 1170.3785
Epoch [ 211/450] -> Loss: 1173.6059
Epoch [ 212/450] -> Loss: 1169.5709
Epoch [ 213/450] -> Loss: 1171.2887
Epoch [ 214/450] -> Loss: 1171.1985
Epoch [ 215/450] -> Loss: 1172.0113
Epoch [ 216/450] -> Loss: 1170.6547
Epoch [ 217/450] -> Loss: 1170.1003
Epoch [ 218/450] -> Loss: 1170.8314
Epoch [ 219/450] -> Loss: 1170.0432
Epoch [ 220/450] -> Loss: 1172.6368
Epoch [ 221/450] -> Loss: 1168.6523
Epoch [ 222/450] -> Loss: 1170.3651
Epoch [ 223/450] -> Loss: 1170.0746
Epoch [ 224/450] -> Loss: 1169.6655
Epoch [ 225/450] -> Loss: 1166.6091
Epoch [ 226/450] -> Loss: 1169.1017
Epoch [ 227/450] -> Loss: 1169.4796
Epoch [ 228/450] -> Loss: 1167.9562
Epoch [ 229/450] -> Loss: 1166.5835
Epoch [ 230/450] -> Loss: 1168.3811
Epoch [ 231/450] -> Loss: 1167.8701
Epoch [ 232/450] -> Loss: 1167.9170
Epoch [ 233/450] -> Loss: 1166.3362
Epoch [ 234/450] -> Loss: 1168.8771
Epoch [ 235/450] -> Loss: 1165.3894
Epoch [ 236/450] -> Loss: 1166.7422
Epoch [ 237/450] -> Loss: 1166.1944
Epoch [ 238/450] -> Loss: 1166.1628
Epoch [ 239/450] -> Loss: 1164.6703
Epoch [ 240/450] -> Loss: 1166.4590
Epoch [ 241/450] -> Loss: 1165.5866
Epoch [ 242/450] -> Loss: 1162.5511
Epoch [ 243/450] -> Loss: 1164.7832
Epoch [ 244/450] -> Loss: 1165.1342
Epoch [ 245/450] -> Loss: 1163.3506
Epoch [ 246/450] -> Loss: 1165.5295
Epoch [ 247/450] -> Loss: 1164.5130
Epoch [ 248/450] -> Loss: 1162.5731
Epoch [ 249/450] -> Loss: 1164.0212
--------------------------------------------------
Model checkpoint saved as FFNN_250.pth
--------------------------------------------------
Epoch [ 250/450] -> Loss: 1163.7243
Epoch [ 251/450] -> Loss: 1162.1069
Epoch [ 252/450] -> Loss: 1162.3554
Epoch [ 253/450] -> Loss: 1163.2775
Epoch [ 254/450] -> Loss: 1161.5526
Epoch [ 255/450] -> Loss: 1163.1092
Epoch [ 256/450] -> Loss: 1161.7476
Epoch [ 257/450] -> Loss: 1162.1200
Epoch [ 258/450] -> Loss: 1162.5884
Epoch [ 259/450] -> Loss: 1164.1133
Epoch [ 260/450] -> Loss: 1161.8069
Epoch [ 261/450] -> Loss: 1160.9703
Epoch [ 262/450] -> Loss: 1158.5560
Epoch [ 263/450] -> Loss: 1161.5104
Epoch [ 264/450] -> Loss: 1161.2017
Epoch [ 265/450] -> Loss: 1159.9614
Epoch [ 266/450] -> Loss: 1163.0501
Epoch [ 267/450] -> Loss: 1160.0139
Epoch [ 268/450] -> Loss: 1158.2789
Epoch [ 269/450] -> Loss: 1159.1358
Epoch [ 270/450] -> Loss: 1160.3448
Epoch [ 271/450] -> Loss: 1159.4966
Epoch [ 272/450] -> Loss: 1157.3945
Epoch [ 273/450] -> Loss: 1160.6563
Epoch [ 274/450] -> Loss: 1157.6479
Epoch [ 275/450] -> Loss: 1157.6128
Epoch [ 276/450] -> Loss: 1159.3929
Epoch [ 277/450] -> Loss: 1157.7399
Epoch [ 278/450] -> Loss: 1157.9117
Epoch [ 279/450] -> Loss: 1158.0874
Epoch [ 280/450] -> Loss: 1155.9991
Epoch [ 281/450] -> Loss: 1158.1249
Epoch [ 282/450] -> Loss: 1157.6090
Epoch [ 283/450] -> Loss: 1157.5014
Epoch [ 284/450] -> Loss: 1157.5365
Epoch [ 285/450] -> Loss: 1154.8282
Epoch [ 286/450] -> Loss: 1156.4339
Epoch [ 287/450] -> Loss: 1157.8466
Epoch [ 288/450] -> Loss: 1157.8354
Epoch [ 289/450] -> Loss: 1155.8209
Epoch [ 290/450] -> Loss: 1155.8691
Epoch [ 291/450] -> Loss: 1154.8825
Epoch [ 292/450] -> Loss: 1155.4287
Epoch [ 293/450] -> Loss: 1154.9889
Epoch [ 294/450] -> Loss: 1155.0268
Epoch [ 295/450] -> Loss: 1154.7843
Epoch   295: reducing learning rate of group 0 to 2.5000e-04.
Epoch [ 296/450] -> Loss: 1156.0445
Epoch [ 297/450] -> Loss: 1152.1598
Epoch [ 298/450] -> Loss: 1151.9329
Epoch [ 299/450] -> Loss: 1152.0775
--------------------------------------------------
Model checkpoint saved as FFNN_300.pth
--------------------------------------------------
Epoch [ 300/450] -> Loss: 1152.2624
Epoch [ 301/450] -> Loss: 1151.4574
Epoch [ 302/450] -> Loss: 1150.5338
Epoch [ 303/450] -> Loss: 1151.2896
Epoch [ 304/450] -> Loss: 1151.3171
Epoch [ 305/450] -> Loss: 1152.0883
Epoch [ 306/450] -> Loss: 1151.1862
Epoch [ 307/450] -> Loss: 1151.8044
Epoch [ 308/450] -> Loss: 1152.4999
Epoch [ 309/450] -> Loss: 1151.2971
Epoch [ 310/450] -> Loss: 1151.1626
Epoch [ 311/450] -> Loss: 1151.4037
Epoch [ 312/450] -> Loss: 1150.4610
Epoch [ 313/450] -> Loss: 1148.9550
Epoch [ 314/450] -> Loss: 1151.0251
Epoch [ 315/450] -> Loss: 1150.4469
Epoch [ 316/450] -> Loss: 1150.6635
Epoch [ 317/450] -> Loss: 1149.9726
Epoch [ 318/450] -> Loss: 1151.1382
Epoch [ 319/450] -> Loss: 1148.8640
Epoch [ 320/450] -> Loss: 1149.9516
Epoch [ 321/450] -> Loss: 1150.3743
Epoch [ 322/450] -> Loss: 1149.9445
Epoch [ 323/450] -> Loss: 1149.5041
Epoch   323: reducing learning rate of group 0 to 1.2500e-04.
Epoch [ 324/450] -> Loss: 1148.9766
Epoch [ 325/450] -> Loss: 1149.7586
Epoch [ 326/450] -> Loss: 1148.3873
Epoch [ 327/450] -> Loss: 1148.2881
Epoch [ 328/450] -> Loss: 1148.0871
Epoch [ 329/450] -> Loss: 1148.1163
Epoch [ 330/450] -> Loss: 1147.7438
Epoch [ 331/450] -> Loss: 1147.0153
Epoch [ 332/450] -> Loss: 1148.1103
Epoch [ 333/450] -> Loss: 1148.3671
Epoch [ 334/450] -> Loss: 1147.8453
Epoch [ 335/450] -> Loss: 1146.9163
Epoch [ 336/450] -> Loss: 1147.8991
Epoch [ 337/450] -> Loss: 1148.1166
Epoch [ 338/450] -> Loss: 1147.7706
Epoch [ 339/450] -> Loss: 1148.0162
Epoch [ 340/450] -> Loss: 1147.4613
Epoch [ 341/450] -> Loss: 1147.4692
Epoch   341: reducing learning rate of group 0 to 6.2500e-05.
Epoch [ 342/450] -> Loss: 1147.5674
Epoch [ 343/450] -> Loss: 1146.7706
Epoch [ 344/450] -> Loss: 1146.6374
Epoch [ 345/450] -> Loss: 1146.5780
Epoch [ 346/450] -> Loss: 1146.6804
Epoch [ 347/450] -> Loss: 1146.7329
Epoch [ 348/450] -> Loss: 1146.5659
Epoch [ 349/450] -> Loss: 1146.6425
--------------------------------------------------
Model checkpoint saved as FFNN_350.pth
--------------------------------------------------
Epoch [ 350/450] -> Loss: 1146.7089
Epoch [ 351/450] -> Loss: 1146.3925
Epoch [ 352/450] -> Loss: 1146.6925
Epoch [ 353/450] -> Loss: 1146.3684
Epoch [ 354/450] -> Loss: 1146.5881
Epoch [ 355/450] -> Loss: 1146.4153
Epoch [ 356/450] -> Loss: 1146.2678
Epoch [ 357/450] -> Loss: 1146.3996
Epoch [ 358/450] -> Loss: 1146.3917
Epoch [ 359/450] -> Loss: 1146.4365
Epoch [ 360/450] -> Loss: 1146.2887
Epoch [ 361/450] -> Loss: 1146.2930
Epoch [ 362/450] -> Loss: 1146.4160
Epoch [ 363/450] -> Loss: 1146.4237
Epoch [ 364/450] -> Loss: 1146.3930
Epoch [ 365/450] -> Loss: 1145.8525
Epoch [ 366/450] -> Loss: 1146.2679
Epoch [ 367/450] -> Loss: 1146.1458
Epoch [ 368/450] -> Loss: 1146.2778
Epoch [ 369/450] -> Loss: 1145.9143
Epoch [ 370/450] -> Loss: 1146.3347
Epoch [ 371/450] -> Loss: 1146.0747
Epoch [ 372/450] -> Loss: 1146.0107
Epoch [ 373/450] -> Loss: 1146.0127
Epoch [ 374/450] -> Loss: 1146.1138
Epoch [ 375/450] -> Loss: 1145.9171
Epoch   375: reducing learning rate of group 0 to 3.1250e-05.
Epoch [ 376/450] -> Loss: 1146.1435
Epoch [ 377/450] -> Loss: 1145.5466
Epoch [ 378/450] -> Loss: 1145.5647
Epoch [ 379/450] -> Loss: 1145.5270
Epoch [ 380/450] -> Loss: 1145.4720
Epoch [ 381/450] -> Loss: 1145.5189
Epoch [ 382/450] -> Loss: 1145.5237
Epoch [ 383/450] -> Loss: 1145.4856
Epoch [ 384/450] -> Loss: 1145.5078
Epoch [ 385/450] -> Loss: 1145.4409
Epoch [ 386/450] -> Loss: 1145.4566
Epoch [ 387/450] -> Loss: 1145.4108
Epoch [ 388/450] -> Loss: 1145.4687
Epoch [ 389/450] -> Loss: 1145.4638
Epoch [ 390/450] -> Loss: 1145.4280
Epoch [ 391/450] -> Loss: 1145.4076
Epoch [ 392/450] -> Loss: 1145.3430
Epoch [ 393/450] -> Loss: 1145.3391
Epoch [ 394/450] -> Loss: 1145.3430
Epoch [ 395/450] -> Loss: 1145.3446
Epoch [ 396/450] -> Loss: 1145.2118
Epoch [ 397/450] -> Loss: 1145.2533
Epoch [ 398/450] -> Loss: 1145.2653
Epoch [ 399/450] -> Loss: 1145.2801
--------------------------------------------------
Model checkpoint saved as FFNN_400.pth
--------------------------------------------------
Epoch [ 400/450] -> Loss: 1145.2982
Epoch [ 401/450] -> Loss: 1145.3668
Epoch [ 402/450] -> Loss: 1145.2420
Epoch [ 403/450] -> Loss: 1145.1678
Epoch [ 404/450] -> Loss: 1145.2358
Epoch [ 405/450] -> Loss: 1145.1173
Epoch [ 406/450] -> Loss: 1145.1471
Epoch   406: reducing learning rate of group 0 to 1.5625e-05.
Epoch [ 407/450] -> Loss: 1145.1078
Epoch [ 408/450] -> Loss: 1145.0331
Epoch [ 409/450] -> Loss: 1145.0236
Epoch [ 410/450] -> Loss: 1144.9663
Epoch [ 411/450] -> Loss: 1144.9350
Epoch [ 412/450] -> Loss: 1144.9568
Epoch [ 413/450] -> Loss: 1144.9952
Epoch [ 414/450] -> Loss: 1144.9023
Epoch [ 415/450] -> Loss: 1145.0037
Epoch [ 416/450] -> Loss: 1144.9376
Epoch [ 417/450] -> Loss: 1144.9015
Epoch [ 418/450] -> Loss: 1144.9551
Epoch [ 419/450] -> Loss: 1144.9401
Epoch [ 420/450] -> Loss: 1144.9425
Epoch [ 421/450] -> Loss: 1144.8525
Epoch [ 422/450] -> Loss: 1144.8786
Epoch [ 423/450] -> Loss: 1144.9006
Epoch [ 424/450] -> Loss: 1144.8725
Epoch   424: reducing learning rate of group 0 to 7.8125e-06.
Epoch [ 425/450] -> Loss: 1144.8784
Epoch [ 426/450] -> Loss: 1144.7592
Epoch [ 427/450] -> Loss: 1144.8082
Epoch [ 428/450] -> Loss: 1144.7693
Epoch [ 429/450] -> Loss: 1144.7479
Epoch [ 430/450] -> Loss: 1144.7763
Epoch [ 431/450] -> Loss: 1144.7824
Epoch [ 432/450] -> Loss: 1144.7441
Epoch [ 433/450] -> Loss: 1144.7332
Epoch [ 434/450] -> Loss: 1144.7463
Epoch [ 435/450] -> Loss: 1144.7263
Epoch [ 436/450] -> Loss: 1144.7412
Epoch   436: reducing learning rate of group 0 to 3.9063e-06.
Epoch [ 437/450] -> Loss: 1144.7342
Epoch [ 438/450] -> Loss: 1144.6743
Epoch [ 439/450] -> Loss: 1144.6652
Epoch [ 440/450] -> Loss: 1144.6646
Epoch [ 441/450] -> Loss: 1144.6701
Epoch [ 442/450] -> Loss: 1144.6540
Epoch [ 443/450] -> Loss: 1144.6656
Epoch [ 444/450] -> Loss: 1144.6615
Epoch [ 445/450] -> Loss: 1144.6643
Epoch [ 446/450] -> Loss: 1144.6649
Epoch [ 447/450] -> Loss: 1144.6490
Epoch   447: reducing learning rate of group 0 to 1.9531e-06.
Epoch [ 448/450] -> Loss: 1144.6548
Epoch [ 449/450] -> Loss: 1144.6330
--------------------------------------------------
Model checkpoint saved as FFNN_450.pth
--------------------------------------------------
Epoch [ 450/450] -> Loss: 1144.6347
--------------------------------------------------
Training finished successfully.
        Saved model checkpoints can be found in: /home/extern/Documents/Research/scripts/models/
        Saved data/loss graphs can be found in: /home/extern/Documents/Research/scripts/graphs/
