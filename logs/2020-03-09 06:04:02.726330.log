--------------------------------------------------
Code running on device: cuda
{'start_date': [[1749, 1], [1754, 12], [1765, 12], [1774, 12], [1784, 1], [1797, 10], [1810, 1], [1822, 8], [1833, 5], [1843, 4], [1855, 8], [1866, 9], [1878, 4], [1889, 5], [1901, 4], [1912, 11], [1922, 11], [1933, 5], [1943, 9], [1953, 11], [1964, 4], [1975, 12], [1985, 12], [1996, 1], [2008, 6]], 'end_date': [[1754, 11], [1765, 11], [1774, 11], [1784, 0], [1797, 9], [1810, 0], [1822, 7], [1833, 4], [1843, 3], [1855, 7], [1866, 8], [1878, 3], [1889, 4], [1901, 3], [1912, 10], [1922, 10], [1933, 4], [1943, 8], [1953, 10], [1964, 3], [1975, 11], [1985, 11], [1996, 0], [2008, 5], [2018, 11]], 'max_date': [[1749, 8], [1760, 12], [1769, 5], [1777, 12], [1787, 7], [1793, 1], [1804, 5], [1815, 10], [1829, 8], [1836, 8], [1848, 1], [1859, 8], [1870, 3], [1883, 6], [1893, 4], [1905, 5], [1917, 3], [1927, 12], [1937, 1], [1947, 2], [1957, 6], [1968, 4], [1979, 6], [1989, 3], [2001, 6]], 'solar_max': [145.65, 133.7625, 183.25833333333333, 251.97083333333333, 228.33333333333334, 77.82083333333333, 80.0125, 78.07083333333333, 114.98750000000001, 234.6583333333334, 212.1625, 181.1291666666667, 224.8125, 117.43750000000001, 142.17083333333335, 102.50416666666666, 164.82499999999996, 124.27083333333333, 188.7083333333333, 210.125, 278.51666666666665, 152.92916666666665, 225.00000000000003, 207.76250000000002, 175.82916666666665], 'length': [70, 131, 107, 108, 164, 146, 150, 128, 118, 147, 132, 138, 132, 142, 138, 119, 125, 123, 121, 124, 139, 119, 120, 148, 125]}
--------------------------------------------------
File location :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_ms_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
No pre-trained models available, initializing model weights
--------------------------------------------------
Training model with: num_epochs=1800, start_lr=0.0005
Epoch [   1/1800] -> Loss: 12061.4748
Epoch [   2/1800] -> Loss: 7357.9399
Epoch [   3/1800] -> Loss: 2620.6608
Epoch [   4/1800] -> Loss: 1924.1832
Epoch [   5/1800] -> Loss: 1911.5686
Epoch [   6/1800] -> Loss: 1892.3728
Epoch [   7/1800] -> Loss: 1882.9922
Epoch [   8/1800] -> Loss: 1872.6691
Epoch [   9/1800] -> Loss: 1865.1113
Epoch [  10/1800] -> Loss: 1878.4465
Epoch [  11/1800] -> Loss: 1883.9233
Epoch [  12/1800] -> Loss: 1842.3877
Epoch [  13/1800] -> Loss: 1868.4926
Epoch [  14/1800] -> Loss: 1829.1843
Epoch [  15/1800] -> Loss: 1832.9680
Epoch [  16/1800] -> Loss: 1819.9490
Epoch [  17/1800] -> Loss: 1815.2688
Epoch [  18/1800] -> Loss: 1814.2748
Epoch [  19/1800] -> Loss: 1793.7312
Epoch [  20/1800] -> Loss: 1797.1522
Epoch [  21/1800] -> Loss: 1783.5678
Epoch [  22/1800] -> Loss: 1778.0600
Epoch [  23/1800] -> Loss: 1767.2815
Epoch [  24/1800] -> Loss: 1763.3964
Epoch [  25/1800] -> Loss: 1734.0620
Epoch [  26/1800] -> Loss: 1722.6449
Epoch [  27/1800] -> Loss: 1720.4580
Epoch [  28/1800] -> Loss: 1697.9675
Epoch [  29/1800] -> Loss: 1689.2405
Epoch [  30/1800] -> Loss: 1674.5914
Epoch [  31/1800] -> Loss: 1660.6844
Epoch [  32/1800] -> Loss: 1646.4664
Epoch [  33/1800] -> Loss: 1633.1844
Epoch [  34/1800] -> Loss: 1612.5339
Epoch [  35/1800] -> Loss: 1603.0260
Epoch [  36/1800] -> Loss: 1602.3442
Epoch [  37/1800] -> Loss: 1573.2697
Epoch [  38/1800] -> Loss: 1565.1642
Epoch [  39/1800] -> Loss: 1539.1000
Epoch [  40/1800] -> Loss: 1526.0257
Epoch [  41/1800] -> Loss: 1510.2617
Epoch [  42/1800] -> Loss: 1498.3477
Epoch [  43/1800] -> Loss: 1480.2058
Epoch [  44/1800] -> Loss: 1461.7734
Epoch [  45/1800] -> Loss: 1452.1993
Epoch [  46/1800] -> Loss: 1436.6370
Epoch [  47/1800] -> Loss: 1488.5711
Epoch [  48/1800] -> Loss: 1408.3835
Epoch [  49/1800] -> Loss: 1397.6625
Epoch [  50/1800] -> Loss: 1381.8179
Epoch [  51/1800] -> Loss: 1371.5112
Epoch [  52/1800] -> Loss: 1369.6935
Epoch [  53/1800] -> Loss: 1347.7145
Epoch [  54/1800] -> Loss: 1336.3288
Epoch [  55/1800] -> Loss: 1328.2913
Epoch [  56/1800] -> Loss: 1317.4654
Epoch [  57/1800] -> Loss: 1303.1899
Epoch [  58/1800] -> Loss: 1318.7814
Epoch [  59/1800] -> Loss: 1306.9911
Epoch [  60/1800] -> Loss: 1280.4904
Epoch [  61/1800] -> Loss: 1272.4521
Epoch [  62/1800] -> Loss: 1285.2497
Epoch [  63/1800] -> Loss: 1255.6118
Epoch [  64/1800] -> Loss: 1246.3468
Epoch [  65/1800] -> Loss: 1238.8931
Epoch [  66/1800] -> Loss: 1231.4242
Epoch [  67/1800] -> Loss: 1223.2625
Epoch [  68/1800] -> Loss: 1260.8229
Epoch [  69/1800] -> Loss: 1207.4742
Epoch [  70/1800] -> Loss: 1204.8168
Epoch [  71/1800] -> Loss: 1194.3276
Epoch [  72/1800] -> Loss: 1186.9207
Epoch [  73/1800] -> Loss: 1186.5877
Epoch [  74/1800] -> Loss: 1177.8173
Epoch [  75/1800] -> Loss: 1167.8588
Epoch [  76/1800] -> Loss: 1172.9905
Epoch [  77/1800] -> Loss: 1165.2541
Epoch [  78/1800] -> Loss: 1154.0572
Epoch [  79/1800] -> Loss: 1151.9286
Epoch [  80/1800] -> Loss: 1141.9707
Epoch [  81/1800] -> Loss: 1139.4577
Epoch [  82/1800] -> Loss: 1130.0913
Epoch [  83/1800] -> Loss: 1123.5623
Epoch [  84/1800] -> Loss: 1128.3570
Epoch [  85/1800] -> Loss: 1114.3760
Epoch [  86/1800] -> Loss: 1113.8617
Epoch [  87/1800] -> Loss: 1103.8573
Epoch [  88/1800] -> Loss: 1108.0914
Epoch [  89/1800] -> Loss: 1098.7488
Epoch [  90/1800] -> Loss: 1099.1801
Epoch [  91/1800] -> Loss: 1092.1517
Epoch [  92/1800] -> Loss: 1093.4650
Epoch [  93/1800] -> Loss: 1079.6915
Epoch [  94/1800] -> Loss: 1082.7413
Epoch [  95/1800] -> Loss: 1077.3726
Epoch [  96/1800] -> Loss: 1070.1968
Epoch [  97/1800] -> Loss: 1082.0801
Epoch [  98/1800] -> Loss: 1066.5885
Epoch [  99/1800] -> Loss: 1054.5688
--------------------------------------------------
Model checkpoint saved as FFNN_100.pth
--------------------------------------------------
Epoch [ 100/1800] -> Loss: 1060.0657
Epoch [ 101/1800] -> Loss: 1061.9958
Epoch [ 102/1800] -> Loss: 1048.7402
Epoch [ 103/1800] -> Loss: 1063.6751
Epoch [ 104/1800] -> Loss: 1045.9873
Epoch [ 105/1800] -> Loss: 1038.0764
Epoch [ 106/1800] -> Loss: 1038.7395
Epoch [ 107/1800] -> Loss: 1039.1930
Epoch [ 108/1800] -> Loss: 1037.6993
Epoch [ 109/1800] -> Loss: 1031.2212
Epoch [ 110/1800] -> Loss: 1035.8219
Epoch [ 111/1800] -> Loss: 1022.6379
Epoch [ 112/1800] -> Loss: 1010.1433
Epoch [ 113/1800] -> Loss: 1024.2888
Epoch [ 114/1800] -> Loss: 1017.8834
Epoch [ 115/1800] -> Loss: 1021.3458
Epoch [ 116/1800] -> Loss: 1013.1555
Epoch [ 117/1800] -> Loss: 1024.3344
Epoch [ 118/1800] -> Loss: 1008.6063
Epoch [ 119/1800] -> Loss: 1006.3387
Epoch [ 120/1800] -> Loss: 1012.3402
Epoch [ 121/1800] -> Loss: 1008.1562
Epoch [ 122/1800] -> Loss: 1004.6070
Epoch [ 123/1800] -> Loss: 1000.6418
Epoch [ 124/1800] -> Loss: 996.5510
Epoch [ 125/1800] -> Loss: 1010.5289
Epoch [ 126/1800] -> Loss: 999.0458
Epoch [ 127/1800] -> Loss: 991.2726
Epoch [ 128/1800] -> Loss: 996.4522
Epoch [ 129/1800] -> Loss: 989.2265
Epoch [ 130/1800] -> Loss: 1005.6930
Epoch [ 131/1800] -> Loss: 998.5670
Epoch [ 132/1800] -> Loss: 990.4954
Epoch [ 133/1800] -> Loss: 986.4244
Epoch [ 134/1800] -> Loss: 985.0803
Epoch [ 135/1800] -> Loss: 977.6162
Epoch [ 136/1800] -> Loss: 993.6333
Epoch [ 137/1800] -> Loss: 980.6612
Epoch [ 138/1800] -> Loss: 980.1944
Epoch [ 139/1800] -> Loss: 987.1228
Epoch [ 140/1800] -> Loss: 982.8681
Epoch [ 141/1800] -> Loss: 976.3291
Epoch [ 142/1800] -> Loss: 977.8754
Epoch [ 143/1800] -> Loss: 977.8966
Epoch [ 144/1800] -> Loss: 976.4783
Epoch [ 145/1800] -> Loss: 967.6641
Epoch [ 146/1800] -> Loss: 975.2750
Epoch [ 147/1800] -> Loss: 968.8817
Epoch [ 148/1800] -> Loss: 979.5772
Epoch [ 149/1800] -> Loss: 973.1539
Epoch [ 150/1800] -> Loss: 966.3901
Epoch [ 151/1800] -> Loss: 973.9865
Epoch [ 152/1800] -> Loss: 972.9779
Epoch [ 153/1800] -> Loss: 965.0564
Epoch [ 154/1800] -> Loss: 969.7547
Epoch [ 155/1800] -> Loss: 966.8342
Epoch [ 156/1800] -> Loss: 964.1866
Epoch [ 157/1800] -> Loss: 966.5637
Epoch [ 158/1800] -> Loss: 961.6678
Epoch [ 159/1800] -> Loss: 969.9607
Epoch [ 160/1800] -> Loss: 964.3354
Epoch [ 161/1800] -> Loss: 960.7507
Epoch [ 162/1800] -> Loss: 982.3026
Epoch [ 163/1800] -> Loss: 971.8748
Epoch [ 164/1800] -> Loss: 967.9527
Epoch [ 165/1800] -> Loss: 969.6352
Epoch [ 166/1800] -> Loss: 957.6468
Epoch [ 167/1800] -> Loss: 970.1910
Epoch [ 168/1800] -> Loss: 960.1746
Epoch [ 169/1800] -> Loss: 963.3721
Epoch [ 170/1800] -> Loss: 960.3003
Epoch [ 171/1800] -> Loss: 954.6426
Epoch [ 172/1800] -> Loss: 961.1407
Epoch [ 173/1800] -> Loss: 959.4739
Epoch [ 174/1800] -> Loss: 958.9585
Epoch [ 175/1800] -> Loss: 958.3707
Epoch [ 176/1800] -> Loss: 955.9058
Epoch [ 177/1800] -> Loss: 969.2839
Epoch [ 178/1800] -> Loss: 958.6364
Epoch [ 179/1800] -> Loss: 958.2390
Epoch [ 180/1800] -> Loss: 959.5863
Epoch [ 181/1800] -> Loss: 958.8985
Epoch   182: reducing learning rate of group 0 to 2.5000e-04.
Epoch [ 182/1800] -> Loss: 961.4185
Epoch [ 183/1800] -> Loss: 947.4899
Epoch [ 184/1800] -> Loss: 950.3948
Epoch [ 185/1800] -> Loss: 952.6060
Epoch [ 186/1800] -> Loss: 951.8840
Epoch [ 187/1800] -> Loss: 953.1649
Epoch [ 188/1800] -> Loss: 955.3062
Epoch [ 189/1800] -> Loss: 950.9844
Epoch [ 190/1800] -> Loss: 960.3815
Epoch [ 191/1800] -> Loss: 954.0588
Epoch [ 192/1800] -> Loss: 964.0319
Epoch [ 193/1800] -> Loss: 953.2537
Epoch   194: reducing learning rate of group 0 to 1.2500e-04.
Epoch [ 194/1800] -> Loss: 953.1117
Epoch [ 195/1800] -> Loss: 949.1174
Epoch [ 196/1800] -> Loss: 948.1062
Epoch [ 197/1800] -> Loss: 949.5162
Epoch [ 198/1800] -> Loss: 946.3569
Epoch [ 199/1800] -> Loss: 950.3915
--------------------------------------------------
Model checkpoint saved as FFNN_200.pth
--------------------------------------------------
Epoch [ 200/1800] -> Loss: 947.5122
Epoch [ 201/1800] -> Loss: 970.8925
Epoch [ 202/1800] -> Loss: 962.4742
Epoch [ 203/1800] -> Loss: 948.5555
Epoch [ 204/1800] -> Loss: 949.4913
Epoch [ 205/1800] -> Loss: 947.5753
Epoch [ 206/1800] -> Loss: 949.2242
Epoch [ 207/1800] -> Loss: 954.2326
Epoch [ 208/1800] -> Loss: 947.7970
Epoch   209: reducing learning rate of group 0 to 6.2500e-05.
Epoch [ 209/1800] -> Loss: 948.4115
Epoch [ 210/1800] -> Loss: 946.4101
Epoch [ 211/1800] -> Loss: 947.2100
Epoch [ 212/1800] -> Loss: 946.1313
Epoch [ 213/1800] -> Loss: 954.1049
Epoch [ 214/1800] -> Loss: 990.8242
Epoch [ 215/1800] -> Loss: 953.8483
Epoch [ 216/1800] -> Loss: 947.3238
Epoch [ 217/1800] -> Loss: 949.7342
Epoch [ 218/1800] -> Loss: 955.2110
Epoch [ 219/1800] -> Loss: 946.7571
Epoch [ 220/1800] -> Loss: 946.7265
Epoch [ 221/1800] -> Loss: 953.7504
Epoch [ 222/1800] -> Loss: 953.4866
Epoch [ 223/1800] -> Loss: 945.3963
Epoch [ 224/1800] -> Loss: 945.1169
Epoch [ 225/1800] -> Loss: 944.1900
Epoch [ 226/1800] -> Loss: 946.4046
Epoch [ 227/1800] -> Loss: 950.7582
Epoch [ 228/1800] -> Loss: 946.4512
Epoch [ 229/1800] -> Loss: 945.8412
Epoch [ 230/1800] -> Loss: 949.4533
Epoch [ 231/1800] -> Loss: 949.1371
Epoch [ 232/1800] -> Loss: 946.9055
Epoch [ 233/1800] -> Loss: 945.2250
Epoch [ 234/1800] -> Loss: 944.5443
Epoch [ 235/1800] -> Loss: 948.3857
Epoch   236: reducing learning rate of group 0 to 3.1250e-05.
Epoch [ 236/1800] -> Loss: 944.7973
Epoch [ 237/1800] -> Loss: 944.2585
Epoch [ 238/1800] -> Loss: 944.7923
Epoch [ 239/1800] -> Loss: 944.1606
Epoch [ 240/1800] -> Loss: 945.6035
Epoch [ 241/1800] -> Loss: 944.4766
Epoch [ 242/1800] -> Loss: 944.5783
Epoch [ 243/1800] -> Loss: 944.9149
Epoch [ 244/1800] -> Loss: 944.2224
Epoch [ 245/1800] -> Loss: 948.8678
Epoch [ 246/1800] -> Loss: 948.5750
Epoch   247: reducing learning rate of group 0 to 1.5625e-05.
Epoch [ 247/1800] -> Loss: 945.3783
Epoch [ 248/1800] -> Loss: 945.0611
Epoch [ 249/1800] -> Loss: 945.7230
Epoch [ 250/1800] -> Loss: 962.4082
Epoch [ 251/1800] -> Loss: 944.4676
Epoch [ 252/1800] -> Loss: 944.2251
Epoch [ 253/1800] -> Loss: 944.1650
Epoch [ 254/1800] -> Loss: 943.8372
Epoch [ 255/1800] -> Loss: 943.8564
Epoch [ 256/1800] -> Loss: 943.8061
Epoch [ 257/1800] -> Loss: 943.9989
Epoch [ 258/1800] -> Loss: 946.6257
Epoch [ 259/1800] -> Loss: 952.7413
Epoch [ 260/1800] -> Loss: 943.8302
Epoch [ 261/1800] -> Loss: 956.3257
Epoch [ 262/1800] -> Loss: 945.1130
Epoch [ 263/1800] -> Loss: 948.4710
Epoch [ 264/1800] -> Loss: 958.2055
Epoch   265: reducing learning rate of group 0 to 7.8125e-06.
Epoch [ 265/1800] -> Loss: 944.3261
Epoch [ 266/1800] -> Loss: 944.3975
Epoch [ 267/1800] -> Loss: 944.5436
Epoch [ 268/1800] -> Loss: 943.4606
Epoch [ 269/1800] -> Loss: 944.2896
Epoch [ 270/1800] -> Loss: 945.3983
Epoch [ 271/1800] -> Loss: 943.9274
Epoch [ 272/1800] -> Loss: 951.6511
Epoch [ 273/1800] -> Loss: 946.8837
Epoch [ 274/1800] -> Loss: 946.2322
Epoch [ 275/1800] -> Loss: 954.8669
Epoch [ 276/1800] -> Loss: 948.5193
Epoch [ 277/1800] -> Loss: 944.0973
Epoch [ 278/1800] -> Loss: 945.8827
Epoch   279: reducing learning rate of group 0 to 3.9063e-06.
Epoch [ 279/1800] -> Loss: 948.7574
Epoch [ 280/1800] -> Loss: 943.6073
Epoch [ 281/1800] -> Loss: 943.9151
Epoch [ 282/1800] -> Loss: 943.3310
Epoch [ 283/1800] -> Loss: 943.4344
Epoch [ 284/1800] -> Loss: 943.4258
Epoch [ 285/1800] -> Loss: 943.5364
Epoch [ 286/1800] -> Loss: 943.6041
Epoch [ 287/1800] -> Loss: 943.2995
Epoch [ 288/1800] -> Loss: 943.3003
Epoch [ 289/1800] -> Loss: 943.4139
Epoch [ 290/1800] -> Loss: 943.5541
Epoch [ 291/1800] -> Loss: 965.0906
Epoch [ 292/1800] -> Loss: 944.5558
Epoch   293: reducing learning rate of group 0 to 1.9531e-06.
Epoch [ 293/1800] -> Loss: 945.7820
Epoch [ 294/1800] -> Loss: 943.2438
Epoch [ 295/1800] -> Loss: 943.7097
Epoch [ 296/1800] -> Loss: 946.1623
Epoch [ 297/1800] -> Loss: 944.8925
Epoch [ 298/1800] -> Loss: 943.2631
Epoch [ 299/1800] -> Loss: 961.4700
--------------------------------------------------
Model checkpoint saved as FFNN_300.pth
--------------------------------------------------
Epoch [ 300/1800] -> Loss: 944.3139
Epoch [ 301/1800] -> Loss: 944.3595
Epoch [ 302/1800] -> Loss: 943.3043
Epoch [ 303/1800] -> Loss: 943.8753
Epoch   304: reducing learning rate of group 0 to 9.7656e-07.
Epoch [ 304/1800] -> Loss: 943.2974
Epoch [ 305/1800] -> Loss: 943.7182
Epoch [ 306/1800] -> Loss: 943.2964
Epoch [ 307/1800] -> Loss: 943.3934
Epoch [ 308/1800] -> Loss: 943.2232
Epoch [ 309/1800] -> Loss: 943.2420
Epoch [ 310/1800] -> Loss: 945.7671
Epoch [ 311/1800] -> Loss: 944.0845
Epoch [ 312/1800] -> Loss: 943.2724
Epoch [ 313/1800] -> Loss: 946.0381
Epoch [ 314/1800] -> Loss: 953.7466
Epoch [ 315/1800] -> Loss: 945.2351
Epoch [ 316/1800] -> Loss: 956.7281
Epoch [ 317/1800] -> Loss: 943.2391
Epoch [ 318/1800] -> Loss: 943.8841
Epoch   319: reducing learning rate of group 0 to 4.8828e-07.
Epoch [ 319/1800] -> Loss: 951.4975
Epoch [ 320/1800] -> Loss: 943.8615
Epoch [ 321/1800] -> Loss: 956.2762
Epoch [ 322/1800] -> Loss: 944.3269
Epoch [ 323/1800] -> Loss: 943.1860
Epoch [ 324/1800] -> Loss: 943.4001
Epoch [ 325/1800] -> Loss: 951.4735
Epoch [ 326/1800] -> Loss: 968.7638
Epoch [ 327/1800] -> Loss: 943.6017
Epoch [ 328/1800] -> Loss: 943.5288
Epoch [ 329/1800] -> Loss: 944.5656
Epoch   330: reducing learning rate of group 0 to 2.4414e-07.
Epoch [ 330/1800] -> Loss: 945.9840
Epoch [ 331/1800] -> Loss: 949.5961
Epoch [ 332/1800] -> Loss: 945.1115
Epoch [ 333/1800] -> Loss: 944.7923
Epoch [ 334/1800] -> Loss: 944.6696
Epoch [ 335/1800] -> Loss: 943.1770
Epoch [ 336/1800] -> Loss: 948.7798
Epoch [ 337/1800] -> Loss: 943.5345
Epoch [ 338/1800] -> Loss: 945.3388
Epoch [ 339/1800] -> Loss: 947.2324
Epoch [ 340/1800] -> Loss: 943.1750
Epoch   341: reducing learning rate of group 0 to 1.2207e-07.
Epoch [ 341/1800] -> Loss: 943.3088
Epoch [ 342/1800] -> Loss: 946.6036
Epoch [ 343/1800] -> Loss: 943.5483
Epoch [ 344/1800] -> Loss: 943.7445
Epoch [ 345/1800] -> Loss: 943.6487
Epoch [ 346/1800] -> Loss: 944.4668
Epoch [ 347/1800] -> Loss: 943.5853
Epoch [ 348/1800] -> Loss: 951.4684
Epoch [ 349/1800] -> Loss: 943.3428
Epoch [ 350/1800] -> Loss: 943.1739
Epoch [ 351/1800] -> Loss: 952.9247
Epoch   352: reducing learning rate of group 0 to 6.1035e-08.
Epoch [ 352/1800] -> Loss: 945.8887
Epoch [ 353/1800] -> Loss: 943.3549
Epoch [ 354/1800] -> Loss: 945.4327
Epoch [ 355/1800] -> Loss: 946.5688
Epoch [ 356/1800] -> Loss: 943.3220
Epoch [ 357/1800] -> Loss: 943.2215
Epoch [ 358/1800] -> Loss: 944.0876
Epoch [ 359/1800] -> Loss: 949.3615
Epoch [ 360/1800] -> Loss: 943.5516
Epoch [ 361/1800] -> Loss: 949.3612
Epoch [ 362/1800] -> Loss: 943.6339
Epoch   363: reducing learning rate of group 0 to 3.0518e-08.
Epoch [ 363/1800] -> Loss: 944.7604
Epoch [ 364/1800] -> Loss: 943.1753
Epoch [ 365/1800] -> Loss: 944.2576
Epoch [ 366/1800] -> Loss: 945.3372
Epoch [ 367/1800] -> Loss: 943.1837
Epoch [ 368/1800] -> Loss: 944.2987
Epoch [ 369/1800] -> Loss: 945.9061
Epoch [ 370/1800] -> Loss: 944.8246
Epoch [ 371/1800] -> Loss: 943.2199
Epoch [ 372/1800] -> Loss: 943.4445
Epoch [ 373/1800] -> Loss: 943.4579
Epoch   374: reducing learning rate of group 0 to 1.5259e-08.
Epoch [ 374/1800] -> Loss: 943.7572
Epoch [ 375/1800] -> Loss: 943.3999
Epoch [ 376/1800] -> Loss: 948.2729
Epoch [ 377/1800] -> Loss: 943.9757
Epoch [ 378/1800] -> Loss: 943.2853
Epoch [ 379/1800] -> Loss: 956.9297
Epoch [ 380/1800] -> Loss: 946.1787
Epoch [ 381/1800] -> Loss: 943.9431
Epoch [ 382/1800] -> Loss: 943.7663
Epoch [ 383/1800] -> Loss: 943.4077
Epoch [ 384/1800] -> Loss: 946.5489
Epoch [ 385/1800] -> Loss: 955.8649
Epoch [ 386/1800] -> Loss: 951.1050
Epoch [ 387/1800] -> Loss: 943.1713
Epoch [ 388/1800] -> Loss: 947.0858
Epoch [ 389/1800] -> Loss: 989.5765
Epoch [ 390/1800] -> Loss: 943.2963
Epoch [ 391/1800] -> Loss: 943.4610
Epoch [ 392/1800] -> Loss: 950.0430
Epoch [ 393/1800] -> Loss: 943.1700
Epoch [ 394/1800] -> Loss: 945.8558
Epoch [ 395/1800] -> Loss: 943.2082
Epoch [ 396/1800] -> Loss: 943.3800
Epoch [ 397/1800] -> Loss: 943.1885
Epoch [ 398/1800] -> Loss: 943.1666
Epoch [ 399/1800] -> Loss: 943.4092
--------------------------------------------------
Model checkpoint saved as FFNN_400.pth
--------------------------------------------------
Epoch [ 400/1800] -> Loss: 943.6459
Epoch [ 401/1800] -> Loss: 946.1113
Epoch [ 402/1800] -> Loss: 944.8242
Epoch [ 403/1800] -> Loss: 943.1668
Epoch [ 404/1800] -> Loss: 943.2313
Epoch [ 405/1800] -> Loss: 943.5514
Epoch [ 406/1800] -> Loss: 943.5339
Epoch [ 407/1800] -> Loss: 946.6871
Epoch [ 408/1800] -> Loss: 944.7034
Epoch [ 409/1800] -> Loss: 943.1819
Epoch [ 410/1800] -> Loss: 943.1681
Epoch [ 411/1800] -> Loss: 944.0949
Epoch [ 412/1800] -> Loss: 943.8417
Epoch [ 413/1800] -> Loss: 943.2205
Epoch [ 414/1800] -> Loss: 943.3790
Epoch [ 415/1800] -> Loss: 944.4101
Epoch [ 416/1800] -> Loss: 951.4200
Epoch [ 417/1800] -> Loss: 950.7698
Epoch [ 418/1800] -> Loss: 946.3760
Epoch [ 419/1800] -> Loss: 944.3309
Epoch [ 420/1800] -> Loss: 944.2706
Epoch [ 421/1800] -> Loss: 944.8092
Epoch [ 422/1800] -> Loss: 945.2785
Epoch [ 423/1800] -> Loss: 943.1995
Epoch [ 424/1800] -> Loss: 956.1314
Epoch [ 425/1800] -> Loss: 943.1920
Epoch [ 426/1800] -> Loss: 943.1747
Epoch [ 427/1800] -> Loss: 949.0332
Epoch [ 428/1800] -> Loss: 943.1830
Epoch [ 429/1800] -> Loss: 943.2845
Epoch [ 430/1800] -> Loss: 943.3355
Epoch [ 431/1800] -> Loss: 943.4344
Epoch [ 432/1800] -> Loss: 945.2380
Epoch [ 433/1800] -> Loss: 943.4819
Epoch [ 434/1800] -> Loss: 950.1874
Epoch [ 435/1800] -> Loss: 953.8306
Epoch [ 436/1800] -> Loss: 949.2110
Epoch [ 437/1800] -> Loss: 943.8526
Epoch [ 438/1800] -> Loss: 943.7550
Epoch [ 439/1800] -> Loss: 943.1769
Epoch [ 440/1800] -> Loss: 943.2789
