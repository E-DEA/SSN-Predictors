--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
File location :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
No pre-trained models available, initializing model weights
--------------------------------------------------
Training model with: num_epochs=2600, start_lr=0.0005
Epoch [   1/2600] -> Loss: 11105.9901
Epoch [   2/2600] -> Loss: 3774.5954
Epoch [   3/2600] -> Loss: 3081.2898
Epoch [   4/2600] -> Loss: 3068.3954
Epoch [   5/2600] -> Loss: 3048.3881
Epoch [   6/2600] -> Loss: 3042.1173
Epoch [   7/2600] -> Loss: 3024.1513
Epoch [   8/2600] -> Loss: 3010.2207
Epoch [   9/2600] -> Loss: 2998.3916
Epoch [  10/2600] -> Loss: 2983.6885
Epoch [  11/2600] -> Loss: 2969.1732
Epoch [  12/2600] -> Loss: 2958.0725
Epoch [  13/2600] -> Loss: 2935.1441
Epoch [  14/2600] -> Loss: 2924.2422
Epoch [  15/2600] -> Loss: 2898.1757
Epoch [  16/2600] -> Loss: 2880.0968
Epoch [  17/2600] -> Loss: 2850.1034
Epoch [  18/2600] -> Loss: 2826.7692
Epoch [  19/2600] -> Loss: 2798.1039
Epoch [  20/2600] -> Loss: 2776.4202
Epoch [  21/2600] -> Loss: 2734.4621
Epoch [  22/2600] -> Loss: 2705.0945
Epoch [  23/2600] -> Loss: 2668.9818
Epoch [  24/2600] -> Loss: 2627.2774
Epoch [  25/2600] -> Loss: 2591.7599
Epoch [  26/2600] -> Loss: 2555.9366
Epoch [  27/2600] -> Loss: 2517.2879
Epoch [  28/2600] -> Loss: 2470.2978
Epoch [  29/2600] -> Loss: 2440.4633
Epoch [  30/2600] -> Loss: 2395.7556
Epoch [  31/2600] -> Loss: 2362.4950
Epoch [  32/2600] -> Loss: 2318.7707
Epoch [  33/2600] -> Loss: 2286.3597
Epoch [  34/2600] -> Loss: 2253.3875
Epoch [  35/2600] -> Loss: 2220.5965
Epoch [  36/2600] -> Loss: 2182.5120
Epoch [  37/2600] -> Loss: 2151.1321
Epoch [  38/2600] -> Loss: 2119.6481
Epoch [  39/2600] -> Loss: 2088.4615
Epoch [  40/2600] -> Loss: 2074.1287
Epoch [  41/2600] -> Loss: 2040.1183
Epoch [  42/2600] -> Loss: 2020.7593
Epoch [  43/2600] -> Loss: 1993.9146
Epoch [  44/2600] -> Loss: 1986.6402
Epoch [  45/2600] -> Loss: 1956.9933
Epoch [  46/2600] -> Loss: 1939.6803
Epoch [  47/2600] -> Loss: 1922.3984
Epoch [  48/2600] -> Loss: 1910.9171
Epoch [  49/2600] -> Loss: 1891.3547
Epoch [  50/2600] -> Loss: 1878.6617
Epoch [  51/2600] -> Loss: 1868.8319
Epoch [  52/2600] -> Loss: 1850.9169
Epoch [  53/2600] -> Loss: 1839.2130
Epoch [  54/2600] -> Loss: 1832.4036
Epoch [  55/2600] -> Loss: 1820.4454
Epoch [  56/2600] -> Loss: 1809.9519
Epoch [  57/2600] -> Loss: 1800.2286
Epoch [  58/2600] -> Loss: 1789.9739
Epoch [  59/2600] -> Loss: 1777.4707
Epoch [  60/2600] -> Loss: 1770.3899
Epoch [  61/2600] -> Loss: 1766.9939
Epoch [  62/2600] -> Loss: 1759.2725
Epoch [  63/2600] -> Loss: 1754.8599
Epoch [  64/2600] -> Loss: 1742.4267
Epoch [  65/2600] -> Loss: 1736.2502
Epoch [  66/2600] -> Loss: 1734.7257
Epoch [  67/2600] -> Loss: 1729.3957
Epoch [  68/2600] -> Loss: 1719.5956
Epoch [  69/2600] -> Loss: 1723.8223
Epoch [  70/2600] -> Loss: 1706.9667
Epoch [  71/2600] -> Loss: 1696.1902
Epoch [  72/2600] -> Loss: 1691.4793
Epoch [  73/2600] -> Loss: 1692.0323
Epoch [  74/2600] -> Loss: 1686.8936
Epoch [  75/2600] -> Loss: 1680.5078
Epoch [  76/2600] -> Loss: 1675.4795
Epoch [  77/2600] -> Loss: 1668.5288
Epoch [  78/2600] -> Loss: 1658.6964
Epoch [  79/2600] -> Loss: 1658.3304
Epoch [  80/2600] -> Loss: 1656.7908
Epoch [  81/2600] -> Loss: 1656.5225
Epoch [  82/2600] -> Loss: 1652.6375
Epoch [  83/2600] -> Loss: 1652.2625
Epoch [  84/2600] -> Loss: 1645.1113
Epoch [  85/2600] -> Loss: 1634.0455
Epoch [  86/2600] -> Loss: 1639.0909
Epoch [  87/2600] -> Loss: 1648.4654
Epoch [  88/2600] -> Loss: 1636.2328
Epoch [  89/2600] -> Loss: 1634.0219
Epoch [  90/2600] -> Loss: 1625.4360
Epoch [  91/2600] -> Loss: 1626.2734
Epoch [  92/2600] -> Loss: 1621.8895
Epoch [  93/2600] -> Loss: 1628.0527
Epoch [  94/2600] -> Loss: 1630.6838
Epoch [  95/2600] -> Loss: 1611.0236
Epoch [  96/2600] -> Loss: 1618.7201
Epoch [  97/2600] -> Loss: 1612.4023
Epoch [  98/2600] -> Loss: 1615.5367
Epoch [  99/2600] -> Loss: 1612.4514
--------------------------------------------------
Model checkpoint saved as FFNN_100.pth
--------------------------------------------------
Epoch [ 100/2600] -> Loss: 1612.1523
Epoch [ 101/2600] -> Loss: 1605.6759
Epoch [ 102/2600] -> Loss: 1609.6139
Epoch [ 103/2600] -> Loss: 1601.4471
Epoch [ 104/2600] -> Loss: 1600.0061
Epoch [ 105/2600] -> Loss: 1599.9045
Epoch [ 106/2600] -> Loss: 1600.7882
Epoch [ 107/2600] -> Loss: 1603.2263
Epoch [ 108/2600] -> Loss: 1594.8287
Epoch [ 109/2600] -> Loss: 1588.3677
Epoch [ 110/2600] -> Loss: 1588.4967
Epoch [ 111/2600] -> Loss: 1586.3833
Epoch [ 112/2600] -> Loss: 1576.8860
Epoch [ 113/2600] -> Loss: 1597.3128
Epoch [ 114/2600] -> Loss: 1588.0799
Epoch [ 115/2600] -> Loss: 1580.0345
Epoch [ 116/2600] -> Loss: 1583.5630
Epoch [ 117/2600] -> Loss: 1583.5396
Epoch [ 118/2600] -> Loss: 1579.1269
Epoch [ 119/2600] -> Loss: 1578.2508
Epoch [ 120/2600] -> Loss: 1576.1380
Epoch [ 121/2600] -> Loss: 1579.3949
Epoch [ 122/2600] -> Loss: 1572.7069
Epoch [ 123/2600] -> Loss: 1578.2309
Epoch [ 124/2600] -> Loss: 1567.5993
Epoch [ 125/2600] -> Loss: 1586.9926
Epoch [ 126/2600] -> Loss: 1574.4662
Epoch [ 127/2600] -> Loss: 1575.8266
Epoch [ 128/2600] -> Loss: 1578.5140
Epoch [ 129/2600] -> Loss: 1573.3807
Epoch [ 130/2600] -> Loss: 1572.7378
Epoch [ 131/2600] -> Loss: 1569.0039
Epoch [ 132/2600] -> Loss: 1579.1728
Epoch [ 133/2600] -> Loss: 1574.2031
Epoch [ 134/2600] -> Loss: 1564.5786
Epoch [ 135/2600] -> Loss: 1577.3354
Epoch [ 136/2600] -> Loss: 1575.7873
Epoch [ 137/2600] -> Loss: 1574.6306
Epoch [ 138/2600] -> Loss: 1563.0316
Epoch [ 139/2600] -> Loss: 1571.6187
Epoch [ 140/2600] -> Loss: 1568.2677
Epoch [ 141/2600] -> Loss: 1566.3459
Epoch [ 142/2600] -> Loss: 1569.3310
Epoch [ 143/2600] -> Loss: 1564.6870
Epoch [ 144/2600] -> Loss: 1570.0506
Epoch [ 145/2600] -> Loss: 1565.4096
Epoch [ 146/2600] -> Loss: 1568.9792
Epoch [ 147/2600] -> Loss: 1565.5679
Epoch [ 148/2600] -> Loss: 1564.3385
Epoch [ 149/2600] -> Loss: 1560.4547
Epoch [ 150/2600] -> Loss: 1565.7995
Epoch [ 151/2600] -> Loss: 1566.0662
Epoch [ 152/2600] -> Loss: 1555.4297
Epoch [ 153/2600] -> Loss: 1558.7097
Epoch [ 154/2600] -> Loss: 1573.5624
Epoch [ 155/2600] -> Loss: 1554.5962
Epoch [ 156/2600] -> Loss: 1577.5356
