--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
Dataset source : NOAA, ISGI
File location :
    SSN - /home/extern/Documents/Research/data/NOAA/table_international-sunspot-numbers_monthly.txt
    AA - /home/extern/Documents/Research/data/ISGI/aa_1869-08-01_2017-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
No pre-trained models available, initializing model weights
--------------------------------------------------
Training model with: num_epochs=1800, start_lr=0.0005
Epoch [   1/1800] -> Loss: 2997.6175
Epoch [   2/1800] -> Loss: 1952.2287
Epoch [   3/1800] -> Loss: 1711.0700
Epoch [   4/1800] -> Loss: 1675.9175
Epoch [   5/1800] -> Loss: 1648.3308
Epoch [   6/1800] -> Loss: 1611.5120
Epoch [   7/1800] -> Loss: 1555.1114
Epoch [   8/1800] -> Loss: 1514.2810
Epoch [   9/1800] -> Loss: 1481.7315
Epoch [  10/1800] -> Loss: 1456.7044
Epoch [  11/1800] -> Loss: 1438.2093
Epoch [  12/1800] -> Loss: 1423.7554
Epoch [  13/1800] -> Loss: 1412.0758
Epoch [  14/1800] -> Loss: 1402.7190
Epoch [  15/1800] -> Loss: 1395.4023
Epoch [  16/1800] -> Loss: 1389.9831
Epoch [  17/1800] -> Loss: 1386.0183
Epoch [  18/1800] -> Loss: 1381.3549
Epoch [  19/1800] -> Loss: 1376.8506
Epoch [  20/1800] -> Loss: 1373.7328
Epoch [  21/1800] -> Loss: 1370.1076
Epoch [  22/1800] -> Loss: 1368.2563
Epoch [  23/1800] -> Loss: 1364.1388
Epoch [  24/1800] -> Loss: 1362.9556
Epoch [  25/1800] -> Loss: 1357.4729
Epoch [  26/1800] -> Loss: 1354.5324
Epoch [  27/1800] -> Loss: 1352.5766
Epoch [  28/1800] -> Loss: 1350.5087
Epoch [  29/1800] -> Loss: 1346.5254
Epoch [  30/1800] -> Loss: 1343.9976
Epoch [  31/1800] -> Loss: 1339.7900
Epoch [  32/1800] -> Loss: 1338.6237
Epoch [  33/1800] -> Loss: 1333.7105
Epoch [  34/1800] -> Loss: 1330.2148
Epoch [  35/1800] -> Loss: 1326.3265
Epoch [  36/1800] -> Loss: 1326.0514
Epoch [  37/1800] -> Loss: 1320.8295
Epoch [  38/1800] -> Loss: 1317.4952
Epoch [  39/1800] -> Loss: 1313.8602
Epoch [  40/1800] -> Loss: 1310.5913
Epoch [  41/1800] -> Loss: 1307.6989
Epoch [  42/1800] -> Loss: 1305.6552
Epoch [  43/1800] -> Loss: 1300.9266
Epoch [  44/1800] -> Loss: 1298.1642
Epoch [  45/1800] -> Loss: 1294.3134
Epoch [  46/1800] -> Loss: 1290.3298
Epoch [  47/1800] -> Loss: 1288.9412
Epoch [  48/1800] -> Loss: 1283.3045
Epoch [  49/1800] -> Loss: 1280.9428
--------------------------------------------------
Model checkpoint saved as FFNN_50.pth
--------------------------------------------------
Epoch [  50/1800] -> Loss: 1276.2847
Epoch [  51/1800] -> Loss: 1271.6935
Epoch [  52/1800] -> Loss: 1270.4951
Epoch [  53/1800] -> Loss: 1267.1018
Epoch [  54/1800] -> Loss: 1262.7496
Epoch [  55/1800] -> Loss: 1258.5822
Epoch [  56/1800] -> Loss: 1254.6826
Epoch [  57/1800] -> Loss: 1253.5079
Epoch [  58/1800] -> Loss: 1248.9623
Epoch [  59/1800] -> Loss: 1245.2704
Epoch [  60/1800] -> Loss: 1239.5001
Epoch [  61/1800] -> Loss: 1237.8049
Epoch [  62/1800] -> Loss: 1234.4381
Epoch [  63/1800] -> Loss: 1231.0839
Epoch [  64/1800] -> Loss: 1229.8260
Epoch [  65/1800] -> Loss: 1226.8061
Epoch [  66/1800] -> Loss: 1221.8573
Epoch [  67/1800] -> Loss: 1217.1184
Epoch [  68/1800] -> Loss: 1215.6022
Epoch [  69/1800] -> Loss: 1212.2126
Epoch [  70/1800] -> Loss: 1208.5278
Epoch [  71/1800] -> Loss: 1207.9086
Epoch [  72/1800] -> Loss: 1204.0946
Epoch [  73/1800] -> Loss: 1205.4671
Epoch [  74/1800] -> Loss: 1197.0745
Epoch [  75/1800] -> Loss: 1195.0963
Epoch [  76/1800] -> Loss: 1193.8233
Epoch [  77/1800] -> Loss: 1189.8915
Epoch [  78/1800] -> Loss: 1187.6893
Epoch [  79/1800] -> Loss: 1185.8140
Epoch [  80/1800] -> Loss: 1182.0786
Epoch [  81/1800] -> Loss: 1180.4146
Epoch [  82/1800] -> Loss: 1178.9761
Epoch [  83/1800] -> Loss: 1176.4527
Epoch [  84/1800] -> Loss: 1174.7425
Epoch [  85/1800] -> Loss: 1171.7571
Epoch [  86/1800] -> Loss: 1174.1945
Epoch [  87/1800] -> Loss: 1171.2535
Epoch [  88/1800] -> Loss: 1168.7199
Epoch [  89/1800] -> Loss: 1166.5558
Epoch [  90/1800] -> Loss: 1160.3405
Epoch [  91/1800] -> Loss: 1161.3092
Epoch [  92/1800] -> Loss: 1160.5892
Epoch [  93/1800] -> Loss: 1158.1003
Epoch [  94/1800] -> Loss: 1156.8118
Epoch [  95/1800] -> Loss: 1155.3213
Epoch [  96/1800] -> Loss: 1154.1982
Epoch [  97/1800] -> Loss: 1153.3005
Epoch [  98/1800] -> Loss: 1154.1807
Epoch [  99/1800] -> Loss: 1150.1838
--------------------------------------------------
Model checkpoint saved as FFNN_100.pth
--------------------------------------------------
Epoch [ 100/1800] -> Loss: 1149.2903
Epoch [ 101/1800] -> Loss: 1147.3741
Epoch [ 102/1800] -> Loss: 1146.0612
Epoch [ 103/1800] -> Loss: 1144.1239
Epoch [ 104/1800] -> Loss: 1145.6077
Epoch [ 105/1800] -> Loss: 1141.0614
Epoch [ 106/1800] -> Loss: 1141.0492
Epoch [ 107/1800] -> Loss: 1142.5877
Epoch [ 108/1800] -> Loss: 1139.9436
Epoch [ 109/1800] -> Loss: 1136.2874
Epoch [ 110/1800] -> Loss: 1138.5457
Epoch [ 111/1800] -> Loss: 1138.7430
Epoch [ 112/1800] -> Loss: 1138.6888
Epoch [ 113/1800] -> Loss: 1136.7537
Epoch [ 114/1800] -> Loss: 1139.0634
Epoch [ 115/1800] -> Loss: 1134.1400
Epoch [ 116/1800] -> Loss: 1133.7375
Epoch [ 117/1800] -> Loss: 1134.5776
Epoch [ 118/1800] -> Loss: 1134.3401
Epoch [ 119/1800] -> Loss: 1133.1853
Epoch [ 120/1800] -> Loss: 1132.8330
Epoch [ 121/1800] -> Loss: 1132.2505
Epoch [ 122/1800] -> Loss: 1131.4660
Epoch [ 123/1800] -> Loss: 1129.9171
Epoch [ 124/1800] -> Loss: 1131.7255
Epoch [ 125/1800] -> Loss: 1130.8234
Epoch [ 126/1800] -> Loss: 1130.0091
Epoch [ 127/1800] -> Loss: 1130.6863
Epoch [ 128/1800] -> Loss: 1127.7149
Epoch [ 129/1800] -> Loss: 1128.1839
Epoch [ 130/1800] -> Loss: 1128.6153
Epoch [ 131/1800] -> Loss: 1128.3482
Epoch [ 132/1800] -> Loss: 1128.0120
Epoch [ 133/1800] -> Loss: 1126.8433
Epoch [ 134/1800] -> Loss: 1128.5251
Epoch [ 135/1800] -> Loss: 1126.9664
Epoch [ 136/1800] -> Loss: 1127.2793
Epoch [ 137/1800] -> Loss: 1125.5800
Epoch [ 138/1800] -> Loss: 1125.3087
Epoch [ 139/1800] -> Loss: 1125.1056
Epoch [ 140/1800] -> Loss: 1125.1853
Epoch [ 141/1800] -> Loss: 1125.1820
Epoch [ 142/1800] -> Loss: 1125.4451
Epoch [ 143/1800] -> Loss: 1125.2030
Epoch [ 144/1800] -> Loss: 1125.1681
Epoch [ 145/1800] -> Loss: 1124.2992
Epoch [ 146/1800] -> Loss: 1124.6259
Epoch [ 147/1800] -> Loss: 1121.4344
Epoch [ 148/1800] -> Loss: 1123.8371
Epoch [ 149/1800] -> Loss: 1122.1592
--------------------------------------------------
Model checkpoint saved as FFNN_150.pth
--------------------------------------------------
Epoch [ 150/1800] -> Loss: 1120.3572
Epoch [ 151/1800] -> Loss: 1123.9674
Epoch [ 152/1800] -> Loss: 1125.9457
Epoch [ 153/1800] -> Loss: 1123.1744
Epoch [ 154/1800] -> Loss: 1125.0309
Epoch [ 155/1800] -> Loss: 1120.8752
Epoch [ 156/1800] -> Loss: 1123.6653
Epoch [ 157/1800] -> Loss: 1121.8647
Epoch [ 158/1800] -> Loss: 1122.7984
Epoch [ 159/1800] -> Loss: 1119.7592
Epoch [ 160/1800] -> Loss: 1124.6866
Epoch [ 161/1800] -> Loss: 1120.5608
Epoch [ 162/1800] -> Loss: 1120.5272
Epoch [ 163/1800] -> Loss: 1123.0668
Epoch [ 164/1800] -> Loss: 1118.2876
Epoch [ 165/1800] -> Loss: 1120.6879
Epoch [ 166/1800] -> Loss: 1119.6745
Epoch [ 167/1800] -> Loss: 1120.7001
Epoch [ 168/1800] -> Loss: 1119.6022
Epoch [ 169/1800] -> Loss: 1116.0718
Epoch [ 170/1800] -> Loss: 1122.2533
Epoch [ 171/1800] -> Loss: 1117.8104
Epoch [ 172/1800] -> Loss: 1116.7940
Epoch [ 173/1800] -> Loss: 1119.0373
Epoch [ 174/1800] -> Loss: 1118.6018
Epoch [ 175/1800] -> Loss: 1119.8749
Epoch [ 176/1800] -> Loss: 1117.7954
Epoch [ 177/1800] -> Loss: 1120.5381
Epoch [ 178/1800] -> Loss: 1117.7766
Epoch [ 179/1800] -> Loss: 1118.7021
Epoch   179: reducing learning rate of group 0 to 2.5000e-04.
Epoch [ 180/1800] -> Loss: 1118.1507
Epoch [ 181/1800] -> Loss: 1116.4848
Epoch [ 182/1800] -> Loss: 1116.4452
Epoch [ 183/1800] -> Loss: 1117.3813
Epoch [ 184/1800] -> Loss: 1116.6329
Epoch [ 185/1800] -> Loss: 1117.5982
Epoch [ 186/1800] -> Loss: 1115.8685
Epoch [ 187/1800] -> Loss: 1117.2072
Epoch [ 188/1800] -> Loss: 1116.5954
Epoch [ 189/1800] -> Loss: 1116.2510
Epoch [ 190/1800] -> Loss: 1116.3019
Epoch [ 191/1800] -> Loss: 1114.6523
Epoch [ 192/1800] -> Loss: 1116.0590
Epoch [ 193/1800] -> Loss: 1118.1770
Epoch [ 194/1800] -> Loss: 1117.4471
Epoch [ 195/1800] -> Loss: 1117.8059
Epoch [ 196/1800] -> Loss: 1116.6525
Epoch [ 197/1800] -> Loss: 1116.9385
Epoch [ 198/1800] -> Loss: 1115.3735
Epoch [ 199/1800] -> Loss: 1114.3533
--------------------------------------------------
Model checkpoint saved as FFNN_200.pth
--------------------------------------------------
Epoch [ 200/1800] -> Loss: 1116.3642
Epoch [ 201/1800] -> Loss: 1116.1263
Epoch [ 202/1800] -> Loss: 1117.0252
Epoch [ 203/1800] -> Loss: 1115.0721
Epoch [ 204/1800] -> Loss: 1115.0713
Epoch [ 205/1800] -> Loss: 1115.6614
Epoch [ 206/1800] -> Loss: 1115.7296
Epoch [ 207/1800] -> Loss: 1114.5097
Epoch [ 208/1800] -> Loss: 1115.9574
Epoch [ 209/1800] -> Loss: 1115.7794
Epoch   209: reducing learning rate of group 0 to 1.2500e-04.
Epoch [ 210/1800] -> Loss: 1114.8300
Epoch [ 211/1800] -> Loss: 1114.6842
Epoch [ 212/1800] -> Loss: 1114.3574
Epoch [ 213/1800] -> Loss: 1113.9415
Epoch [ 214/1800] -> Loss: 1114.0608
Epoch [ 215/1800] -> Loss: 1114.3214
Epoch [ 216/1800] -> Loss: 1114.3584
Epoch [ 217/1800] -> Loss: 1113.8866
Epoch [ 218/1800] -> Loss: 1114.0168
Epoch [ 219/1800] -> Loss: 1113.7934
Epoch [ 220/1800] -> Loss: 1113.8867
Epoch [ 221/1800] -> Loss: 1114.1938
Epoch [ 222/1800] -> Loss: 1113.7082
Epoch [ 223/1800] -> Loss: 1113.6962
Epoch [ 224/1800] -> Loss: 1113.4601
Epoch [ 225/1800] -> Loss: 1113.0741
Epoch [ 226/1800] -> Loss: 1113.9214
Epoch [ 227/1800] -> Loss: 1113.3917
Epoch [ 228/1800] -> Loss: 1113.4322
Epoch [ 229/1800] -> Loss: 1113.5471
Epoch [ 230/1800] -> Loss: 1113.5622
Epoch [ 231/1800] -> Loss: 1114.1702
Epoch [ 232/1800] -> Loss: 1114.4964
Epoch [ 233/1800] -> Loss: 1114.0176
Epoch [ 234/1800] -> Loss: 1114.2574
Epoch [ 235/1800] -> Loss: 1113.5756
Epoch   235: reducing learning rate of group 0 to 6.2500e-05.
Epoch [ 236/1800] -> Loss: 1113.1539
Epoch [ 237/1800] -> Loss: 1113.0582
Epoch [ 238/1800] -> Loss: 1112.8912
Epoch [ 239/1800] -> Loss: 1112.9358
Epoch [ 240/1800] -> Loss: 1112.8052
Epoch [ 241/1800] -> Loss: 1113.0400
Epoch [ 242/1800] -> Loss: 1112.7483
Epoch [ 243/1800] -> Loss: 1112.8539
Epoch [ 244/1800] -> Loss: 1112.5457
Epoch [ 245/1800] -> Loss: 1112.6031
Epoch [ 246/1800] -> Loss: 1112.8177
Epoch [ 247/1800] -> Loss: 1112.7994
Epoch [ 248/1800] -> Loss: 1112.7596
Epoch [ 249/1800] -> Loss: 1112.7621
--------------------------------------------------
Model checkpoint saved as FFNN_250.pth
--------------------------------------------------
Epoch [ 250/1800] -> Loss: 1113.2048
Epoch [ 251/1800] -> Loss: 1112.5119
Epoch [ 252/1800] -> Loss: 1112.6120
Epoch [ 253/1800] -> Loss: 1112.8175
Epoch [ 254/1800] -> Loss: 1112.6539
Epoch   254: reducing learning rate of group 0 to 3.1250e-05.
Epoch [ 255/1800] -> Loss: 1112.5586
Epoch [ 256/1800] -> Loss: 1112.3462
Epoch [ 257/1800] -> Loss: 1112.4867
Epoch [ 258/1800] -> Loss: 1112.3259
Epoch [ 259/1800] -> Loss: 1112.3580
Epoch [ 260/1800] -> Loss: 1112.2653
Epoch [ 261/1800] -> Loss: 1112.4753
Epoch [ 262/1800] -> Loss: 1112.4096
Epoch [ 263/1800] -> Loss: 1112.3993
Epoch [ 264/1800] -> Loss: 1112.2912
Epoch [ 265/1800] -> Loss: 1112.5478
Epoch [ 266/1800] -> Loss: 1112.2946
Epoch   266: reducing learning rate of group 0 to 1.5625e-05.
Epoch [ 267/1800] -> Loss: 1112.3966
Epoch [ 268/1800] -> Loss: 1112.1646
Epoch [ 269/1800] -> Loss: 1112.1431
Epoch [ 270/1800] -> Loss: 1112.1178
Epoch [ 271/1800] -> Loss: 1112.1735
Epoch [ 272/1800] -> Loss: 1112.1285
Epoch [ 273/1800] -> Loss: 1112.1367
Epoch [ 274/1800] -> Loss: 1112.2479
Epoch [ 275/1800] -> Loss: 1112.1671
Epoch [ 276/1800] -> Loss: 1112.1416
Epoch [ 277/1800] -> Loss: 1112.1499
Epoch [ 278/1800] -> Loss: 1112.1218
Epoch   278: reducing learning rate of group 0 to 7.8125e-06.
Epoch [ 279/1800] -> Loss: 1112.1071
Epoch [ 280/1800] -> Loss: 1112.0923
Epoch [ 281/1800] -> Loss: 1112.0852
Epoch [ 282/1800] -> Loss: 1112.0842
Epoch [ 283/1800] -> Loss: 1112.0589
Epoch [ 284/1800] -> Loss: 1112.0958
Epoch [ 285/1800] -> Loss: 1112.0472
Epoch [ 286/1800] -> Loss: 1112.0900
Epoch [ 287/1800] -> Loss: 1112.0501
Epoch [ 288/1800] -> Loss: 1112.0585
Epoch [ 289/1800] -> Loss: 1112.1323
Epoch [ 290/1800] -> Loss: 1112.0706
Epoch [ 291/1800] -> Loss: 1112.0373
Epoch [ 292/1800] -> Loss: 1112.0488
Epoch [ 293/1800] -> Loss: 1112.1293
Epoch [ 294/1800] -> Loss: 1112.0408
Epoch [ 295/1800] -> Loss: 1112.0161
Epoch   295: reducing learning rate of group 0 to 3.9063e-06.
Epoch [ 296/1800] -> Loss: 1112.0984
Epoch [ 297/1800] -> Loss: 1111.9937
Epoch [ 298/1800] -> Loss: 1112.0088
Epoch [ 299/1800] -> Loss: 1112.0104
--------------------------------------------------
Model checkpoint saved as FFNN_300.pth
--------------------------------------------------
Epoch [ 300/1800] -> Loss: 1112.0202
Epoch [ 301/1800] -> Loss: 1112.0038
Epoch [ 302/1800] -> Loss: 1111.9932
Epoch [ 303/1800] -> Loss: 1111.9940
Epoch [ 304/1800] -> Loss: 1111.9930
Epoch [ 305/1800] -> Loss: 1111.9942
Epoch [ 306/1800] -> Loss: 1112.0086
Epoch   306: reducing learning rate of group 0 to 1.9531e-06.
Epoch [ 307/1800] -> Loss: 1111.9943
Epoch [ 308/1800] -> Loss: 1111.9890
Epoch [ 309/1800] -> Loss: 1111.9749
Epoch [ 310/1800] -> Loss: 1111.9780
Epoch [ 311/1800] -> Loss: 1111.9679
Epoch [ 312/1800] -> Loss: 1111.9737
Epoch [ 313/1800] -> Loss: 1111.9814
Epoch [ 314/1800] -> Loss: 1111.9792
Epoch [ 315/1800] -> Loss: 1111.9712
Epoch [ 316/1800] -> Loss: 1111.9690
Epoch [ 317/1800] -> Loss: 1111.9729
Epoch   317: reducing learning rate of group 0 to 9.7656e-07.
Epoch [ 318/1800] -> Loss: 1111.9772
Epoch [ 319/1800] -> Loss: 1111.9642
Epoch [ 320/1800] -> Loss: 1111.9728
Epoch [ 321/1800] -> Loss: 1111.9653
Epoch [ 322/1800] -> Loss: 1111.9607
Epoch [ 323/1800] -> Loss: 1111.9621
Epoch [ 324/1800] -> Loss: 1111.9670
Epoch [ 325/1800] -> Loss: 1111.9688
Epoch [ 326/1800] -> Loss: 1111.9631
Epoch [ 327/1800] -> Loss: 1111.9628
Epoch [ 328/1800] -> Loss: 1111.9616
Epoch   328: reducing learning rate of group 0 to 4.8828e-07.
Epoch [ 329/1800] -> Loss: 1111.9612
Epoch [ 330/1800] -> Loss: 1111.9581
Epoch [ 331/1800] -> Loss: 1111.9598
Epoch [ 332/1800] -> Loss: 1111.9591
Epoch [ 333/1800] -> Loss: 1111.9581
Epoch [ 334/1800] -> Loss: 1111.9568
Epoch [ 335/1800] -> Loss: 1111.9586
Epoch [ 336/1800] -> Loss: 1111.9612
Epoch [ 337/1800] -> Loss: 1111.9566
Epoch [ 338/1800] -> Loss: 1111.9569
Epoch [ 339/1800] -> Loss: 1111.9562
Epoch   339: reducing learning rate of group 0 to 2.4414e-07.
Epoch [ 340/1800] -> Loss: 1111.9585
Epoch [ 341/1800] -> Loss: 1111.9568
Epoch [ 342/1800] -> Loss: 1111.9558
Epoch [ 343/1800] -> Loss: 1111.9551
Epoch [ 344/1800] -> Loss: 1111.9555
Epoch [ 345/1800] -> Loss: 1111.9552
Epoch [ 346/1800] -> Loss: 1111.9542
Epoch [ 347/1800] -> Loss: 1111.9550
Epoch [ 348/1800] -> Loss: 1111.9561
Epoch [ 349/1800] -> Loss: 1111.9547
--------------------------------------------------
Model checkpoint saved as FFNN_350.pth
--------------------------------------------------
Epoch [ 350/1800] -> Loss: 1111.9549
Epoch   350: reducing learning rate of group 0 to 1.2207e-07.
Epoch [ 351/1800] -> Loss: 1111.9570
Epoch [ 352/1800] -> Loss: 1111.9530
Epoch [ 353/1800] -> Loss: 1111.9533
Epoch [ 354/1800] -> Loss: 1111.9541
Epoch [ 355/1800] -> Loss: 1111.9536
Epoch [ 356/1800] -> Loss: 1111.9531
Epoch [ 357/1800] -> Loss: 1111.9534
Epoch [ 358/1800] -> Loss: 1111.9530
Epoch [ 359/1800] -> Loss: 1111.9532
Epoch [ 360/1800] -> Loss: 1111.9539
Epoch [ 361/1800] -> Loss: 1111.9532
Epoch   361: reducing learning rate of group 0 to 6.1035e-08.
Epoch [ 362/1800] -> Loss: 1111.9539
Epoch [ 363/1800] -> Loss: 1111.9525
Epoch [ 364/1800] -> Loss: 1111.9526
Epoch [ 365/1800] -> Loss: 1111.9525
Epoch [ 366/1800] -> Loss: 1111.9525
Epoch [ 367/1800] -> Loss: 1111.9530
Epoch [ 368/1800] -> Loss: 1111.9527
Epoch [ 369/1800] -> Loss: 1111.9529
Epoch [ 370/1800] -> Loss: 1111.9524
Epoch [ 371/1800] -> Loss: 1111.9532
Epoch [ 372/1800] -> Loss: 1111.9527
Epoch   372: reducing learning rate of group 0 to 3.0518e-08.
Epoch [ 373/1800] -> Loss: 1111.9526
Epoch [ 374/1800] -> Loss: 1111.9522
Epoch [ 375/1800] -> Loss: 1111.9523
Epoch [ 376/1800] -> Loss: 1111.9522
Epoch [ 377/1800] -> Loss: 1111.9523
Epoch [ 378/1800] -> Loss: 1111.9523
Epoch [ 379/1800] -> Loss: 1111.9523
Epoch [ 380/1800] -> Loss: 1111.9522
Epoch [ 381/1800] -> Loss: 1111.9523
Epoch [ 382/1800] -> Loss: 1111.9522
Epoch [ 383/1800] -> Loss: 1111.9523
Epoch   383: reducing learning rate of group 0 to 1.5259e-08.
Epoch [ 384/1800] -> Loss: 1111.9522
Epoch [ 385/1800] -> Loss: 1111.9522
Epoch [ 386/1800] -> Loss: 1111.9522
Epoch [ 387/1800] -> Loss: 1111.9522
Epoch [ 388/1800] -> Loss: 1111.9522
Epoch [ 389/1800] -> Loss: 1111.9521
Epoch [ 390/1800] -> Loss: 1111.9521
Epoch [ 391/1800] -> Loss: 1111.9521
Epoch [ 392/1800] -> Loss: 1111.9522
Epoch [ 393/1800] -> Loss: 1111.9521
Epoch [ 394/1800] -> Loss: 1111.9521
Epoch [ 395/1800] -> Loss: 1111.9521
Epoch [ 396/1800] -> Loss: 1111.9522
Epoch [ 397/1800] -> Loss: 1111.9521
Epoch [ 398/1800] -> Loss: 1111.9522
Epoch [ 399/1800] -> Loss: 1111.9521
--------------------------------------------------
Model checkpoint saved as FFNN_400.pth
--------------------------------------------------
Epoch [ 400/1800] -> Loss: 1111.9521
Epoch [ 401/1800] -> Loss: 1111.9522
Epoch [ 402/1800] -> Loss: 1111.9521
Epoch [ 403/1800] -> Loss: 1111.9521
Epoch [ 404/1800] -> Loss: 1111.9521
Epoch [ 405/1800] -> Loss: 1111.9521
Epoch [ 406/1800] -> Loss: 1111.9522
Epoch [ 407/1800] -> Loss: 1111.9521
Epoch [ 408/1800] -> Loss: 1111.9521
Epoch [ 409/1800] -> Loss: 1111.9522
Epoch [ 410/1800] -> Loss: 1111.9522
Epoch [ 411/1800] -> Loss: 1111.9522
Epoch [ 412/1800] -> Loss: 1111.9521
Epoch [ 413/1800] -> Loss: 1111.9522
Epoch [ 414/1800] -> Loss: 1111.9521
Epoch [ 415/1800] -> Loss: 1111.9521
Epoch [ 416/1800] -> Loss: 1111.9521
Epoch [ 417/1800] -> Loss: 1111.9521
Epoch [ 418/1800] -> Loss: 1111.9521
Epoch [ 419/1800] -> Loss: 1111.9521
Epoch [ 420/1800] -> Loss: 1111.9521
Epoch [ 421/1800] -> Loss: 1111.9521
Epoch [ 422/1800] -> Loss: 1111.9521
Epoch [ 423/1800] -> Loss: 1111.9521
Epoch [ 424/1800] -> Loss: 1111.9521
Epoch [ 425/1800] -> Loss: 1111.9522
Epoch [ 426/1800] -> Loss: 1111.9521
Epoch [ 427/1800] -> Loss: 1111.9522
Epoch [ 428/1800] -> Loss: 1111.9522
Epoch [ 429/1800] -> Loss: 1111.9521
Epoch [ 430/1800] -> Loss: 1111.9521
Epoch [ 431/1800] -> Loss: 1111.9521
Epoch [ 432/1800] -> Loss: 1111.9521
Epoch [ 433/1800] -> Loss: 1111.9522
Epoch [ 434/1800] -> Loss: 1111.9521
Epoch [ 435/1800] -> Loss: 1111.9521
Epoch [ 436/1800] -> Loss: 1111.9521
Epoch [ 437/1800] -> Loss: 1111.9521
Epoch [ 438/1800] -> Loss: 1111.9521
Epoch [ 439/1800] -> Loss: 1111.9521
Epoch [ 440/1800] -> Loss: 1111.9521
Epoch [ 441/1800] -> Loss: 1111.9521
Epoch [ 442/1800] -> Loss: 1111.9521
Epoch [ 443/1800] -> Loss: 1111.9521
Epoch [ 444/1800] -> Loss: 1111.9521
Epoch [ 445/1800] -> Loss: 1111.9522
Epoch [ 446/1800] -> Loss: 1111.9521
Epoch [ 447/1800] -> Loss: 1111.9522
Epoch [ 448/1800] -> Loss: 1111.9521
Epoch [ 449/1800] -> Loss: 1111.9521
--------------------------------------------------
Model checkpoint saved as FFNN_450.pth
--------------------------------------------------
Epoch [ 450/1800] -> Loss: 1111.9521
Epoch [ 451/1800] -> Loss: 1111.9521
Epoch [ 452/1800] -> Loss: 1111.9521
Epoch [ 453/1800] -> Loss: 1111.9521
Epoch [ 454/1800] -> Loss: 1111.9521
Epoch [ 455/1800] -> Loss: 1111.9521
Epoch [ 456/1800] -> Loss: 1111.9521
Epoch [ 457/1800] -> Loss: 1111.9521
Epoch [ 458/1800] -> Loss: 1111.9521
Epoch [ 459/1800] -> Loss: 1111.9521
Epoch [ 460/1800] -> Loss: 1111.9521
Epoch [ 461/1800] -> Loss: 1111.9522
Epoch [ 462/1800] -> Loss: 1111.9521
Epoch [ 463/1800] -> Loss: 1111.9521
Epoch [ 464/1800] -> Loss: 1111.9521
Epoch [ 465/1800] -> Loss: 1111.9521
Epoch [ 466/1800] -> Loss: 1111.9521
Epoch [ 467/1800] -> Loss: 1111.9521
Epoch [ 468/1800] -> Loss: 1111.9521
Epoch [ 469/1800] -> Loss: 1111.9521
Epoch [ 470/1800] -> Loss: 1111.9521
Epoch [ 471/1800] -> Loss: 1111.9521
Epoch [ 472/1800] -> Loss: 1111.9521
Epoch [ 473/1800] -> Loss: 1111.9521
Epoch [ 474/1800] -> Loss: 1111.9521
Epoch [ 475/1800] -> Loss: 1111.9521
Epoch [ 476/1800] -> Loss: 1111.9521
Epoch [ 477/1800] -> Loss: 1111.9521
Epoch [ 478/1800] -> Loss: 1111.9521
Epoch [ 479/1800] -> Loss: 1111.9521
Epoch [ 480/1800] -> Loss: 1111.9521
Epoch [ 481/1800] -> Loss: 1111.9521
Epoch [ 482/1800] -> Loss: 1111.9521
Epoch [ 483/1800] -> Loss: 1111.9521
Epoch [ 484/1800] -> Loss: 1111.9521
Epoch [ 485/1800] -> Loss: 1111.9521
Epoch [ 486/1800] -> Loss: 1111.9521
Epoch [ 487/1800] -> Loss: 1111.9521
Epoch [ 488/1800] -> Loss: 1111.9521
Epoch [ 489/1800] -> Loss: 1111.9521
Epoch [ 490/1800] -> Loss: 1111.9521
Epoch [ 491/1800] -> Loss: 1111.9521
Epoch [ 492/1800] -> Loss: 1111.9521
Epoch [ 493/1800] -> Loss: 1111.9521
Epoch [ 494/1800] -> Loss: 1111.9521
Epoch [ 495/1800] -> Loss: 1111.9522
Epoch [ 496/1800] -> Loss: 1111.9521
Epoch [ 497/1800] -> Loss: 1111.9521
Epoch [ 498/1800] -> Loss: 1111.9521
Epoch [ 499/1800] -> Loss: 1111.9522
--------------------------------------------------
Model checkpoint saved as FFNN_500.pth
--------------------------------------------------
Epoch [ 500/1800] -> Loss: 1111.9521
Epoch [ 501/1800] -> Loss: 1111.9521
Epoch [ 502/1800] -> Loss: 1111.9521
Epoch [ 503/1800] -> Loss: 1111.9521
Epoch [ 504/1800] -> Loss: 1111.9521
Epoch [ 505/1800] -> Loss: 1111.9521
Epoch [ 506/1800] -> Loss: 1111.9521
Epoch [ 507/1800] -> Loss: 1111.9521
Epoch [ 508/1800] -> Loss: 1111.9521
Epoch [ 509/1800] -> Loss: 1111.9521
Epoch [ 510/1800] -> Loss: 1111.9521
Epoch [ 511/1800] -> Loss: 1111.9521
Epoch [ 512/1800] -> Loss: 1111.9521
Epoch [ 513/1800] -> Loss: 1111.9521
Epoch [ 514/1800] -> Loss: 1111.9521
Epoch [ 515/1800] -> Loss: 1111.9521
Epoch [ 516/1800] -> Loss: 1111.9521
Epoch [ 517/1800] -> Loss: 1111.9521
Epoch [ 518/1800] -> Loss: 1111.9521
Epoch [ 519/1800] -> Loss: 1111.9521
Epoch [ 520/1800] -> Loss: 1111.9521
Epoch [ 521/1800] -> Loss: 1111.9521
Epoch [ 522/1800] -> Loss: 1111.9521
Epoch [ 523/1800] -> Loss: 1111.9521
Epoch [ 524/1800] -> Loss: 1111.9521
Epoch [ 525/1800] -> Loss: 1111.9521
Epoch [ 526/1800] -> Loss: 1111.9521
Epoch [ 527/1800] -> Loss: 1111.9521
Epoch [ 528/1800] -> Loss: 1111.9521
Epoch [ 529/1800] -> Loss: 1111.9521
Epoch [ 530/1800] -> Loss: 1111.9521
Epoch [ 531/1800] -> Loss: 1111.9521
Epoch [ 532/1800] -> Loss: 1111.9521
Epoch [ 533/1800] -> Loss: 1111.9521
Epoch [ 534/1800] -> Loss: 1111.9521
Epoch [ 535/1800] -> Loss: 1111.9521
Epoch [ 536/1800] -> Loss: 1111.9521
Epoch [ 537/1800] -> Loss: 1111.9521
Epoch [ 538/1800] -> Loss: 1111.9521
Epoch [ 539/1800] -> Loss: 1111.9521
Epoch [ 540/1800] -> Loss: 1111.9521
Epoch [ 541/1800] -> Loss: 1111.9521
Epoch [ 542/1800] -> Loss: 1111.9521
Epoch [ 543/1800] -> Loss: 1111.9521
Epoch [ 544/1800] -> Loss: 1111.9521
Epoch [ 545/1800] -> Loss: 1111.9521
Epoch [ 546/1800] -> Loss: 1111.9521
Epoch [ 547/1800] -> Loss: 1111.9521
Epoch [ 548/1800] -> Loss: 1111.9521
Epoch [ 549/1800] -> Loss: 1111.9521
--------------------------------------------------
Model checkpoint saved as FFNN_550.pth
--------------------------------------------------
Epoch [ 550/1800] -> Loss: 1111.9521
Epoch [ 551/1800] -> Loss: 1111.9521
Epoch [ 552/1800] -> Loss: 1111.9521
Epoch [ 553/1800] -> Loss: 1111.9521
Epoch [ 554/1800] -> Loss: 1111.9521
Epoch [ 555/1800] -> Loss: 1111.9521
Epoch [ 556/1800] -> Loss: 1111.9521
Epoch [ 557/1800] -> Loss: 1111.9521
Epoch [ 558/1800] -> Loss: 1111.9521
Epoch [ 559/1800] -> Loss: 1111.9521
Epoch [ 560/1800] -> Loss: 1111.9521
Epoch [ 561/1800] -> Loss: 1111.9521
Epoch [ 562/1800] -> Loss: 1111.9521
Epoch [ 563/1800] -> Loss: 1111.9521
Epoch [ 564/1800] -> Loss: 1111.9521
Epoch [ 565/1800] -> Loss: 1111.9521
Epoch [ 566/1800] -> Loss: 1111.9522
Epoch [ 567/1800] -> Loss: 1111.9521
Epoch [ 568/1800] -> Loss: 1111.9521
Epoch [ 569/1800] -> Loss: 1111.9521
Epoch [ 570/1800] -> Loss: 1111.9522
Epoch [ 571/1800] -> Loss: 1111.9521
Epoch [ 572/1800] -> Loss: 1111.9521
Epoch [ 573/1800] -> Loss: 1111.9521
Epoch [ 574/1800] -> Loss: 1111.9521
Epoch [ 575/1800] -> Loss: 1111.9520
Epoch [ 576/1800] -> Loss: 1111.9521
Epoch [ 577/1800] -> Loss: 1111.9521
Epoch [ 578/1800] -> Loss: 1111.9521
Epoch [ 579/1800] -> Loss: 1111.9521
Epoch [ 580/1800] -> Loss: 1111.9521
Epoch [ 581/1800] -> Loss: 1111.9521
Epoch [ 582/1800] -> Loss: 1111.9521
Epoch [ 583/1800] -> Loss: 1111.9521
Epoch [ 584/1800] -> Loss: 1111.9521
Epoch [ 585/1800] -> Loss: 1111.9520
Epoch [ 586/1800] -> Loss: 1111.9521
Epoch [ 587/1800] -> Loss: 1111.9521
Epoch [ 588/1800] -> Loss: 1111.9521
Epoch [ 589/1800] -> Loss: 1111.9521
Epoch [ 590/1800] -> Loss: 1111.9520
Epoch [ 591/1800] -> Loss: 1111.9521
Epoch [ 592/1800] -> Loss: 1111.9521
Epoch [ 593/1800] -> Loss: 1111.9521
Epoch [ 594/1800] -> Loss: 1111.9521
Epoch [ 595/1800] -> Loss: 1111.9521
Epoch [ 596/1800] -> Loss: 1111.9521
Epoch [ 597/1800] -> Loss: 1111.9521
Epoch [ 598/1800] -> Loss: 1111.9521
Epoch [ 599/1800] -> Loss: 1111.9521
