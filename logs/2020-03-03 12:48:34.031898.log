--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
File location :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
No pre-trained models available, initializing model weights
--------------------------------------------------
Training model with: num_epochs=2600, start_lr=0.0005
Epoch [   1/2600] -> Loss: 13245.5212
Epoch [   2/2600] -> Loss: 11887.8559
Epoch [   3/2600] -> Loss: 9132.1151
Epoch [   4/2600] -> Loss: 5777.3364
Epoch [   5/2600] -> Loss: 3742.5900
Epoch [   6/2600] -> Loss: 3167.6718
Epoch [   7/2600] -> Loss: 3084.6374
Epoch [   8/2600] -> Loss: 3072.3457
Epoch [   9/2600] -> Loss: 3073.5315
Epoch [  10/2600] -> Loss: 3071.3181
Epoch [  11/2600] -> Loss: 3064.2462
Epoch [  12/2600] -> Loss: 3055.6979
Epoch [  13/2600] -> Loss: 3045.0628
Epoch [  14/2600] -> Loss: 3041.9831
Epoch [  15/2600] -> Loss: 3035.4696
Epoch [  16/2600] -> Loss: 3029.5561
Epoch [  17/2600] -> Loss: 3023.5502
Epoch [  18/2600] -> Loss: 3013.2352
Epoch [  19/2600] -> Loss: 3015.9010
Epoch [  20/2600] -> Loss: 3008.0274
Epoch [  21/2600] -> Loss: 3013.0546
Epoch [  22/2600] -> Loss: 2989.3808
Epoch [  23/2600] -> Loss: 2984.2103
Epoch [  24/2600] -> Loss: 2983.4658
Epoch [  25/2600] -> Loss: 2969.9403
Epoch [  26/2600] -> Loss: 2966.0303
Epoch [  27/2600] -> Loss: 2958.3301
Epoch [  28/2600] -> Loss: 2945.3414
Epoch [  29/2600] -> Loss: 2941.1569
Epoch [  30/2600] -> Loss: 2932.7062
Epoch [  31/2600] -> Loss: 2917.6389
Epoch [  32/2600] -> Loss: 2918.3989
Epoch [  33/2600] -> Loss: 2902.7322
Epoch [  34/2600] -> Loss: 2892.8564
Epoch [  35/2600] -> Loss: 2884.5106
Epoch [  36/2600] -> Loss: 2869.3133
Epoch [  37/2600] -> Loss: 2853.0753
Epoch [  38/2600] -> Loss: 2851.1431
Epoch [  39/2600] -> Loss: 2820.8351
Epoch [  40/2600] -> Loss: 2826.9332
Epoch [  41/2600] -> Loss: 2798.2381
Epoch [  42/2600] -> Loss: 2782.8059
Epoch [  43/2600] -> Loss: 2765.9151
Epoch [  44/2600] -> Loss: 2751.3579
Epoch [  45/2600] -> Loss: 2732.6371
Epoch [  46/2600] -> Loss: 2715.2188
Epoch [  47/2600] -> Loss: 2701.7916
Epoch [  48/2600] -> Loss: 2680.5547
Epoch [  49/2600] -> Loss: 2666.5463
Epoch [  50/2600] -> Loss: 2648.4859
Epoch [  51/2600] -> Loss: 2627.6815
Epoch [  52/2600] -> Loss: 2613.2718
Epoch [  53/2600] -> Loss: 2588.1984
Epoch [  54/2600] -> Loss: 2573.8159
Epoch [  55/2600] -> Loss: 2549.1728
Epoch [  56/2600] -> Loss: 2532.7868
Epoch [  57/2600] -> Loss: 2508.1748
Epoch [  58/2600] -> Loss: 2498.7985
Epoch [  59/2600] -> Loss: 2483.0882
Epoch [  60/2600] -> Loss: 2452.7836
Epoch [  61/2600] -> Loss: 2431.9435
Epoch [  62/2600] -> Loss: 2415.6983
Epoch [  63/2600] -> Loss: 2395.2447
Epoch [  64/2600] -> Loss: 2371.6013
Epoch [  65/2600] -> Loss: 2358.0592
Epoch [  66/2600] -> Loss: 2344.2631
Epoch [  67/2600] -> Loss: 2315.1730
Epoch [  68/2600] -> Loss: 2294.1557
Epoch [  69/2600] -> Loss: 2285.3455
Epoch [  70/2600] -> Loss: 2262.0359
Epoch [  71/2600] -> Loss: 2241.8192
Epoch [  72/2600] -> Loss: 2225.1716
Epoch [  73/2600] -> Loss: 2204.9776
Epoch [  74/2600] -> Loss: 2180.6044
Epoch [  75/2600] -> Loss: 2175.2868
Epoch [  76/2600] -> Loss: 2152.8188
Epoch [  77/2600] -> Loss: 2144.2921
Epoch [  78/2600] -> Loss: 2122.9253
Epoch [  79/2600] -> Loss: 2108.7220
Epoch [  80/2600] -> Loss: 2095.9898
Epoch [  81/2600] -> Loss: 2075.0596
Epoch [  82/2600] -> Loss: 2065.3021
Epoch [  83/2600] -> Loss: 2052.3454
Epoch [  84/2600] -> Loss: 2035.5015
Epoch [  85/2600] -> Loss: 2036.3347
Epoch [  86/2600] -> Loss: 2006.7022
Epoch [  87/2600] -> Loss: 1994.7029
Epoch [  88/2600] -> Loss: 1983.6838
Epoch [  89/2600] -> Loss: 1971.0244
Epoch [  90/2600] -> Loss: 1973.3787
Epoch [  91/2600] -> Loss: 1943.2863
Epoch [  92/2600] -> Loss: 1941.4670
Epoch [  93/2600] -> Loss: 1925.7768
Epoch [  94/2600] -> Loss: 1917.2010
Epoch [  95/2600] -> Loss: 1910.9681
Epoch [  96/2600] -> Loss: 1898.2764
Epoch [  97/2600] -> Loss: 1889.7918
Epoch [  98/2600] -> Loss: 1876.0403
Epoch [  99/2600] -> Loss: 1871.3665
--------------------------------------------------
Model checkpoint saved as FFNN_100.pth
--------------------------------------------------
Epoch [ 100/2600] -> Loss: 1871.8638
Epoch [ 101/2600] -> Loss: 1858.0465
Epoch [ 102/2600] -> Loss: 1839.0176
Epoch [ 103/2600] -> Loss: 1840.0118
Epoch [ 104/2600] -> Loss: 1832.7695
Epoch [ 105/2600] -> Loss: 1817.2348
Epoch [ 106/2600] -> Loss: 1819.7327
Epoch [ 107/2600] -> Loss: 1805.3373
Epoch [ 108/2600] -> Loss: 1803.1567
Epoch [ 109/2600] -> Loss: 1791.4179
Epoch [ 110/2600] -> Loss: 1781.1934
Epoch [ 111/2600] -> Loss: 1778.3019
Epoch [ 112/2600] -> Loss: 1771.2469
Epoch [ 113/2600] -> Loss: 1769.3323
Epoch [ 114/2600] -> Loss: 1758.1157
Epoch [ 115/2600] -> Loss: 1748.0895
Epoch [ 116/2600] -> Loss: 1747.0775
Epoch [ 117/2600] -> Loss: 1734.9568
Epoch [ 118/2600] -> Loss: 1730.1513
Epoch [ 119/2600] -> Loss: 1724.5913
Epoch [ 120/2600] -> Loss: 1730.8415
Epoch [ 121/2600] -> Loss: 1721.9999
Epoch [ 122/2600] -> Loss: 1699.9456
Epoch [ 123/2600] -> Loss: 1702.4392
Epoch [ 124/2600] -> Loss: 1695.3101
Epoch [ 125/2600] -> Loss: 1703.6258
Epoch [ 126/2600] -> Loss: 1686.0146
Epoch [ 127/2600] -> Loss: 1681.4596
Epoch [ 128/2600] -> Loss: 1677.4914
Epoch [ 129/2600] -> Loss: 1672.5750
Epoch [ 130/2600] -> Loss: 1673.6577
Epoch [ 131/2600] -> Loss: 1663.6618
Epoch [ 132/2600] -> Loss: 1668.0446
Epoch [ 133/2600] -> Loss: 1655.9340
Epoch [ 134/2600] -> Loss: 1650.7518
Epoch [ 135/2600] -> Loss: 1648.2307
Epoch [ 136/2600] -> Loss: 1645.0843
Epoch [ 137/2600] -> Loss: 1636.8710
Epoch [ 138/2600] -> Loss: 1632.3900
Epoch [ 139/2600] -> Loss: 1630.5684
Epoch [ 140/2600] -> Loss: 1628.5451
Epoch [ 141/2600] -> Loss: 1623.0076
Epoch [ 142/2600] -> Loss: 1625.5201
Epoch [ 143/2600] -> Loss: 1614.9442
Epoch [ 144/2600] -> Loss: 1611.3622
Epoch [ 145/2600] -> Loss: 1610.0065
Epoch [ 146/2600] -> Loss: 1609.8722
Epoch [ 147/2600] -> Loss: 1606.3560
Epoch [ 148/2600] -> Loss: 1598.4559
Epoch [ 149/2600] -> Loss: 1604.9348
Epoch [ 150/2600] -> Loss: 1594.4883
Epoch [ 151/2600] -> Loss: 1598.3824
Epoch [ 152/2600] -> Loss: 1590.7402
Epoch [ 153/2600] -> Loss: 1589.7201
Epoch [ 154/2600] -> Loss: 1596.1249
Epoch [ 155/2600] -> Loss: 1582.2779
Epoch [ 156/2600] -> Loss: 1590.4451
Epoch [ 157/2600] -> Loss: 1582.8570
Epoch [ 158/2600] -> Loss: 1579.5853
Epoch [ 159/2600] -> Loss: 1591.2225
Epoch [ 160/2600] -> Loss: 1576.4149
Epoch [ 161/2600] -> Loss: 1580.8134
Epoch [ 162/2600] -> Loss: 1573.6952
Epoch [ 163/2600] -> Loss: 1574.1322
Epoch [ 164/2600] -> Loss: 1576.1303
Epoch [ 165/2600] -> Loss: 1576.1080
Epoch [ 166/2600] -> Loss: 1564.0430
Epoch [ 167/2600] -> Loss: 1575.6310
Epoch [ 168/2600] -> Loss: 1566.1154
Epoch [ 169/2600] -> Loss: 1563.1474
Epoch [ 170/2600] -> Loss: 1564.0058
Epoch [ 171/2600] -> Loss: 1554.6303
Epoch [ 172/2600] -> Loss: 1563.0157
Epoch [ 173/2600] -> Loss: 1557.5285
Epoch [ 174/2600] -> Loss: 1567.4784
Epoch [ 175/2600] -> Loss: 1562.7817
Epoch [ 176/2600] -> Loss: 1562.3337
Epoch [ 177/2600] -> Loss: 1559.5348
Epoch [ 178/2600] -> Loss: 1560.8605
Epoch [ 179/2600] -> Loss: 1560.5238
Epoch [ 180/2600] -> Loss: 1557.5292
Epoch [ 181/2600] -> Loss: 1551.5023
Epoch [ 182/2600] -> Loss: 1553.7971
Epoch [ 183/2600] -> Loss: 1549.2271
Epoch [ 184/2600] -> Loss: 1548.2119
Epoch [ 185/2600] -> Loss: 1555.4402
Epoch [ 186/2600] -> Loss: 1554.6691
Epoch [ 187/2600] -> Loss: 1548.5800
Epoch [ 188/2600] -> Loss: 1563.9945
Epoch [ 189/2600] -> Loss: 1550.1372
Epoch [ 190/2600] -> Loss: 1543.5369
Epoch [ 191/2600] -> Loss: 1559.3666
Epoch [ 192/2600] -> Loss: 1555.8585
Epoch [ 193/2600] -> Loss: 1545.7735
Epoch [ 194/2600] -> Loss: 1548.2326
Epoch [ 195/2600] -> Loss: 1544.1889
Epoch [ 196/2600] -> Loss: 1552.1000
Epoch [ 197/2600] -> Loss: 1542.4204
Epoch [ 198/2600] -> Loss: 1554.3175
Epoch [ 199/2600] -> Loss: 1541.4452
--------------------------------------------------
Model checkpoint saved as FFNN_200.pth
--------------------------------------------------
Epoch [ 200/2600] -> Loss: 1543.6080
Epoch [ 201/2600] -> Loss: 1548.7492
Epoch [ 202/2600] -> Loss: 1548.2092
Epoch [ 203/2600] -> Loss: 1541.6153
Epoch [ 204/2600] -> Loss: 1549.5929
Epoch [ 205/2600] -> Loss: 1537.5413
Epoch [ 206/2600] -> Loss: 1541.0966
Epoch [ 207/2600] -> Loss: 1543.8377
Epoch [ 208/2600] -> Loss: 1535.4303
Epoch [ 209/2600] -> Loss: 1541.2381
Epoch [ 210/2600] -> Loss: 1539.2802
Epoch [ 211/2600] -> Loss: 1538.8176
Epoch [ 212/2600] -> Loss: 1550.1238
Epoch [ 213/2600] -> Loss: 1542.1283
Epoch [ 214/2600] -> Loss: 1534.8981
Epoch [ 215/2600] -> Loss: 1539.7291
Epoch [ 216/2600] -> Loss: 1532.0665
Epoch [ 217/2600] -> Loss: 1536.7723
Epoch [ 218/2600] -> Loss: 1540.1215
Epoch [ 219/2600] -> Loss: 1535.9672
Epoch [ 220/2600] -> Loss: 1528.6861
Epoch [ 221/2600] -> Loss: 1525.6676
Epoch [ 222/2600] -> Loss: 1539.3799
Epoch [ 223/2600] -> Loss: 1532.2934
Epoch [ 224/2600] -> Loss: 1538.1300
Epoch [ 225/2600] -> Loss: 1533.7643
Epoch [ 226/2600] -> Loss: 1536.1388
Epoch [ 227/2600] -> Loss: 1534.1215
Epoch [ 228/2600] -> Loss: 1536.3221
Epoch [ 229/2600] -> Loss: 1542.9360
Epoch [ 230/2600] -> Loss: 1525.4330
Epoch [ 231/2600] -> Loss: 1531.1516
Epoch [ 232/2600] -> Loss: 1529.9779
Epoch [ 233/2600] -> Loss: 1526.8788
Epoch [ 234/2600] -> Loss: 1534.0979
Epoch [ 235/2600] -> Loss: 1536.5556
Epoch [ 236/2600] -> Loss: 1529.5891
Epoch [ 237/2600] -> Loss: 1530.2697
Epoch [ 238/2600] -> Loss: 1528.5039
Epoch [ 239/2600] -> Loss: 1536.1929
Epoch [ 240/2600] -> Loss: 1537.0657
Epoch   241: reducing learning rate of group 0 to 2.5000e-04.
Epoch [ 241/2600] -> Loss: 1537.3002
Epoch [ 242/2600] -> Loss: 1527.6092
Epoch [ 243/2600] -> Loss: 1526.4581
Epoch [ 244/2600] -> Loss: 1527.6410
Epoch [ 245/2600] -> Loss: 1523.9447
Epoch [ 246/2600] -> Loss: 1523.9878
Epoch [ 247/2600] -> Loss: 1518.3361
Epoch [ 248/2600] -> Loss: 1529.7870
Epoch [ 249/2600] -> Loss: 1523.7638
Epoch [ 250/2600] -> Loss: 1527.0594
Epoch [ 251/2600] -> Loss: 1525.3302
Epoch [ 252/2600] -> Loss: 1526.6271
Epoch [ 253/2600] -> Loss: 1524.2933
Epoch [ 254/2600] -> Loss: 1524.6720
Epoch [ 255/2600] -> Loss: 1527.4265
Epoch [ 256/2600] -> Loss: 1524.9018
Epoch [ 257/2600] -> Loss: 1525.8005
Epoch   258: reducing learning rate of group 0 to 1.2500e-04.
Epoch [ 258/2600] -> Loss: 1522.1330
Epoch [ 259/2600] -> Loss: 1526.9733
Epoch [ 260/2600] -> Loss: 1518.3610
Epoch [ 261/2600] -> Loss: 1519.4107
Epoch [ 262/2600] -> Loss: 1518.7791
Epoch [ 263/2600] -> Loss: 1516.4003
Epoch [ 264/2600] -> Loss: 1522.5469
Epoch [ 265/2600] -> Loss: 1521.2990
Epoch [ 266/2600] -> Loss: 1518.7947
Epoch [ 267/2600] -> Loss: 1517.2124
Epoch [ 268/2600] -> Loss: 1517.3827
Epoch [ 269/2600] -> Loss: 1522.9359
Epoch [ 270/2600] -> Loss: 1517.8566
Epoch [ 271/2600] -> Loss: 1519.6244
Epoch [ 272/2600] -> Loss: 1521.2223
Epoch [ 273/2600] -> Loss: 1523.3131
Epoch   274: reducing learning rate of group 0 to 6.2500e-05.
Epoch [ 274/2600] -> Loss: 1523.0011
Epoch [ 275/2600] -> Loss: 1517.9272
Epoch [ 276/2600] -> Loss: 1518.7094
Epoch [ 277/2600] -> Loss: 1521.1324
Epoch [ 278/2600] -> Loss: 1518.4898
Epoch [ 279/2600] -> Loss: 1520.9095
Epoch [ 280/2600] -> Loss: 1517.4388
Epoch [ 281/2600] -> Loss: 1516.6673
Epoch [ 282/2600] -> Loss: 1522.6089
Epoch [ 283/2600] -> Loss: 1521.3346
Epoch [ 284/2600] -> Loss: 1519.8484
Epoch   285: reducing learning rate of group 0 to 3.1250e-05.
Epoch [ 285/2600] -> Loss: 1519.4737
Epoch [ 286/2600] -> Loss: 1518.0205
Epoch [ 287/2600] -> Loss: 1521.7920
Epoch [ 288/2600] -> Loss: 1517.5712
Epoch [ 289/2600] -> Loss: 1516.5640
Epoch [ 290/2600] -> Loss: 1517.5363
Epoch [ 291/2600] -> Loss: 1518.2919
Epoch [ 292/2600] -> Loss: 1515.5364
Epoch [ 293/2600] -> Loss: 1517.2009
Epoch [ 294/2600] -> Loss: 1516.7774
Epoch [ 295/2600] -> Loss: 1517.5891
Epoch [ 296/2600] -> Loss: 1514.4726
Epoch [ 297/2600] -> Loss: 1516.7976
Epoch [ 298/2600] -> Loss: 1515.9295
Epoch [ 299/2600] -> Loss: 1516.2122
--------------------------------------------------
Model checkpoint saved as FFNN_300.pth
--------------------------------------------------
Epoch [ 300/2600] -> Loss: 1516.1282
Epoch [ 301/2600] -> Loss: 1516.8658
Epoch [ 302/2600] -> Loss: 1516.8585
Epoch [ 303/2600] -> Loss: 1515.0023
Epoch [ 304/2600] -> Loss: 1518.9148
Epoch [ 305/2600] -> Loss: 1520.4822
Epoch [ 306/2600] -> Loss: 1515.0798
Epoch   307: reducing learning rate of group 0 to 1.5625e-05.
Epoch [ 307/2600] -> Loss: 1522.1886
Epoch [ 308/2600] -> Loss: 1514.8442
Epoch [ 309/2600] -> Loss: 1515.8006
Epoch [ 310/2600] -> Loss: 1514.4014
Epoch [ 311/2600] -> Loss: 1517.1145
Epoch [ 312/2600] -> Loss: 1515.8082
Epoch [ 313/2600] -> Loss: 1515.8698
Epoch [ 314/2600] -> Loss: 1514.4702
Epoch [ 315/2600] -> Loss: 1514.8803
Epoch [ 316/2600] -> Loss: 1515.7098
Epoch [ 317/2600] -> Loss: 1518.5724
Epoch   318: reducing learning rate of group 0 to 7.8125e-06.
Epoch [ 318/2600] -> Loss: 1516.5457
Epoch [ 319/2600] -> Loss: 1515.1525
Epoch [ 320/2600] -> Loss: 1514.8572
Epoch [ 321/2600] -> Loss: 1517.5079
Epoch [ 322/2600] -> Loss: 1517.5573
Epoch [ 323/2600] -> Loss: 1515.8613
Epoch [ 324/2600] -> Loss: 1515.2940
Epoch [ 325/2600] -> Loss: 1513.9366
Epoch [ 326/2600] -> Loss: 1521.2082
Epoch [ 327/2600] -> Loss: 1521.2407
Epoch [ 328/2600] -> Loss: 1515.2102
Epoch [ 329/2600] -> Loss: 1515.1921
Epoch [ 330/2600] -> Loss: 1517.0393
Epoch [ 331/2600] -> Loss: 1514.9231
Epoch [ 332/2600] -> Loss: 1515.9122
Epoch [ 333/2600] -> Loss: 1518.3791
Epoch [ 334/2600] -> Loss: 1517.1677
Epoch [ 335/2600] -> Loss: 1525.9063
Epoch   336: reducing learning rate of group 0 to 3.9063e-06.
Epoch [ 336/2600] -> Loss: 1516.8884
Epoch [ 337/2600] -> Loss: 1514.3166
Epoch [ 338/2600] -> Loss: 1516.1083
Epoch [ 339/2600] -> Loss: 1516.2631
Epoch [ 340/2600] -> Loss: 1515.8619
Epoch [ 341/2600] -> Loss: 1515.0312
Epoch [ 342/2600] -> Loss: 1518.9421
Epoch [ 343/2600] -> Loss: 1517.7863
Epoch [ 344/2600] -> Loss: 1513.3935
Epoch [ 345/2600] -> Loss: 1516.9193
Epoch [ 346/2600] -> Loss: 1518.3626
Epoch [ 347/2600] -> Loss: 1517.0918
Epoch [ 348/2600] -> Loss: 1520.7020
Epoch [ 349/2600] -> Loss: 1514.3307
Epoch [ 350/2600] -> Loss: 1516.1026
Epoch [ 351/2600] -> Loss: 1518.3368
Epoch [ 352/2600] -> Loss: 1514.4584
Epoch [ 353/2600] -> Loss: 1514.9164
Epoch [ 354/2600] -> Loss: 1517.0110
Epoch   355: reducing learning rate of group 0 to 1.9531e-06.
Epoch [ 355/2600] -> Loss: 1514.4900
Epoch [ 356/2600] -> Loss: 1517.1097
Epoch [ 357/2600] -> Loss: 1515.8408
Epoch [ 358/2600] -> Loss: 1514.4521
Epoch [ 359/2600] -> Loss: 1515.7013
Epoch [ 360/2600] -> Loss: 1515.9518
Epoch [ 361/2600] -> Loss: 1518.1127
Epoch [ 362/2600] -> Loss: 1514.4700
Epoch [ 363/2600] -> Loss: 1515.4307
Epoch [ 364/2600] -> Loss: 1515.5359
Epoch [ 365/2600] -> Loss: 1514.8605
Epoch   366: reducing learning rate of group 0 to 9.7656e-07.
Epoch [ 366/2600] -> Loss: 1515.5420
Epoch [ 367/2600] -> Loss: 1515.8652
Epoch [ 368/2600] -> Loss: 1517.0295
Epoch [ 369/2600] -> Loss: 1514.6451
Epoch [ 370/2600] -> Loss: 1513.7259
Epoch [ 371/2600] -> Loss: 1519.0749
Epoch [ 372/2600] -> Loss: 1515.3778
Epoch [ 373/2600] -> Loss: 1516.7241
Epoch [ 374/2600] -> Loss: 1514.4090
Epoch [ 375/2600] -> Loss: 1516.3066
Epoch [ 376/2600] -> Loss: 1514.2435
Epoch   377: reducing learning rate of group 0 to 4.8828e-07.
Epoch [ 377/2600] -> Loss: 1516.3324
Epoch [ 378/2600] -> Loss: 1517.5133
Epoch [ 379/2600] -> Loss: 1514.3314
Epoch [ 380/2600] -> Loss: 1515.3559
Epoch [ 381/2600] -> Loss: 1515.2274
Epoch [ 382/2600] -> Loss: 1516.2110
Epoch [ 383/2600] -> Loss: 1515.7711
Epoch [ 384/2600] -> Loss: 1515.1863
Epoch [ 385/2600] -> Loss: 1515.8073
Epoch [ 386/2600] -> Loss: 1514.5562
Epoch [ 387/2600] -> Loss: 1514.8883
Epoch   388: reducing learning rate of group 0 to 2.4414e-07.
Epoch [ 388/2600] -> Loss: 1516.9718
Epoch [ 389/2600] -> Loss: 1515.8525
Epoch [ 390/2600] -> Loss: 1517.9272
Epoch [ 391/2600] -> Loss: 1516.0433
Epoch [ 392/2600] -> Loss: 1516.7392
