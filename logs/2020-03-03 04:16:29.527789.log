--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
File location :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): Linear(in_features=6, out_features=6, bias=True)
    (2): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
No pre-trained models available, initializing model weights
--------------------------------------------------
Training model with: num_epochs=2600, start_lr=0.005
Epoch [   1/2600] -> Loss: 10380.0395
Epoch [   2/2600] -> Loss: 3558.7780
Epoch [   3/2600] -> Loss: 3382.3822
Epoch [   4/2600] -> Loss: 3115.3021
Epoch [   5/2600] -> Loss: 2974.4195
Epoch [   6/2600] -> Loss: 2988.0152
Epoch [   7/2600] -> Loss: 2957.6415
Epoch [   8/2600] -> Loss: 2955.8368
Epoch [   9/2600] -> Loss: 2934.8024
Epoch [  10/2600] -> Loss: 2964.0852
Epoch [  11/2600] -> Loss: 2919.2470
Epoch [  12/2600] -> Loss: 2904.1437
Epoch [  13/2600] -> Loss: 2931.4666
Epoch [  14/2600] -> Loss: 2895.5609
Epoch [  15/2600] -> Loss: 2893.0953
Epoch [  16/2600] -> Loss: 2859.4820
Epoch [  17/2600] -> Loss: 2854.8012
Epoch [  18/2600] -> Loss: 2835.1812
Epoch [  19/2600] -> Loss: 2821.6263
Epoch [  20/2600] -> Loss: 2802.8427
Epoch [  21/2600] -> Loss: 2803.7663
Epoch [  22/2600] -> Loss: 2787.4692
Epoch [  23/2600] -> Loss: 2771.1139
Epoch [  24/2600] -> Loss: 2740.1720
Epoch [  25/2600] -> Loss: 2734.0609
Epoch [  26/2600] -> Loss: 2717.6298
Epoch [  27/2600] -> Loss: 2683.2519
Epoch [  28/2600] -> Loss: 2682.3570
Epoch [  29/2600] -> Loss: 2641.9649
Epoch [  30/2600] -> Loss: 2615.4116
Epoch [  31/2600] -> Loss: 2591.1244
Epoch [  32/2600] -> Loss: 2586.3228
Epoch [  33/2600] -> Loss: 2543.6084
Epoch [  34/2600] -> Loss: 2527.1322
Epoch [  35/2600] -> Loss: 2512.8193
Epoch [  36/2600] -> Loss: 2455.5456
Epoch [  37/2600] -> Loss: 2433.7229
Epoch [  38/2600] -> Loss: 2400.1087
Epoch [  39/2600] -> Loss: 2367.8909
Epoch [  40/2600] -> Loss: 2351.3874
Epoch [  41/2600] -> Loss: 2293.5104
Epoch [  42/2600] -> Loss: 2274.7166
Epoch [  43/2600] -> Loss: 2235.2635
Epoch [  44/2600] -> Loss: 2200.3639
Epoch [  45/2600] -> Loss: 2168.7320
Epoch [  46/2600] -> Loss: 2142.7872
Epoch [  47/2600] -> Loss: 2103.4402
Epoch [  48/2600] -> Loss: 2089.7658
Epoch [  49/2600] -> Loss: 2047.2183
Epoch [  50/2600] -> Loss: 2024.5296
Epoch [  51/2600] -> Loss: 1999.1999
Epoch [  52/2600] -> Loss: 1980.0339
Epoch [  53/2600] -> Loss: 1956.1029
Epoch [  54/2600] -> Loss: 1930.7837
Epoch [  55/2600] -> Loss: 1931.0649
Epoch [  56/2600] -> Loss: 1920.3383
Epoch [  57/2600] -> Loss: 1905.2537
Epoch [  58/2600] -> Loss: 1886.1423
Epoch [  59/2600] -> Loss: 1870.8203
Epoch [  60/2600] -> Loss: 1864.3311
Epoch [  61/2600] -> Loss: 1833.0413
Epoch [  62/2600] -> Loss: 1805.3665
Epoch [  63/2600] -> Loss: 1810.8051
Epoch [  64/2600] -> Loss: 1793.9130
Epoch [  65/2600] -> Loss: 1766.8184
Epoch [  66/2600] -> Loss: 1755.5068
Epoch [  67/2600] -> Loss: 1739.3641
Epoch [  68/2600] -> Loss: 1726.1024
Epoch [  69/2600] -> Loss: 1731.2248
Epoch [  70/2600] -> Loss: 1713.2074
Epoch [  71/2600] -> Loss: 1727.1169
Epoch [  72/2600] -> Loss: 1731.2230
Epoch [  73/2600] -> Loss: 1693.8217
Epoch [  74/2600] -> Loss: 1680.4241
Epoch [  75/2600] -> Loss: 1709.2236
Epoch [  76/2600] -> Loss: 1675.4334
Epoch [  77/2600] -> Loss: 1668.1845
Epoch [  78/2600] -> Loss: 1669.5844
Epoch [  79/2600] -> Loss: 1667.7098
Epoch [  80/2600] -> Loss: 1699.9836
Epoch [  81/2600] -> Loss: 1679.6528
Epoch [  82/2600] -> Loss: 1676.8394
Epoch [  83/2600] -> Loss: 1681.2569
Epoch [  84/2600] -> Loss: 1672.2837
Epoch [  85/2600] -> Loss: 1665.6440
Epoch [  86/2600] -> Loss: 1634.9742
Epoch [  87/2600] -> Loss: 1650.2102
Epoch [  88/2600] -> Loss: 1672.9850
Epoch [  89/2600] -> Loss: 1636.3569
Epoch [  90/2600] -> Loss: 1658.8241
Epoch [  91/2600] -> Loss: 1632.7022
Epoch [  92/2600] -> Loss: 1677.7481
Epoch [  93/2600] -> Loss: 1698.9558
Epoch [  94/2600] -> Loss: 1651.3470
Epoch [  95/2600] -> Loss: 1639.9812
Epoch [  96/2600] -> Loss: 1713.0899
Epoch [  97/2600] -> Loss: 1676.4462
Epoch [  98/2600] -> Loss: 1645.4940
Epoch [  99/2600] -> Loss: 1630.1397
Epoch [ 100/2600] -> Loss: 1642.7544
Epoch [ 101/2600] -> Loss: 1666.3529
Epoch [ 102/2600] -> Loss: 1623.5461
Epoch [ 103/2600] -> Loss: 1624.8658
Epoch [ 104/2600] -> Loss: 1652.2337
Epoch [ 105/2600] -> Loss: 1644.3312
Epoch [ 106/2600] -> Loss: 1616.3312
Epoch [ 107/2600] -> Loss: 1626.8377
Epoch [ 108/2600] -> Loss: 1643.1812
Epoch [ 109/2600] -> Loss: 1631.7838
Epoch [ 110/2600] -> Loss: 1621.4462
Epoch [ 111/2600] -> Loss: 1628.8504
Epoch [ 112/2600] -> Loss: 1667.9022
Epoch [ 113/2600] -> Loss: 1673.7736
Epoch [ 114/2600] -> Loss: 1634.1615
Epoch [ 115/2600] -> Loss: 1644.2427
Epoch [ 116/2600] -> Loss: 1650.2264
Epoch   117: reducing learning rate of group 0 to 2.5000e-03.
Epoch [ 117/2600] -> Loss: 1635.0960
Epoch [ 118/2600] -> Loss: 1661.4656
Epoch [ 119/2600] -> Loss: 1642.9428
Epoch [ 120/2600] -> Loss: 1616.2618
Epoch [ 121/2600] -> Loss: 1620.0012
Epoch [ 122/2600] -> Loss: 1629.1116
Epoch [ 123/2600] -> Loss: 1642.0825
Epoch [ 124/2600] -> Loss: 1642.8144
Epoch [ 125/2600] -> Loss: 1644.7310
Epoch [ 126/2600] -> Loss: 1617.3405
Epoch [ 127/2600] -> Loss: 1633.7202
Epoch   128: reducing learning rate of group 0 to 1.2500e-03.
Epoch [ 128/2600] -> Loss: 1616.6733
Epoch [ 129/2600] -> Loss: 1625.7538
Epoch [ 130/2600] -> Loss: 1620.7526
Epoch [ 131/2600] -> Loss: 1615.3112
Epoch [ 132/2600] -> Loss: 1620.2209
Epoch [ 133/2600] -> Loss: 1609.2671
Epoch [ 134/2600] -> Loss: 1616.8142
Epoch [ 135/2600] -> Loss: 1617.6447
Epoch [ 136/2600] -> Loss: 1617.6532
Epoch [ 137/2600] -> Loss: 1606.6241
Epoch [ 138/2600] -> Loss: 1622.5983
Epoch [ 139/2600] -> Loss: 1620.5091
Epoch [ 140/2600] -> Loss: 1624.3582
Epoch [ 141/2600] -> Loss: 1611.6947
Epoch [ 142/2600] -> Loss: 1621.6198
Epoch [ 143/2600] -> Loss: 1618.8318
Epoch [ 144/2600] -> Loss: 1615.4594
Epoch [ 145/2600] -> Loss: 1610.7467
Epoch [ 146/2600] -> Loss: 1613.0642
Epoch [ 147/2600] -> Loss: 1614.1968
Epoch   148: reducing learning rate of group 0 to 6.2500e-04.
Epoch [ 148/2600] -> Loss: 1621.8247
Epoch [ 149/2600] -> Loss: 1620.8721
Epoch [ 150/2600] -> Loss: 1610.3267
Epoch [ 151/2600] -> Loss: 1613.1647
Epoch [ 152/2600] -> Loss: 1610.2388
Epoch [ 153/2600] -> Loss: 1621.6688
Epoch [ 154/2600] -> Loss: 1617.7840
Epoch [ 155/2600] -> Loss: 1617.6271
Epoch [ 156/2600] -> Loss: 1612.1171
Epoch [ 157/2600] -> Loss: 1615.1203
Epoch [ 158/2600] -> Loss: 1609.1246
Epoch   159: reducing learning rate of group 0 to 3.1250e-04.
Epoch [ 159/2600] -> Loss: 1607.6003
Epoch [ 160/2600] -> Loss: 1610.7804
Epoch [ 161/2600] -> Loss: 1611.7547
Epoch [ 162/2600] -> Loss: 1617.5328
Epoch [ 163/2600] -> Loss: 1610.5363
Epoch [ 164/2600] -> Loss: 1610.9455
Epoch [ 165/2600] -> Loss: 1617.6486
Epoch [ 166/2600] -> Loss: 1627.3369
Epoch [ 167/2600] -> Loss: 1609.2502
Epoch [ 168/2600] -> Loss: 1609.8654
Epoch [ 169/2600] -> Loss: 1609.3884
Epoch   170: reducing learning rate of group 0 to 1.5625e-04.
Epoch [ 170/2600] -> Loss: 1614.7011
Epoch [ 171/2600] -> Loss: 1621.7513
Epoch [ 172/2600] -> Loss: 1615.8004
Epoch [ 173/2600] -> Loss: 1610.9751
Epoch [ 174/2600] -> Loss: 1609.5543
Epoch [ 175/2600] -> Loss: 1615.1322
Epoch [ 176/2600] -> Loss: 1612.7681
Epoch [ 177/2600] -> Loss: 1616.2023
Epoch [ 178/2600] -> Loss: 1617.8282
Epoch [ 179/2600] -> Loss: 1611.2449
Epoch [ 180/2600] -> Loss: 1628.7755
Epoch   181: reducing learning rate of group 0 to 7.8125e-05.
Epoch [ 181/2600] -> Loss: 1607.4512
Epoch [ 182/2600] -> Loss: 1610.5234
Epoch [ 183/2600] -> Loss: 1615.4588
Epoch [ 184/2600] -> Loss: 1608.2690
Epoch [ 185/2600] -> Loss: 1608.5568
Epoch [ 186/2600] -> Loss: 1625.2628
Epoch [ 187/2600] -> Loss: 1612.0297
Epoch [ 188/2600] -> Loss: 1619.0904
Epoch [ 189/2600] -> Loss: 1623.0858
Epoch [ 190/2600] -> Loss: 1619.0975
Epoch [ 191/2600] -> Loss: 1614.1121
Epoch   192: reducing learning rate of group 0 to 3.9063e-05.
Epoch [ 192/2600] -> Loss: 1607.5648
Epoch [ 193/2600] -> Loss: 1609.0455
Epoch [ 194/2600] -> Loss: 1622.8309
Epoch [ 195/2600] -> Loss: 1617.0329
Epoch [ 196/2600] -> Loss: 1613.7188
Epoch [ 197/2600] -> Loss: 1616.9130
Epoch [ 198/2600] -> Loss: 1611.2249
Epoch [ 199/2600] -> Loss: 1606.9573
--------------------------------------------------
Model checkpoint saved as FFNN_200.pth
--------------------------------------------------
Epoch [ 200/2600] -> Loss: 1625.4275
Epoch [ 201/2600] -> Loss: 1618.9816
Epoch [ 202/2600] -> Loss: 1615.8412
Epoch [ 203/2600] -> Loss: 1602.7044
Epoch [ 204/2600] -> Loss: 1617.3935
Epoch [ 205/2600] -> Loss: 1613.3447
Epoch [ 206/2600] -> Loss: 1603.6263
Epoch [ 207/2600] -> Loss: 1613.2650
Epoch [ 208/2600] -> Loss: 1614.3515
Epoch [ 209/2600] -> Loss: 1608.7724
Epoch [ 210/2600] -> Loss: 1608.2084
Epoch [ 211/2600] -> Loss: 1623.2113
Epoch [ 212/2600] -> Loss: 1602.1669
Epoch [ 213/2600] -> Loss: 1620.3263
Epoch [ 214/2600] -> Loss: 1612.8182
Epoch [ 215/2600] -> Loss: 1608.9008
Epoch [ 216/2600] -> Loss: 1608.3868
Epoch [ 217/2600] -> Loss: 1607.9235
Epoch [ 218/2600] -> Loss: 1618.6003
Epoch [ 219/2600] -> Loss: 1614.4069
Epoch [ 220/2600] -> Loss: 1615.8393
Epoch [ 221/2600] -> Loss: 1608.0486
Epoch [ 222/2600] -> Loss: 1603.2124
Epoch   223: reducing learning rate of group 0 to 1.9531e-05.
Epoch [ 223/2600] -> Loss: 1610.2472
Epoch [ 224/2600] -> Loss: 1615.8416
Epoch [ 225/2600] -> Loss: 1603.2541
Epoch [ 226/2600] -> Loss: 1624.6328
Epoch [ 227/2600] -> Loss: 1618.8431
Epoch [ 228/2600] -> Loss: 1618.7618
Epoch [ 229/2600] -> Loss: 1612.2189
Epoch [ 230/2600] -> Loss: 1626.5839
Epoch [ 231/2600] -> Loss: 1609.7918
Epoch [ 232/2600] -> Loss: 1609.6266
Epoch [ 233/2600] -> Loss: 1620.7742
Epoch   234: reducing learning rate of group 0 to 9.7656e-06.
Epoch [ 234/2600] -> Loss: 1617.9149
Epoch [ 235/2600] -> Loss: 1618.0932
Epoch [ 236/2600] -> Loss: 1601.3931
Epoch [ 237/2600] -> Loss: 1613.3652
Epoch [ 238/2600] -> Loss: 1609.7728
Epoch [ 239/2600] -> Loss: 1617.8100
Epoch [ 240/2600] -> Loss: 1604.5310
Epoch [ 241/2600] -> Loss: 1607.7128
Epoch [ 242/2600] -> Loss: 1610.1712
Epoch [ 243/2600] -> Loss: 1609.5563
Epoch [ 244/2600] -> Loss: 1612.8062
Epoch [ 245/2600] -> Loss: 1620.8795
Epoch [ 246/2600] -> Loss: 1605.1153
Epoch   247: reducing learning rate of group 0 to 4.8828e-06.
Epoch [ 247/2600] -> Loss: 1615.1339
Epoch [ 248/2600] -> Loss: 1616.9834
Epoch [ 249/2600] -> Loss: 1612.2820
Epoch [ 250/2600] -> Loss: 1606.3414
Epoch [ 251/2600] -> Loss: 1614.7940
Epoch [ 252/2600] -> Loss: 1612.2782
Epoch [ 253/2600] -> Loss: 1609.9900
Epoch [ 254/2600] -> Loss: 1613.5629
Epoch [ 255/2600] -> Loss: 1614.6472
Epoch [ 256/2600] -> Loss: 1617.4720
Epoch [ 257/2600] -> Loss: 1615.9748
Epoch   258: reducing learning rate of group 0 to 2.4414e-06.
Epoch [ 258/2600] -> Loss: 1616.0793
Epoch [ 259/2600] -> Loss: 1619.1906
Epoch [ 260/2600] -> Loss: 1611.7773
Epoch [ 261/2600] -> Loss: 1608.8135
Epoch [ 262/2600] -> Loss: 1607.0678
Epoch [ 263/2600] -> Loss: 1612.0252
Epoch [ 264/2600] -> Loss: 1615.7607
Epoch [ 265/2600] -> Loss: 1614.5689
Epoch [ 266/2600] -> Loss: 1623.2140
Epoch [ 267/2600] -> Loss: 1614.4107
Epoch [ 268/2600] -> Loss: 1617.8189
Epoch   269: reducing learning rate of group 0 to 1.2207e-06.
Epoch [ 269/2600] -> Loss: 1626.1655
Epoch [ 270/2600] -> Loss: 1626.0853
Epoch [ 271/2600] -> Loss: 1609.0756
Epoch [ 272/2600] -> Loss: 1616.9221
Epoch [ 273/2600] -> Loss: 1612.4844
Epoch [ 274/2600] -> Loss: 1613.3881
Epoch [ 275/2600] -> Loss: 1607.2405
Epoch [ 276/2600] -> Loss: 1609.3340
Epoch [ 277/2600] -> Loss: 1622.0055
Epoch [ 278/2600] -> Loss: 1606.3117
Epoch [ 279/2600] -> Loss: 1614.1029
Epoch   280: reducing learning rate of group 0 to 6.1035e-07.
Epoch [ 280/2600] -> Loss: 1612.0325
Epoch [ 281/2600] -> Loss: 1608.5704
Epoch [ 282/2600] -> Loss: 1617.7349
Epoch [ 283/2600] -> Loss: 1612.6361
Epoch [ 284/2600] -> Loss: 1620.9131
Epoch [ 285/2600] -> Loss: 1608.9718
Epoch [ 286/2600] -> Loss: 1619.8643
Epoch [ 287/2600] -> Loss: 1617.0032
Epoch [ 288/2600] -> Loss: 1617.2811
Epoch [ 289/2600] -> Loss: 1617.3572
Epoch [ 290/2600] -> Loss: 1608.2439
Epoch   291: reducing learning rate of group 0 to 3.0518e-07.
Epoch [ 291/2600] -> Loss: 1613.5852
Epoch [ 292/2600] -> Loss: 1618.5862
Epoch [ 293/2600] -> Loss: 1619.0783
Epoch [ 294/2600] -> Loss: 1627.6625
Epoch [ 295/2600] -> Loss: 1610.6659
Epoch [ 296/2600] -> Loss: 1609.7903
Epoch [ 297/2600] -> Loss: 1620.6517
Epoch [ 298/2600] -> Loss: 1610.6482
Epoch [ 299/2600] -> Loss: 1615.3959
Epoch [ 300/2600] -> Loss: 1609.8037
Epoch [ 301/2600] -> Loss: 1610.4454
Epoch   302: reducing learning rate of group 0 to 1.5259e-07.
Epoch [ 302/2600] -> Loss: 1613.3507
Epoch [ 303/2600] -> Loss: 1615.2799
Epoch [ 304/2600] -> Loss: 1615.8335
Epoch [ 305/2600] -> Loss: 1624.3864
Epoch [ 306/2600] -> Loss: 1607.1609
Epoch [ 307/2600] -> Loss: 1611.4470
Epoch [ 308/2600] -> Loss: 1618.6732
Epoch [ 309/2600] -> Loss: 1617.7489
Epoch [ 310/2600] -> Loss: 1617.1013
Epoch [ 311/2600] -> Loss: 1612.3707
Epoch [ 312/2600] -> Loss: 1602.0334
Epoch   313: reducing learning rate of group 0 to 7.6294e-08.
Epoch [ 313/2600] -> Loss: 1619.1792
Epoch [ 314/2600] -> Loss: 1612.2159
Epoch [ 315/2600] -> Loss: 1611.5683
Epoch [ 316/2600] -> Loss: 1612.5817
Epoch [ 317/2600] -> Loss: 1615.0253
Epoch [ 318/2600] -> Loss: 1608.9286
Epoch [ 319/2600] -> Loss: 1610.6290
Epoch [ 320/2600] -> Loss: 1620.8106
Epoch [ 321/2600] -> Loss: 1609.6927
Epoch [ 322/2600] -> Loss: 1619.7762
Epoch [ 323/2600] -> Loss: 1604.3765
Epoch   324: reducing learning rate of group 0 to 3.8147e-08.
Epoch [ 324/2600] -> Loss: 1619.8912
Epoch [ 325/2600] -> Loss: 1608.1368
Epoch [ 326/2600] -> Loss: 1623.4782
Epoch [ 327/2600] -> Loss: 1614.9771
Epoch [ 328/2600] -> Loss: 1615.1452
Epoch [ 329/2600] -> Loss: 1603.8985
Epoch [ 330/2600] -> Loss: 1610.8911
Epoch [ 331/2600] -> Loss: 1617.2457
Epoch [ 332/2600] -> Loss: 1612.8750
Epoch [ 333/2600] -> Loss: 1613.8693
Epoch [ 334/2600] -> Loss: 1620.0547
Epoch   335: reducing learning rate of group 0 to 1.9073e-08.
Epoch [ 335/2600] -> Loss: 1618.3474
Epoch [ 336/2600] -> Loss: 1611.3328
Epoch [ 337/2600] -> Loss: 1614.8241
Epoch [ 338/2600] -> Loss: 1607.0904
Epoch [ 339/2600] -> Loss: 1622.3605
Epoch [ 340/2600] -> Loss: 1609.6524
Epoch [ 341/2600] -> Loss: 1612.7149
Epoch [ 342/2600] -> Loss: 1627.9909
Epoch [ 343/2600] -> Loss: 1614.6307
Epoch [ 344/2600] -> Loss: 1610.5839
Epoch [ 345/2600] -> Loss: 1612.1796
Epoch [ 346/2600] -> Loss: 1613.8924
Epoch [ 347/2600] -> Loss: 1617.4000
Epoch [ 348/2600] -> Loss: 1620.1413
Epoch [ 349/2600] -> Loss: 1615.8242
Epoch [ 350/2600] -> Loss: 1614.0330
Epoch [ 351/2600] -> Loss: 1608.8903
Epoch [ 352/2600] -> Loss: 1606.6075
Epoch [ 353/2600] -> Loss: 1611.3503
Epoch [ 354/2600] -> Loss: 1623.0096
Epoch [ 355/2600] -> Loss: 1609.5400
Epoch [ 356/2600] -> Loss: 1611.2736
Epoch [ 357/2600] -> Loss: 1612.1164
Epoch [ 358/2600] -> Loss: 1613.2879
Epoch [ 359/2600] -> Loss: 1613.3246
Epoch [ 360/2600] -> Loss: 1612.5035
Epoch [ 361/2600] -> Loss: 1613.1476
Epoch [ 362/2600] -> Loss: 1609.8022
Epoch [ 363/2600] -> Loss: 1624.1199
Epoch [ 364/2600] -> Loss: 1613.1368
Epoch [ 365/2600] -> Loss: 1603.0481
Epoch [ 366/2600] -> Loss: 1601.9812
Epoch [ 367/2600] -> Loss: 1613.3263
Epoch [ 368/2600] -> Loss: 1614.4086
Epoch [ 369/2600] -> Loss: 1608.2948
Epoch [ 370/2600] -> Loss: 1612.2927
Epoch [ 371/2600] -> Loss: 1616.1561
Epoch [ 372/2600] -> Loss: 1620.5724
Epoch [ 373/2600] -> Loss: 1622.3336
Epoch [ 374/2600] -> Loss: 1611.4912
Epoch [ 375/2600] -> Loss: 1606.1507
Epoch [ 376/2600] -> Loss: 1622.0575
Epoch [ 377/2600] -> Loss: 1610.3085
Epoch [ 378/2600] -> Loss: 1614.9405
Epoch [ 379/2600] -> Loss: 1619.0055
Epoch [ 380/2600] -> Loss: 1614.1271
Epoch [ 381/2600] -> Loss: 1613.6726
Epoch [ 382/2600] -> Loss: 1609.3740
Epoch [ 383/2600] -> Loss: 1611.1055
Epoch [ 384/2600] -> Loss: 1610.3328
Epoch [ 385/2600] -> Loss: 1611.6104
Epoch [ 386/2600] -> Loss: 1612.5685
Epoch [ 387/2600] -> Loss: 1622.7417
Epoch [ 388/2600] -> Loss: 1617.8108
Epoch [ 389/2600] -> Loss: 1608.4260
Epoch [ 390/2600] -> Loss: 1611.7127
Epoch [ 391/2600] -> Loss: 1609.7514
Epoch [ 392/2600] -> Loss: 1618.3679
Epoch [ 393/2600] -> Loss: 1610.2937
Epoch [ 394/2600] -> Loss: 1616.8104
Epoch [ 395/2600] -> Loss: 1618.2587
Epoch [ 396/2600] -> Loss: 1611.8317
Epoch [ 397/2600] -> Loss: 1619.0706
Epoch [ 398/2600] -> Loss: 1625.9847
Epoch [ 399/2600] -> Loss: 1618.5463
--------------------------------------------------
Model checkpoint saved as FFNN_400.pth
--------------------------------------------------
Epoch [ 400/2600] -> Loss: 1612.9675
Epoch [ 401/2600] -> Loss: 1613.5625
Epoch [ 402/2600] -> Loss: 1621.1988
Epoch [ 403/2600] -> Loss: 1612.7263
Epoch [ 404/2600] -> Loss: 1628.3044
Epoch [ 405/2600] -> Loss: 1610.8086
Epoch [ 406/2600] -> Loss: 1620.4845
Epoch [ 407/2600] -> Loss: 1615.7673
Epoch [ 408/2600] -> Loss: 1624.6096
Epoch [ 409/2600] -> Loss: 1610.2033
Epoch [ 410/2600] -> Loss: 1616.3135
Epoch [ 411/2600] -> Loss: 1611.1588
Epoch [ 412/2600] -> Loss: 1621.6073
Epoch [ 413/2600] -> Loss: 1613.6475
Epoch [ 414/2600] -> Loss: 1609.9894
Epoch [ 415/2600] -> Loss: 1615.1453
Epoch [ 416/2600] -> Loss: 1615.2393
Epoch [ 417/2600] -> Loss: 1612.2443
Epoch [ 418/2600] -> Loss: 1610.9607
Epoch [ 419/2600] -> Loss: 1621.5426
Epoch [ 420/2600] -> Loss: 1613.8570
Epoch [ 421/2600] -> Loss: 1606.6587
Epoch [ 422/2600] -> Loss: 1610.4054
Epoch [ 423/2600] -> Loss: 1609.1366
Epoch [ 424/2600] -> Loss: 1614.7514
Epoch [ 425/2600] -> Loss: 1617.6010
Epoch [ 426/2600] -> Loss: 1608.6515
Epoch [ 427/2600] -> Loss: 1617.8110
Epoch [ 428/2600] -> Loss: 1621.6411
Epoch [ 429/2600] -> Loss: 1615.3408
Epoch [ 430/2600] -> Loss: 1611.0540
Epoch [ 431/2600] -> Loss: 1607.7416
Epoch [ 432/2600] -> Loss: 1608.1328
Epoch [ 433/2600] -> Loss: 1617.1917
Epoch [ 434/2600] -> Loss: 1611.1254
Epoch [ 435/2600] -> Loss: 1610.1680
Epoch [ 436/2600] -> Loss: 1606.6486
Epoch [ 437/2600] -> Loss: 1610.9940
Epoch [ 438/2600] -> Loss: 1615.2623
Epoch [ 439/2600] -> Loss: 1615.9981
Epoch [ 440/2600] -> Loss: 1608.0746
Epoch [ 441/2600] -> Loss: 1611.4371
Epoch [ 442/2600] -> Loss: 1608.8557
Epoch [ 443/2600] -> Loss: 1616.3974
Epoch [ 444/2600] -> Loss: 1615.6578
Epoch [ 445/2600] -> Loss: 1611.1404
Epoch [ 446/2600] -> Loss: 1613.9142
Epoch [ 447/2600] -> Loss: 1614.8645
Epoch [ 448/2600] -> Loss: 1623.3087
Epoch [ 449/2600] -> Loss: 1617.6700
Epoch [ 450/2600] -> Loss: 1617.4307
Epoch [ 451/2600] -> Loss: 1624.0902
Epoch [ 452/2600] -> Loss: 1614.4945
Epoch [ 453/2600] -> Loss: 1613.2232
Epoch [ 454/2600] -> Loss: 1632.3208
Epoch [ 455/2600] -> Loss: 1604.7379
Epoch [ 456/2600] -> Loss: 1611.2927
Epoch [ 457/2600] -> Loss: 1609.0728
Epoch [ 458/2600] -> Loss: 1612.3520
Epoch [ 459/2600] -> Loss: 1615.5132
Epoch [ 460/2600] -> Loss: 1608.6334
Epoch [ 461/2600] -> Loss: 1607.6691
Epoch [ 462/2600] -> Loss: 1614.6018
Epoch [ 463/2600] -> Loss: 1614.4808
Epoch [ 464/2600] -> Loss: 1610.7643
Epoch [ 465/2600] -> Loss: 1624.6494
Epoch [ 466/2600] -> Loss: 1610.2924
Epoch [ 467/2600] -> Loss: 1606.2589
Epoch [ 468/2600] -> Loss: 1612.5131
Epoch [ 469/2600] -> Loss: 1611.8474
Epoch [ 470/2600] -> Loss: 1608.9226
Epoch [ 471/2600] -> Loss: 1615.7575
Epoch [ 472/2600] -> Loss: 1616.8540
Epoch [ 473/2600] -> Loss: 1620.5585
Epoch [ 474/2600] -> Loss: 1618.2675
Epoch [ 475/2600] -> Loss: 1623.3043
Epoch [ 476/2600] -> Loss: 1620.9590
Epoch [ 477/2600] -> Loss: 1612.0150
Epoch [ 478/2600] -> Loss: 1612.2173
Epoch [ 479/2600] -> Loss: 1614.3522
Epoch [ 480/2600] -> Loss: 1616.9020
Epoch [ 481/2600] -> Loss: 1613.1093
Epoch [ 482/2600] -> Loss: 1612.1585
Epoch [ 483/2600] -> Loss: 1604.1740
Epoch [ 484/2600] -> Loss: 1615.4132
Epoch [ 485/2600] -> Loss: 1609.2924
Epoch [ 486/2600] -> Loss: 1607.5107
Epoch [ 487/2600] -> Loss: 1612.1687
Epoch [ 488/2600] -> Loss: 1606.9362
Epoch [ 489/2600] -> Loss: 1608.1280
Epoch [ 490/2600] -> Loss: 1619.4783
Epoch [ 491/2600] -> Loss: 1618.4038
Epoch [ 492/2600] -> Loss: 1614.5358
Epoch [ 493/2600] -> Loss: 1611.7364
Epoch [ 494/2600] -> Loss: 1612.0843
Epoch [ 495/2600] -> Loss: 1614.1982
Epoch [ 496/2600] -> Loss: 1616.9136
Epoch [ 497/2600] -> Loss: 1606.0966
Epoch [ 498/2600] -> Loss: 1614.5356
Epoch [ 499/2600] -> Loss: 1611.9038
Epoch [ 500/2600] -> Loss: 1618.3678
Epoch [ 501/2600] -> Loss: 1620.1822
Epoch [ 502/2600] -> Loss: 1610.6384
Epoch [ 503/2600] -> Loss: 1607.0147
Epoch [ 504/2600] -> Loss: 1607.2806
Epoch [ 505/2600] -> Loss: 1624.7358
Epoch [ 506/2600] -> Loss: 1610.7863
Epoch [ 507/2600] -> Loss: 1617.8117
Epoch [ 508/2600] -> Loss: 1615.5647
Epoch [ 509/2600] -> Loss: 1615.3082
Epoch [ 510/2600] -> Loss: 1615.7999
Epoch [ 511/2600] -> Loss: 1629.5989
Epoch [ 512/2600] -> Loss: 1619.5601
Epoch [ 513/2600] -> Loss: 1609.6074
Epoch [ 514/2600] -> Loss: 1608.1130
Epoch [ 515/2600] -> Loss: 1609.4613
Epoch [ 516/2600] -> Loss: 1603.8858
Epoch [ 517/2600] -> Loss: 1605.1678
Epoch [ 518/2600] -> Loss: 1609.6458
Epoch [ 519/2600] -> Loss: 1616.3687
Epoch [ 520/2600] -> Loss: 1607.5643
Epoch [ 521/2600] -> Loss: 1606.1390
Epoch [ 522/2600] -> Loss: 1604.1832
Epoch [ 523/2600] -> Loss: 1613.4396
Epoch [ 524/2600] -> Loss: 1612.4739
Epoch [ 525/2600] -> Loss: 1611.3911
Epoch [ 526/2600] -> Loss: 1620.7768
Epoch [ 527/2600] -> Loss: 1618.0663
Epoch [ 528/2600] -> Loss: 1618.5275
Epoch [ 529/2600] -> Loss: 1611.3390
Epoch [ 530/2600] -> Loss: 1618.4680
Epoch [ 531/2600] -> Loss: 1613.8818
Epoch [ 532/2600] -> Loss: 1613.6764
Epoch [ 533/2600] -> Loss: 1615.6708
Epoch [ 534/2600] -> Loss: 1626.9895
Epoch [ 535/2600] -> Loss: 1608.5959
Epoch [ 536/2600] -> Loss: 1610.1256
Epoch [ 537/2600] -> Loss: 1613.9291
Epoch [ 538/2600] -> Loss: 1608.8385
Epoch [ 539/2600] -> Loss: 1627.9416
Epoch [ 540/2600] -> Loss: 1614.0194
Epoch [ 541/2600] -> Loss: 1607.1559
Epoch [ 542/2600] -> Loss: 1610.2324
Epoch [ 543/2600] -> Loss: 1608.4798
Epoch [ 544/2600] -> Loss: 1620.1792
