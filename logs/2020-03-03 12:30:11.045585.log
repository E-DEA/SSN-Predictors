--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
File location :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
No pre-trained models available, initializing model weights
--------------------------------------------------
Training model with: num_epochs=2600, start_lr=5e-05
Epoch [   1/2600] -> Loss: 8775.4323
Epoch [   2/2600] -> Loss: 6415.2584
Epoch [   3/2600] -> Loss: 6074.7907
Epoch [   4/2600] -> Loss: 9592.2231
Epoch [   5/2600] -> Loss: 5358.7793
Epoch [   6/2600] -> Loss: 4263.3794
Epoch [   7/2600] -> Loss: 10530.5698
Epoch [   8/2600] -> Loss: 4696.0202
Epoch [   9/2600] -> Loss: 4655.4272
Epoch [  10/2600] -> Loss: 10357.9793
Epoch [  11/2600] -> Loss: 12377.7409
Epoch [  12/2600] -> Loss: 12228.4398
Epoch [  13/2600] -> Loss: 12242.2398
Epoch [  14/2600] -> Loss: 11937.9994
Epoch [  15/2600] -> Loss: 11719.6111
Epoch [  16/2600] -> Loss: 11120.7238
Epoch    17: reducing learning rate of group 0 to 2.5000e-05.
Epoch [  17/2600] -> Loss: 10134.5347
Epoch [  18/2600] -> Loss: 8838.3833
Epoch [  19/2600] -> Loss: 7635.1711
Epoch [  20/2600] -> Loss: 6229.2080
Epoch [  21/2600] -> Loss: 4871.4881
Epoch [  22/2600] -> Loss: 3833.4825
Epoch [  23/2600] -> Loss: 3273.6830
Epoch [  24/2600] -> Loss: 3040.1944
Epoch [  25/2600] -> Loss: 2990.9108
Epoch [  26/2600] -> Loss: 2976.1162
Epoch [  27/2600] -> Loss: 2959.1865
Epoch [  28/2600] -> Loss: 2969.1107
Epoch [  29/2600] -> Loss: 2955.7378
Epoch [  30/2600] -> Loss: 2951.8053
Epoch [  31/2600] -> Loss: 2944.4297
Epoch [  32/2600] -> Loss: 2963.9052
Epoch [  33/2600] -> Loss: 2949.7130
Epoch [  34/2600] -> Loss: 2963.3291
Epoch [  35/2600] -> Loss: 2975.2609
Epoch [  36/2600] -> Loss: 2939.8278
Epoch [  37/2600] -> Loss: 2952.5203
Epoch [  38/2600] -> Loss: 2950.4921
Epoch [  39/2600] -> Loss: 2952.1518
Epoch [  40/2600] -> Loss: 2960.7128
Epoch [  41/2600] -> Loss: 2942.6056
Epoch [  42/2600] -> Loss: 2948.1969
Epoch [  43/2600] -> Loss: 2937.0263
Epoch [  44/2600] -> Loss: 2942.2830
Epoch [  45/2600] -> Loss: 2929.2094
Epoch [  46/2600] -> Loss: 2924.6356
Epoch [  47/2600] -> Loss: 2929.8742
Epoch [  48/2600] -> Loss: 2932.7017
Epoch [  49/2600] -> Loss: 2936.2070
Epoch [  50/2600] -> Loss: 2933.3119
Epoch [  51/2600] -> Loss: 2942.2678
Epoch [  52/2600] -> Loss: 2935.4739
Epoch [  53/2600] -> Loss: 2947.5067
Epoch [  54/2600] -> Loss: 2920.5316
Epoch [  55/2600] -> Loss: 2932.4684
Epoch [  56/2600] -> Loss: 2941.1489
Epoch [  57/2600] -> Loss: 2938.3392
Epoch [  58/2600] -> Loss: 2928.9046
Epoch [  59/2600] -> Loss: 2919.4877
Epoch [  60/2600] -> Loss: 2928.7392
Epoch [  61/2600] -> Loss: 2920.3537
Epoch [  62/2600] -> Loss: 2921.7584
Epoch [  63/2600] -> Loss: 2931.3321
Epoch [  64/2600] -> Loss: 2916.3177
Epoch [  65/2600] -> Loss: 2928.4999
Epoch [  66/2600] -> Loss: 2924.1554
Epoch [  67/2600] -> Loss: 2923.8720
Epoch [  68/2600] -> Loss: 2921.7331
Epoch [  69/2600] -> Loss: 2917.4933
Epoch [  70/2600] -> Loss: 2913.5733
Epoch [  71/2600] -> Loss: 2928.3416
Epoch [  72/2600] -> Loss: 2922.4421
Epoch [  73/2600] -> Loss: 2918.5789
Epoch [  74/2600] -> Loss: 2897.3461
Epoch [  75/2600] -> Loss: 2914.0741
Epoch [  76/2600] -> Loss: 2911.4238
Epoch [  77/2600] -> Loss: 2916.5352
Epoch [  78/2600] -> Loss: 2902.3740
Epoch [  79/2600] -> Loss: 2896.6273
Epoch [  80/2600] -> Loss: 2900.3522
Epoch [  81/2600] -> Loss: 2902.4159
Epoch [  82/2600] -> Loss: 2903.1338
Epoch [  83/2600] -> Loss: 2917.8609
Epoch [  84/2600] -> Loss: 2895.7050
Epoch [  85/2600] -> Loss: 2903.1440
Epoch [  86/2600] -> Loss: 2889.3569
Epoch [  87/2600] -> Loss: 2903.4394
Epoch [  88/2600] -> Loss: 2918.6328
Epoch [  89/2600] -> Loss: 2905.2961
Epoch [  90/2600] -> Loss: 2887.6203
Epoch [  91/2600] -> Loss: 2903.7441
Epoch [  92/2600] -> Loss: 2882.4683
Epoch [  93/2600] -> Loss: 2892.2527
Epoch [  94/2600] -> Loss: 2910.2948
Epoch [  95/2600] -> Loss: 2905.4073
Epoch [  96/2600] -> Loss: 2886.4321
Epoch [  97/2600] -> Loss: 2895.1923
Epoch [  98/2600] -> Loss: 2893.8856
Epoch [  99/2600] -> Loss: 2900.7934
Epoch [ 100/2600] -> Loss: 2884.6965
Epoch [ 101/2600] -> Loss: 2879.8799
Epoch [ 102/2600] -> Loss: 2875.6230
Epoch [ 103/2600] -> Loss: 2878.9086
Epoch [ 104/2600] -> Loss: 2897.1724
Epoch [ 105/2600] -> Loss: 2887.6910
Epoch [ 106/2600] -> Loss: 2883.9693
Epoch [ 107/2600] -> Loss: 2906.8673
Epoch [ 108/2600] -> Loss: 2887.0596
Epoch [ 109/2600] -> Loss: 2891.1469
Epoch [ 110/2600] -> Loss: 2886.2213
Epoch [ 111/2600] -> Loss: 2900.3393
Epoch [ 112/2600] -> Loss: 2889.6377
Epoch   113: reducing learning rate of group 0 to 1.2500e-05.
Epoch [ 113/2600] -> Loss: 2888.6039
Epoch [ 114/2600] -> Loss: 2872.5595
Epoch [ 115/2600] -> Loss: 2871.4948
Epoch [ 116/2600] -> Loss: 2887.4616
Epoch [ 117/2600] -> Loss: 2880.2105
Epoch [ 118/2600] -> Loss: 2866.0701
Epoch [ 119/2600] -> Loss: 2882.5469
Epoch [ 120/2600] -> Loss: 2878.1904
Epoch [ 121/2600] -> Loss: 2873.4426
Epoch [ 122/2600] -> Loss: 2898.0251
Epoch [ 123/2600] -> Loss: 2883.7635
Epoch [ 124/2600] -> Loss: 2902.6172
Epoch [ 125/2600] -> Loss: 2898.3605
Epoch [ 126/2600] -> Loss: 2878.5480
Epoch [ 127/2600] -> Loss: 2877.9743
Epoch [ 128/2600] -> Loss: 2878.4918
Epoch   129: reducing learning rate of group 0 to 6.2500e-06.
Epoch [ 129/2600] -> Loss: 2887.1689
Epoch [ 130/2600] -> Loss: 2883.8758
Epoch [ 131/2600] -> Loss: 2870.7807
Epoch [ 132/2600] -> Loss: 2874.3017
Epoch [ 133/2600] -> Loss: 2864.0777
Epoch [ 134/2600] -> Loss: 2874.1416
Epoch [ 135/2600] -> Loss: 2866.7923
Epoch [ 136/2600] -> Loss: 2883.9464
Epoch [ 137/2600] -> Loss: 2861.9478
Epoch [ 138/2600] -> Loss: 2876.5975
Epoch [ 139/2600] -> Loss: 2881.1525
Epoch [ 140/2600] -> Loss: 2883.8904
Epoch [ 141/2600] -> Loss: 2869.5290
Epoch [ 142/2600] -> Loss: 2882.0050
Epoch [ 143/2600] -> Loss: 2879.0942
Epoch [ 144/2600] -> Loss: 2877.8519
Epoch [ 145/2600] -> Loss: 2875.2961
Epoch [ 146/2600] -> Loss: 2876.7791
Epoch [ 147/2600] -> Loss: 2883.8565
Epoch   148: reducing learning rate of group 0 to 3.1250e-06.
Epoch [ 148/2600] -> Loss: 2872.7958
Epoch [ 149/2600] -> Loss: 2877.2278
Epoch [ 150/2600] -> Loss: 2868.1263
Epoch [ 151/2600] -> Loss: 2869.5122
Epoch [ 152/2600] -> Loss: 2869.7121
Epoch [ 153/2600] -> Loss: 2864.7190
Epoch [ 154/2600] -> Loss: 2889.1538
Epoch [ 155/2600] -> Loss: 2871.7585
Epoch [ 156/2600] -> Loss: 2874.3912
Epoch [ 157/2600] -> Loss: 2880.3104
Epoch [ 158/2600] -> Loss: 2863.7016
Epoch   159: reducing learning rate of group 0 to 1.5625e-06.
Epoch [ 159/2600] -> Loss: 2868.5182
Epoch [ 160/2600] -> Loss: 2865.8939
Epoch [ 161/2600] -> Loss: 2859.7818
Epoch [ 162/2600] -> Loss: 2879.7913
Epoch [ 163/2600] -> Loss: 2855.2603
Epoch [ 164/2600] -> Loss: 2866.2837
Epoch [ 165/2600] -> Loss: 2889.9141
Epoch [ 166/2600] -> Loss: 2889.2696
Epoch [ 167/2600] -> Loss: 2871.9032
Epoch [ 168/2600] -> Loss: 2857.0123
Epoch [ 169/2600] -> Loss: 2862.5132
Epoch [ 170/2600] -> Loss: 2869.4721
Epoch [ 171/2600] -> Loss: 2884.9779
Epoch [ 172/2600] -> Loss: 2873.0555
Epoch [ 173/2600] -> Loss: 2859.7429
Epoch   174: reducing learning rate of group 0 to 7.8125e-07.
Epoch [ 174/2600] -> Loss: 2870.6815
Epoch [ 175/2600] -> Loss: 2875.4111
Epoch [ 176/2600] -> Loss: 2871.3221
Epoch [ 177/2600] -> Loss: 2871.9779
Epoch [ 178/2600] -> Loss: 2880.3594
Epoch [ 179/2600] -> Loss: 2869.4099
Epoch [ 180/2600] -> Loss: 2887.5773
Epoch [ 181/2600] -> Loss: 2862.3150
Epoch [ 182/2600] -> Loss: 2869.2172
Epoch [ 183/2600] -> Loss: 2879.5768
Epoch [ 184/2600] -> Loss: 2872.0345
Epoch   185: reducing learning rate of group 0 to 3.9063e-07.
Epoch [ 185/2600] -> Loss: 2861.2294
Epoch [ 186/2600] -> Loss: 2897.5142
Epoch [ 187/2600] -> Loss: 2877.6554
Epoch [ 188/2600] -> Loss: 2872.1161
Epoch [ 189/2600] -> Loss: 2882.7483
Epoch [ 190/2600] -> Loss: 2890.0836
Epoch [ 191/2600] -> Loss: 2869.5014
Epoch [ 192/2600] -> Loss: 2873.7953
Epoch [ 193/2600] -> Loss: 2864.7552
Epoch [ 194/2600] -> Loss: 2886.0137
Epoch [ 195/2600] -> Loss: 2875.8822
Epoch   196: reducing learning rate of group 0 to 1.9531e-07.
Epoch [ 196/2600] -> Loss: 2876.3899
Epoch [ 197/2600] -> Loss: 2879.8798
Epoch [ 198/2600] -> Loss: 2862.7452
Epoch [ 199/2600] -> Loss: 2863.0860
--------------------------------------------------
Model checkpoint saved as FFNN_200.pth
--------------------------------------------------
Epoch [ 200/2600] -> Loss: 2891.9136
Epoch [ 201/2600] -> Loss: 2884.6614
Epoch [ 202/2600] -> Loss: 2870.4996
Epoch [ 203/2600] -> Loss: 2854.2634
Epoch [ 204/2600] -> Loss: 2864.5129
Epoch [ 205/2600] -> Loss: 2878.5344
Epoch [ 206/2600] -> Loss: 2853.5878
Epoch [ 207/2600] -> Loss: 2869.1495
Epoch [ 208/2600] -> Loss: 2864.4428
Epoch [ 209/2600] -> Loss: 2869.9119
Epoch [ 210/2600] -> Loss: 2874.4527
Epoch [ 211/2600] -> Loss: 2874.7926
Epoch [ 212/2600] -> Loss: 2865.0935
Epoch [ 213/2600] -> Loss: 2875.5369
Epoch [ 214/2600] -> Loss: 2870.9467
Epoch [ 215/2600] -> Loss: 2869.2831
Epoch [ 216/2600] -> Loss: 2865.2734
Epoch   217: reducing learning rate of group 0 to 9.7656e-08.
Epoch [ 217/2600] -> Loss: 2864.1554
Epoch [ 218/2600] -> Loss: 2878.1544
Epoch [ 219/2600] -> Loss: 2879.2967
Epoch [ 220/2600] -> Loss: 2876.9648
Epoch [ 221/2600] -> Loss: 2862.4020
Epoch [ 222/2600] -> Loss: 2858.7069
Epoch [ 223/2600] -> Loss: 2865.4283
Epoch [ 224/2600] -> Loss: 2868.0999
Epoch [ 225/2600] -> Loss: 2863.5407
Epoch [ 226/2600] -> Loss: 2904.7643
Epoch [ 227/2600] -> Loss: 2885.2557
Epoch   228: reducing learning rate of group 0 to 4.8828e-08.
Epoch [ 228/2600] -> Loss: 2879.3242
Epoch [ 229/2600] -> Loss: 2875.3774
Epoch [ 230/2600] -> Loss: 2904.7031
Epoch [ 231/2600] -> Loss: 2861.2826
Epoch [ 232/2600] -> Loss: 2857.6543
Epoch [ 233/2600] -> Loss: 2885.2058
Epoch [ 234/2600] -> Loss: 2878.7697
Epoch [ 235/2600] -> Loss: 2880.3494
Epoch [ 236/2600] -> Loss: 2851.0660
Epoch [ 237/2600] -> Loss: 2870.8994
Epoch [ 238/2600] -> Loss: 2868.7486
Epoch [ 239/2600] -> Loss: 2861.3872
Epoch [ 240/2600] -> Loss: 2854.2083
Epoch [ 241/2600] -> Loss: 2867.1011
Epoch [ 242/2600] -> Loss: 2867.7599
Epoch [ 243/2600] -> Loss: 2861.8847
Epoch [ 244/2600] -> Loss: 2866.6560
Epoch [ 245/2600] -> Loss: 2892.1922
Epoch [ 246/2600] -> Loss: 2868.2663
Epoch   247: reducing learning rate of group 0 to 2.4414e-08.
Epoch [ 247/2600] -> Loss: 2877.5208
Epoch [ 248/2600] -> Loss: 2882.0379
Epoch [ 249/2600] -> Loss: 2873.9334
Epoch [ 250/2600] -> Loss: 2858.2650
Epoch [ 251/2600] -> Loss: 2875.6085
Epoch [ 252/2600] -> Loss: 2869.6072
Epoch [ 253/2600] -> Loss: 2871.1540
Epoch [ 254/2600] -> Loss: 2877.1828
Epoch [ 255/2600] -> Loss: 2870.5523
Epoch [ 256/2600] -> Loss: 2890.0245
Epoch [ 257/2600] -> Loss: 2876.7519
Epoch   258: reducing learning rate of group 0 to 1.2207e-08.
Epoch [ 258/2600] -> Loss: 2875.0020
Epoch [ 259/2600] -> Loss: 2885.3603
Epoch [ 260/2600] -> Loss: 2863.6349
Epoch [ 261/2600] -> Loss: 2871.4754
Epoch [ 262/2600] -> Loss: 2858.6630
Epoch [ 263/2600] -> Loss: 2875.0797
Epoch [ 264/2600] -> Loss: 2877.6075
Epoch [ 265/2600] -> Loss: 2874.0229
Epoch [ 266/2600] -> Loss: 2881.8331
Epoch [ 267/2600] -> Loss: 2880.4155
Epoch [ 268/2600] -> Loss: 2875.5007
Epoch [ 269/2600] -> Loss: 2884.6459
Epoch [ 270/2600] -> Loss: 2881.0299
Epoch [ 271/2600] -> Loss: 2869.7961
Epoch [ 272/2600] -> Loss: 2870.9678
Epoch [ 273/2600] -> Loss: 2875.9049
Epoch [ 274/2600] -> Loss: 2869.1959
Epoch [ 275/2600] -> Loss: 2866.3397
Epoch [ 276/2600] -> Loss: 2870.8644
Epoch [ 277/2600] -> Loss: 2875.8311
Epoch [ 278/2600] -> Loss: 2855.0208
Epoch [ 279/2600] -> Loss: 2874.2867
Epoch [ 280/2600] -> Loss: 2865.1356
Epoch [ 281/2600] -> Loss: 2869.3097
Epoch [ 282/2600] -> Loss: 2879.3909
Epoch [ 283/2600] -> Loss: 2865.9572
Epoch [ 284/2600] -> Loss: 2896.3673
Epoch [ 285/2600] -> Loss: 2868.1567
Epoch [ 286/2600] -> Loss: 2878.9203
Epoch [ 287/2600] -> Loss: 2868.1084
Epoch [ 288/2600] -> Loss: 2886.5106
Epoch [ 289/2600] -> Loss: 2874.2161
Epoch [ 290/2600] -> Loss: 2855.7477
Epoch [ 291/2600] -> Loss: 2879.7767
Epoch [ 292/2600] -> Loss: 2877.1537
Epoch [ 293/2600] -> Loss: 2881.2882
Epoch [ 294/2600] -> Loss: 2886.4913
Epoch [ 295/2600] -> Loss: 2865.9491
Epoch [ 296/2600] -> Loss: 2868.2684
Epoch [ 297/2600] -> Loss: 2879.4458
Epoch [ 298/2600] -> Loss: 2859.5542
Epoch [ 299/2600] -> Loss: 2870.5370
Epoch [ 300/2600] -> Loss: 2867.0332
Epoch [ 301/2600] -> Loss: 2867.2474
Epoch [ 302/2600] -> Loss: 2873.0271
Epoch [ 303/2600] -> Loss: 2877.7416
Epoch [ 304/2600] -> Loss: 2879.1404
Epoch [ 305/2600] -> Loss: 2892.7338
Epoch [ 306/2600] -> Loss: 2864.4338
Epoch [ 307/2600] -> Loss: 2869.6749
Epoch [ 308/2600] -> Loss: 2883.7340
Epoch [ 309/2600] -> Loss: 2893.4728
Epoch [ 310/2600] -> Loss: 2872.2991
Epoch [ 311/2600] -> Loss: 2870.3531
Epoch [ 312/2600] -> Loss: 2860.6491
Epoch [ 313/2600] -> Loss: 2879.1060
Epoch [ 314/2600] -> Loss: 2873.0987
Epoch [ 315/2600] -> Loss: 2869.8484
Epoch [ 316/2600] -> Loss: 2874.5019
Epoch [ 317/2600] -> Loss: 2871.3350
Epoch [ 318/2600] -> Loss: 2874.7639
Epoch [ 319/2600] -> Loss: 2868.0826
Epoch [ 320/2600] -> Loss: 2891.1315
Epoch [ 321/2600] -> Loss: 2871.7559
Epoch [ 322/2600] -> Loss: 2871.7463
Epoch [ 323/2600] -> Loss: 2859.5349
Epoch [ 324/2600] -> Loss: 2888.9824
Epoch [ 325/2600] -> Loss: 2854.6573
Epoch [ 326/2600] -> Loss: 2887.7989
Epoch [ 327/2600] -> Loss: 2878.3048
Epoch [ 328/2600] -> Loss: 2877.5268
Epoch [ 329/2600] -> Loss: 2857.8651
Epoch [ 330/2600] -> Loss: 2865.5901
Epoch [ 331/2600] -> Loss: 2872.1106
Epoch [ 332/2600] -> Loss: 2871.0576
Epoch [ 333/2600] -> Loss: 2874.8118
Epoch [ 334/2600] -> Loss: 2875.9663
Epoch [ 335/2600] -> Loss: 2892.3893
Epoch [ 336/2600] -> Loss: 2869.3461
Epoch [ 337/2600] -> Loss: 2877.4395
Epoch [ 338/2600] -> Loss: 2865.4579
Epoch [ 339/2600] -> Loss: 2884.3691
Epoch [ 340/2600] -> Loss: 2868.4923
Epoch [ 341/2600] -> Loss: 2864.7614
Epoch [ 342/2600] -> Loss: 2890.6360
Epoch [ 343/2600] -> Loss: 2872.7074
Epoch [ 344/2600] -> Loss: 2873.7597
Epoch [ 345/2600] -> Loss: 2863.7751
Epoch [ 346/2600] -> Loss: 2876.2173
Epoch [ 347/2600] -> Loss: 2881.6791
Epoch [ 348/2600] -> Loss: 2871.5343
Epoch [ 349/2600] -> Loss: 2880.8438
Epoch [ 350/2600] -> Loss: 2872.6165
Epoch [ 351/2600] -> Loss: 2869.2592
Epoch [ 352/2600] -> Loss: 2864.5732
Epoch [ 353/2600] -> Loss: 2871.7456
Epoch [ 354/2600] -> Loss: 2886.7771
Epoch [ 355/2600] -> Loss: 2873.6224
Epoch [ 356/2600] -> Loss: 2874.8299
Epoch [ 357/2600] -> Loss: 2874.3750
Epoch [ 358/2600] -> Loss: 2881.3106
Epoch [ 359/2600] -> Loss: 2864.6276
Epoch [ 360/2600] -> Loss: 2873.3615
Epoch [ 361/2600] -> Loss: 2868.4542
Epoch [ 362/2600] -> Loss: 2871.0862
Epoch [ 363/2600] -> Loss: 2885.0817
Epoch [ 364/2600] -> Loss: 2880.4742
Epoch [ 365/2600] -> Loss: 2852.8788
Epoch [ 366/2600] -> Loss: 2858.1396
Epoch [ 367/2600] -> Loss: 2867.7867
Epoch [ 368/2600] -> Loss: 2872.2290
Epoch [ 369/2600] -> Loss: 2864.4022
Epoch [ 370/2600] -> Loss: 2877.5100
Epoch [ 371/2600] -> Loss: 2873.5641
Epoch [ 372/2600] -> Loss: 2881.1261
Epoch [ 373/2600] -> Loss: 2886.1212
Epoch [ 374/2600] -> Loss: 2870.1452
Epoch [ 375/2600] -> Loss: 2870.2865
Epoch [ 376/2600] -> Loss: 2888.8205
Epoch [ 377/2600] -> Loss: 2862.5833
Epoch [ 378/2600] -> Loss: 2883.1904
Epoch [ 379/2600] -> Loss: 2880.1175
Epoch [ 380/2600] -> Loss: 2869.6629
Epoch [ 381/2600] -> Loss: 2870.2867
Epoch [ 382/2600] -> Loss: 2865.3104
Epoch [ 383/2600] -> Loss: 2870.2427
Epoch [ 384/2600] -> Loss: 2868.4610
Epoch [ 385/2600] -> Loss: 2864.3346
Epoch [ 386/2600] -> Loss: 2876.8014
Epoch [ 387/2600] -> Loss: 2881.7768
Epoch [ 388/2600] -> Loss: 2882.2060
Epoch [ 389/2600] -> Loss: 2864.4536
Epoch [ 390/2600] -> Loss: 2869.4277
Epoch [ 391/2600] -> Loss: 2863.9089
Epoch [ 392/2600] -> Loss: 2878.1284
Epoch [ 393/2600] -> Loss: 2868.9347
Epoch [ 394/2600] -> Loss: 2869.8446
Epoch [ 395/2600] -> Loss: 2873.8123
Epoch [ 396/2600] -> Loss: 2865.9464
Epoch [ 397/2600] -> Loss: 2868.3166
Epoch [ 398/2600] -> Loss: 2889.9339
Epoch [ 399/2600] -> Loss: 2884.9434
--------------------------------------------------
Model checkpoint saved as FFNN_400.pth
--------------------------------------------------
Epoch [ 400/2600] -> Loss: 2873.9950
Epoch [ 401/2600] -> Loss: 2870.9504
Epoch [ 402/2600] -> Loss: 2876.2531
Epoch [ 403/2600] -> Loss: 2871.8798
Epoch [ 404/2600] -> Loss: 2892.9247
Epoch [ 405/2600] -> Loss: 2876.9854
Epoch [ 406/2600] -> Loss: 2886.5058
Epoch [ 407/2600] -> Loss: 2880.4189
Epoch [ 408/2600] -> Loss: 2892.2648
Epoch [ 409/2600] -> Loss: 2873.2195
Epoch [ 410/2600] -> Loss: 2879.0250
Epoch [ 411/2600] -> Loss: 2880.8347
Epoch [ 412/2600] -> Loss: 2884.0133
Epoch [ 413/2600] -> Loss: 2872.0842
Epoch [ 414/2600] -> Loss: 2873.7652
Epoch [ 415/2600] -> Loss: 2872.3025
Epoch [ 416/2600] -> Loss: 2872.8666
Epoch [ 417/2600] -> Loss: 2865.6904
Epoch [ 418/2600] -> Loss: 2867.3645
Epoch [ 419/2600] -> Loss: 2883.5009
Epoch [ 420/2600] -> Loss: 2874.2470
Epoch [ 421/2600] -> Loss: 2866.9441
Epoch [ 422/2600] -> Loss: 2862.0880
Epoch [ 423/2600] -> Loss: 2859.4339
Epoch [ 424/2600] -> Loss: 2870.8373
Epoch [ 425/2600] -> Loss: 2882.1896
Epoch [ 426/2600] -> Loss: 2870.4658
Epoch [ 427/2600] -> Loss: 2890.9544
Epoch [ 428/2600] -> Loss: 2878.5756
Epoch [ 429/2600] -> Loss: 2870.6663
Epoch [ 430/2600] -> Loss: 2882.0640
Epoch [ 431/2600] -> Loss: 2862.5299
Epoch [ 432/2600] -> Loss: 2865.6448
Epoch [ 433/2600] -> Loss: 2874.0540
Epoch [ 434/2600] -> Loss: 2866.4674
Epoch [ 435/2600] -> Loss: 2873.0038
Epoch [ 436/2600] -> Loss: 2868.1660
Epoch [ 437/2600] -> Loss: 2866.9146
Epoch [ 438/2600] -> Loss: 2863.6150
Epoch [ 439/2600] -> Loss: 2877.8039
Epoch [ 440/2600] -> Loss: 2872.0933
Epoch [ 441/2600] -> Loss: 2871.5757
Epoch [ 442/2600] -> Loss: 2868.5222
Epoch [ 443/2600] -> Loss: 2880.6478
Epoch [ 444/2600] -> Loss: 2872.1004
Epoch [ 445/2600] -> Loss: 2864.7808
Epoch [ 446/2600] -> Loss: 2883.6279
Epoch [ 447/2600] -> Loss: 2876.2172
Epoch [ 448/2600] -> Loss: 2879.3956
Epoch [ 449/2600] -> Loss: 2868.9634
Epoch [ 450/2600] -> Loss: 2881.3814
Epoch [ 451/2600] -> Loss: 2878.6098
Epoch [ 452/2600] -> Loss: 2868.7210
Epoch [ 453/2600] -> Loss: 2871.1930
Epoch [ 454/2600] -> Loss: 2910.0086
Epoch [ 455/2600] -> Loss: 2856.3768
Epoch [ 456/2600] -> Loss: 2866.7892
Epoch [ 457/2600] -> Loss: 2870.7423
Epoch [ 458/2600] -> Loss: 2875.4494
Epoch [ 459/2600] -> Loss: 2877.7956
Epoch [ 460/2600] -> Loss: 2865.9203
Epoch [ 461/2600] -> Loss: 2867.3591
Epoch [ 462/2600] -> Loss: 2872.2406
Epoch [ 463/2600] -> Loss: 2876.6826
Epoch [ 464/2600] -> Loss: 2865.0177
Epoch [ 465/2600] -> Loss: 2891.0803
Epoch [ 466/2600] -> Loss: 2865.7609
Epoch [ 467/2600] -> Loss: 2855.7054
Epoch [ 468/2600] -> Loss: 2875.6510
Epoch [ 469/2600] -> Loss: 2866.8902
Epoch [ 470/2600] -> Loss: 2864.0244
Epoch [ 471/2600] -> Loss: 2874.8198
Epoch [ 472/2600] -> Loss: 2879.7552
Epoch [ 473/2600] -> Loss: 2880.4988
Epoch [ 474/2600] -> Loss: 2882.6522
Epoch [ 475/2600] -> Loss: 2886.9855
Epoch [ 476/2600] -> Loss: 2885.0013
Epoch [ 477/2600] -> Loss: 2871.7549
Epoch [ 478/2600] -> Loss: 2863.0435
Epoch [ 479/2600] -> Loss: 2865.0011
Epoch [ 480/2600] -> Loss: 2877.3080
Epoch [ 481/2600] -> Loss: 2864.5050
Epoch [ 482/2600] -> Loss: 2868.6099
Epoch [ 483/2600] -> Loss: 2860.3883
Epoch [ 484/2600] -> Loss: 2868.2099
Epoch [ 485/2600] -> Loss: 2864.2182
Epoch [ 486/2600] -> Loss: 2875.9257
Epoch [ 487/2600] -> Loss: 2870.2127
Epoch [ 488/2600] -> Loss: 2858.2269
Epoch [ 489/2600] -> Loss: 2868.4956
Epoch [ 490/2600] -> Loss: 2886.2448
Epoch [ 491/2600] -> Loss: 2882.2506
Epoch [ 492/2600] -> Loss: 2881.1937
Epoch [ 493/2600] -> Loss: 2869.9150
Epoch [ 494/2600] -> Loss: 2868.1975
Epoch [ 495/2600] -> Loss: 2875.2985
Epoch [ 496/2600] -> Loss: 2876.0019
Epoch [ 497/2600] -> Loss: 2861.9898
Epoch [ 498/2600] -> Loss: 2874.6457
Epoch [ 499/2600] -> Loss: 2866.9739
Epoch [ 500/2600] -> Loss: 2879.4175
Epoch [ 501/2600] -> Loss: 2871.9376
Epoch [ 502/2600] -> Loss: 2870.5802
Epoch [ 503/2600] -> Loss: 2862.7051
Epoch [ 504/2600] -> Loss: 2860.9534
Epoch [ 505/2600] -> Loss: 2883.3709
Epoch [ 506/2600] -> Loss: 2870.6859
Epoch [ 507/2600] -> Loss: 2873.8443
Epoch [ 508/2600] -> Loss: 2874.6896
Epoch [ 509/2600] -> Loss: 2874.7132
Epoch [ 510/2600] -> Loss: 2878.0791
Epoch [ 511/2600] -> Loss: 2897.9612
Epoch [ 512/2600] -> Loss: 2886.8967
Epoch [ 513/2600] -> Loss: 2862.6655
Epoch [ 514/2600] -> Loss: 2860.2072
Epoch [ 515/2600] -> Loss: 2861.4784
Epoch [ 516/2600] -> Loss: 2867.8161
Epoch [ 517/2600] -> Loss: 2851.5493
Epoch [ 518/2600] -> Loss: 2863.8085
Epoch [ 519/2600] -> Loss: 2880.5266
Epoch [ 520/2600] -> Loss: 2854.4746
Epoch [ 521/2600] -> Loss: 2867.0685
Epoch [ 522/2600] -> Loss: 2854.3333
Epoch [ 523/2600] -> Loss: 2871.3434
Epoch [ 524/2600] -> Loss: 2876.0761
Epoch [ 525/2600] -> Loss: 2870.2597
Epoch [ 526/2600] -> Loss: 2888.8007
Epoch [ 527/2600] -> Loss: 2874.4522
Epoch [ 528/2600] -> Loss: 2876.7688
Epoch [ 529/2600] -> Loss: 2870.5482
Epoch [ 530/2600] -> Loss: 2887.2280
Epoch [ 531/2600] -> Loss: 2870.5613
Epoch [ 532/2600] -> Loss: 2876.7022
Epoch [ 533/2600] -> Loss: 2863.6932
Epoch [ 534/2600] -> Loss: 2897.2630
Epoch [ 535/2600] -> Loss: 2870.1350
Epoch [ 536/2600] -> Loss: 2867.7821
Epoch [ 537/2600] -> Loss: 2871.7590
Epoch [ 538/2600] -> Loss: 2866.3025
Epoch [ 539/2600] -> Loss: 2882.8034
Epoch [ 540/2600] -> Loss: 2866.5184
Epoch [ 541/2600] -> Loss: 2865.2723
Epoch [ 542/2600] -> Loss: 2866.5330
Epoch [ 543/2600] -> Loss: 2862.4532
Epoch [ 544/2600] -> Loss: 2889.0223
Epoch [ 545/2600] -> Loss: 2875.4945
Epoch [ 546/2600] -> Loss: 2868.9093
Epoch [ 547/2600] -> Loss: 2867.5944
Epoch [ 548/2600] -> Loss: 2869.7939
Epoch [ 549/2600] -> Loss: 2881.2969
Epoch [ 550/2600] -> Loss: 2862.3737
Epoch [ 551/2600] -> Loss: 2882.3597
Epoch [ 552/2600] -> Loss: 2881.3127
Epoch [ 553/2600] -> Loss: 2875.5872
Epoch [ 554/2600] -> Loss: 2877.0143
Epoch [ 555/2600] -> Loss: 2876.6657
Epoch [ 556/2600] -> Loss: 2864.3242
Epoch [ 557/2600] -> Loss: 2875.8566
Epoch [ 558/2600] -> Loss: 2877.3143
Epoch [ 559/2600] -> Loss: 2867.9972
Epoch [ 560/2600] -> Loss: 2872.9528
Epoch [ 561/2600] -> Loss: 2874.7720
Epoch [ 562/2600] -> Loss: 2875.9122
Epoch [ 563/2600] -> Loss: 2896.9226
Epoch [ 564/2600] -> Loss: 2860.0354
Epoch [ 565/2600] -> Loss: 2862.3331
Epoch [ 566/2600] -> Loss: 2866.4278
Epoch [ 567/2600] -> Loss: 2887.4424
Epoch [ 568/2600] -> Loss: 2866.3925
Epoch [ 569/2600] -> Loss: 2876.5245
Epoch [ 570/2600] -> Loss: 2879.1755
Epoch [ 571/2600] -> Loss: 2878.0649
Epoch [ 572/2600] -> Loss: 2873.8097
Epoch [ 573/2600] -> Loss: 2866.9740
Epoch [ 574/2600] -> Loss: 2866.4229
Epoch [ 575/2600] -> Loss: 2867.7556
Epoch [ 576/2600] -> Loss: 2886.0938
Epoch [ 577/2600] -> Loss: 2894.5229
Epoch [ 578/2600] -> Loss: 2873.3530
Epoch [ 579/2600] -> Loss: 2862.4890
Epoch [ 580/2600] -> Loss: 2861.1690
Epoch [ 581/2600] -> Loss: 2889.0962
Epoch [ 582/2600] -> Loss: 2886.5523
Epoch [ 583/2600] -> Loss: 2872.5431
Epoch [ 584/2600] -> Loss: 2882.2142
Epoch [ 585/2600] -> Loss: 2863.9666
Epoch [ 586/2600] -> Loss: 2862.9034
Epoch [ 587/2600] -> Loss: 2878.9733
Epoch [ 588/2600] -> Loss: 2868.7501
Epoch [ 589/2600] -> Loss: 2873.3030
Epoch [ 590/2600] -> Loss: 2878.0607
Epoch [ 591/2600] -> Loss: 2868.5382
Epoch [ 592/2600] -> Loss: 2873.5676
Epoch [ 593/2600] -> Loss: 2865.4246
Epoch [ 594/2600] -> Loss: 2884.4110
Epoch [ 595/2600] -> Loss: 2875.9761
Epoch [ 596/2600] -> Loss: 2869.2383
Epoch [ 597/2600] -> Loss: 2868.7562
Epoch [ 598/2600] -> Loss: 2872.2765
Epoch [ 599/2600] -> Loss: 2871.9500
--------------------------------------------------
Model checkpoint saved as FFNN_600.pth
--------------------------------------------------
Epoch [ 600/2600] -> Loss: 2880.8199
Epoch [ 601/2600] -> Loss: 2867.5761
Epoch [ 602/2600] -> Loss: 2864.5552
Epoch [ 603/2600] -> Loss: 2859.8969
Epoch [ 604/2600] -> Loss: 2866.4610
Epoch [ 605/2600] -> Loss: 2879.5368
Epoch [ 606/2600] -> Loss: 2856.2197
Epoch [ 607/2600] -> Loss: 2864.6948
Epoch [ 608/2600] -> Loss: 2863.7034
Epoch [ 609/2600] -> Loss: 2871.7462
Epoch [ 610/2600] -> Loss: 2857.2745
Epoch [ 611/2600] -> Loss: 2868.5022
Epoch [ 612/2600] -> Loss: 2888.7841
Epoch [ 613/2600] -> Loss: 2877.1981
Epoch [ 614/2600] -> Loss: 2879.9037
Epoch [ 615/2600] -> Loss: 2876.6479
Epoch [ 616/2600] -> Loss: 2872.1909
Epoch [ 617/2600] -> Loss: 2871.2354
Epoch [ 618/2600] -> Loss: 2864.0892
Epoch [ 619/2600] -> Loss: 2871.3326
Epoch [ 620/2600] -> Loss: 2860.0697
Epoch [ 621/2600] -> Loss: 2870.2144
Epoch [ 622/2600] -> Loss: 2885.5780
Epoch [ 623/2600] -> Loss: 2873.6362
Epoch [ 624/2600] -> Loss: 2877.6447
Epoch [ 625/2600] -> Loss: 2866.4594
Epoch [ 626/2600] -> Loss: 2867.2857
Epoch [ 627/2600] -> Loss: 2866.6294
Epoch [ 628/2600] -> Loss: 2880.8965
Epoch [ 629/2600] -> Loss: 2870.9436
Epoch [ 630/2600] -> Loss: 2880.4744
Epoch [ 631/2600] -> Loss: 2899.2751
Epoch [ 632/2600] -> Loss: 2869.0302
Epoch [ 633/2600] -> Loss: 2884.8457
Epoch [ 634/2600] -> Loss: 2869.3404
Epoch [ 635/2600] -> Loss: 2864.7171
Epoch [ 636/2600] -> Loss: 2876.4702
Epoch [ 637/2600] -> Loss: 2871.2784
Epoch [ 638/2600] -> Loss: 2884.1219
Epoch [ 639/2600] -> Loss: 2892.8162
Epoch [ 640/2600] -> Loss: 2874.5340
Epoch [ 641/2600] -> Loss: 2865.7163
Epoch [ 642/2600] -> Loss: 2887.6541
Epoch [ 643/2600] -> Loss: 2863.7982
Epoch [ 644/2600] -> Loss: 2882.0904
Epoch [ 645/2600] -> Loss: 2879.3740
Epoch [ 646/2600] -> Loss: 2884.6724
Epoch [ 647/2600] -> Loss: 2863.6643
Epoch [ 648/2600] -> Loss: 2873.0699
Epoch [ 649/2600] -> Loss: 2867.1545
Epoch [ 650/2600] -> Loss: 2877.8551
Epoch [ 651/2600] -> Loss: 2870.5913
Epoch [ 652/2600] -> Loss: 2886.9591
Epoch [ 653/2600] -> Loss: 2861.7549
Epoch [ 654/2600] -> Loss: 2869.7823
Epoch [ 655/2600] -> Loss: 2877.2342
Epoch [ 656/2600] -> Loss: 2876.4987
Epoch [ 657/2600] -> Loss: 2855.6307
Epoch [ 658/2600] -> Loss: 2872.8625
Epoch [ 659/2600] -> Loss: 2869.2109
Epoch [ 660/2600] -> Loss: 2865.7093
Epoch [ 661/2600] -> Loss: 2870.6992
Epoch [ 662/2600] -> Loss: 2863.3392
Epoch [ 663/2600] -> Loss: 2869.7589
Epoch [ 664/2600] -> Loss: 2862.2110
Epoch [ 665/2600] -> Loss: 2874.8456
Epoch [ 666/2600] -> Loss: 2859.6194
Epoch [ 667/2600] -> Loss: 2862.1553
Epoch [ 668/2600] -> Loss: 2873.4676
Epoch [ 669/2600] -> Loss: 2879.5501
Epoch [ 670/2600] -> Loss: 2870.3494
Epoch [ 671/2600] -> Loss: 2877.3995
Epoch [ 672/2600] -> Loss: 2881.7937
Epoch [ 673/2600] -> Loss: 2864.7845
Epoch [ 674/2600] -> Loss: 2863.7039
Epoch [ 675/2600] -> Loss: 2877.6239
Epoch [ 676/2600] -> Loss: 2873.6525
Epoch [ 677/2600] -> Loss: 2876.5950
Epoch [ 678/2600] -> Loss: 2864.7988
Epoch [ 679/2600] -> Loss: 2866.9041
Epoch [ 680/2600] -> Loss: 2880.1356
Epoch [ 681/2600] -> Loss: 2889.0750
Epoch [ 682/2600] -> Loss: 2887.4341
Epoch [ 683/2600] -> Loss: 2865.7164
Epoch [ 684/2600] -> Loss: 2875.0403
Epoch [ 685/2600] -> Loss: 2886.5008
Epoch [ 686/2600] -> Loss: 2865.5299
Epoch [ 687/2600] -> Loss: 2872.9736
Epoch [ 688/2600] -> Loss: 2872.3162
Epoch [ 689/2600] -> Loss: 2886.0467
Epoch [ 690/2600] -> Loss: 2866.3186
Epoch [ 691/2600] -> Loss: 2867.3050
Epoch [ 692/2600] -> Loss: 2871.7807
Epoch [ 693/2600] -> Loss: 2875.9792
Epoch [ 694/2600] -> Loss: 2875.8739
Epoch [ 695/2600] -> Loss: 2900.4888
Epoch [ 696/2600] -> Loss: 2874.9352
Epoch [ 697/2600] -> Loss: 2876.4048
Epoch [ 698/2600] -> Loss: 2861.9094
Epoch [ 699/2600] -> Loss: 2876.2882
Epoch [ 700/2600] -> Loss: 2864.1304
Epoch [ 701/2600] -> Loss: 2884.0544
Epoch [ 702/2600] -> Loss: 2877.9017
Epoch [ 703/2600] -> Loss: 2872.0673
Epoch [ 704/2600] -> Loss: 2869.1441
Epoch [ 705/2600] -> Loss: 2889.7884
Epoch [ 706/2600] -> Loss: 2867.0528
Epoch [ 707/2600] -> Loss: 2878.5653
Epoch [ 708/2600] -> Loss: 2880.5362
Epoch [ 709/2600] -> Loss: 2884.3084
Epoch [ 710/2600] -> Loss: 2874.5803
Epoch [ 711/2600] -> Loss: 2871.5261
Epoch [ 712/2600] -> Loss: 2867.2441
Epoch [ 713/2600] -> Loss: 2881.4246
Epoch [ 714/2600] -> Loss: 2869.8512
Epoch [ 715/2600] -> Loss: 2874.7737
Epoch [ 716/2600] -> Loss: 2880.0894
Epoch [ 717/2600] -> Loss: 2859.6958
Epoch [ 718/2600] -> Loss: 2873.2169
Epoch [ 719/2600] -> Loss: 2879.6815
Epoch [ 720/2600] -> Loss: 2874.2820
Epoch [ 721/2600] -> Loss: 2862.5102
Epoch [ 722/2600] -> Loss: 2861.1191
Epoch [ 723/2600] -> Loss: 2880.8089
Epoch [ 724/2600] -> Loss: 2870.0575
Epoch [ 725/2600] -> Loss: 2866.5587
Epoch [ 726/2600] -> Loss: 2873.4285
Epoch [ 727/2600] -> Loss: 2860.8279
Epoch [ 728/2600] -> Loss: 2861.6884
Epoch [ 729/2600] -> Loss: 2878.0133
Epoch [ 730/2600] -> Loss: 2863.5072
Epoch [ 731/2600] -> Loss: 2866.9003
Epoch [ 732/2600] -> Loss: 2885.9033
Epoch [ 733/2600] -> Loss: 2875.5039
Epoch [ 734/2600] -> Loss: 2865.4912
Epoch [ 735/2600] -> Loss: 2873.5999
Epoch [ 736/2600] -> Loss: 2870.3477
Epoch [ 737/2600] -> Loss: 2868.6980
Epoch [ 738/2600] -> Loss: 2889.9930
Epoch [ 739/2600] -> Loss: 2892.5026
Epoch [ 740/2600] -> Loss: 2855.1020
Epoch [ 741/2600] -> Loss: 2873.9012
Epoch [ 742/2600] -> Loss: 2875.4577
Epoch [ 743/2600] -> Loss: 2862.6281
Epoch [ 744/2600] -> Loss: 2879.6345
Epoch [ 745/2600] -> Loss: 2867.5884
Epoch [ 746/2600] -> Loss: 2873.9416
Epoch [ 747/2600] -> Loss: 2875.0464
Epoch [ 748/2600] -> Loss: 2864.1662
Epoch [ 749/2600] -> Loss: 2875.5206
Epoch [ 750/2600] -> Loss: 2883.1187
Epoch [ 751/2600] -> Loss: 2862.6044
Epoch [ 752/2600] -> Loss: 2869.8018
Epoch [ 753/2600] -> Loss: 2862.8906
Epoch [ 754/2600] -> Loss: 2867.8807
Epoch [ 755/2600] -> Loss: 2879.8492
Epoch [ 756/2600] -> Loss: 2862.9682
Epoch [ 757/2600] -> Loss: 2869.6872
Epoch [ 758/2600] -> Loss: 2871.2090
Epoch [ 759/2600] -> Loss: 2887.4950
Epoch [ 760/2600] -> Loss: 2879.7445
Epoch [ 761/2600] -> Loss: 2872.0526
Epoch [ 762/2600] -> Loss: 2876.5557
Epoch [ 763/2600] -> Loss: 2864.7269
Epoch [ 764/2600] -> Loss: 2872.0441
Epoch [ 765/2600] -> Loss: 2864.1859
Epoch [ 766/2600] -> Loss: 2870.2023
Epoch [ 767/2600] -> Loss: 2879.1573
Epoch [ 768/2600] -> Loss: 2867.4484
Epoch [ 769/2600] -> Loss: 2864.7031
Epoch [ 770/2600] -> Loss: 2880.6829
Epoch [ 771/2600] -> Loss: 2867.9843
Epoch [ 772/2600] -> Loss: 2868.9274
Epoch [ 773/2600] -> Loss: 2872.0854
Epoch [ 774/2600] -> Loss: 2870.9233
Epoch [ 775/2600] -> Loss: 2867.9321
Epoch [ 776/2600] -> Loss: 2876.4778
Epoch [ 777/2600] -> Loss: 2881.6044
Epoch [ 778/2600] -> Loss: 2889.0678
Epoch [ 779/2600] -> Loss: 2867.7233
Epoch [ 780/2600] -> Loss: 2877.6564
Epoch [ 781/2600] -> Loss: 2871.7049
Epoch [ 782/2600] -> Loss: 2870.8238
Epoch [ 783/2600] -> Loss: 2894.6745
Epoch [ 784/2600] -> Loss: 2865.3198
Epoch [ 785/2600] -> Loss: 2880.7967
Epoch [ 786/2600] -> Loss: 2870.6854
Epoch [ 787/2600] -> Loss: 2881.6897
Epoch [ 788/2600] -> Loss: 2877.7197
Epoch [ 789/2600] -> Loss: 2882.7553
Epoch [ 790/2600] -> Loss: 2886.3065
Epoch [ 791/2600] -> Loss: 2869.8010
Epoch [ 792/2600] -> Loss: 2868.3735
Epoch [ 793/2600] -> Loss: 2871.6533
Epoch [ 794/2600] -> Loss: 2864.1363
Epoch [ 795/2600] -> Loss: 2871.3470
Epoch [ 796/2600] -> Loss: 2879.4068
Epoch [ 797/2600] -> Loss: 2880.1786
Epoch [ 798/2600] -> Loss: 2865.2044
Epoch [ 799/2600] -> Loss: 2864.5214
--------------------------------------------------
Model checkpoint saved as FFNN_800.pth
--------------------------------------------------
Epoch [ 800/2600] -> Loss: 2867.5604
Epoch [ 801/2600] -> Loss: 2872.0685
Epoch [ 802/2600] -> Loss: 2867.5471
Epoch [ 803/2600] -> Loss: 2862.6802
Epoch [ 804/2600] -> Loss: 2867.7894
Epoch [ 805/2600] -> Loss: 2869.2731
Epoch [ 806/2600] -> Loss: 2885.7534
Epoch [ 807/2600] -> Loss: 2881.6107
Epoch [ 808/2600] -> Loss: 2878.5904
Epoch [ 809/2600] -> Loss: 2881.8787
Epoch [ 810/2600] -> Loss: 2864.1757
Epoch [ 811/2600] -> Loss: 2892.4675
Epoch [ 812/2600] -> Loss: 2861.3652
Epoch [ 813/2600] -> Loss: 2869.6898
Epoch [ 814/2600] -> Loss: 2865.6680
Epoch [ 815/2600] -> Loss: 2866.4756
Epoch [ 816/2600] -> Loss: 2873.2503
Epoch [ 817/2600] -> Loss: 2865.9370
Epoch [ 818/2600] -> Loss: 2878.4818
Epoch [ 819/2600] -> Loss: 2875.3634
Epoch [ 820/2600] -> Loss: 2883.1008
Epoch [ 821/2600] -> Loss: 2854.0236
Epoch [ 822/2600] -> Loss: 2853.8331
Epoch [ 823/2600] -> Loss: 2867.2955
Epoch [ 824/2600] -> Loss: 2881.0567
Epoch [ 825/2600] -> Loss: 2878.0427
Epoch [ 826/2600] -> Loss: 2863.0669
Epoch [ 827/2600] -> Loss: 2862.4616
Epoch [ 828/2600] -> Loss: 2858.1533
Epoch [ 829/2600] -> Loss: 2878.7298
Epoch [ 830/2600] -> Loss: 2872.7558
Epoch [ 831/2600] -> Loss: 2877.7835
Epoch [ 832/2600] -> Loss: 2867.5849
Epoch [ 833/2600] -> Loss: 2871.0301
Epoch [ 834/2600] -> Loss: 2880.8902
Epoch [ 835/2600] -> Loss: 2874.5208
Epoch [ 836/2600] -> Loss: 2876.3411
Epoch [ 837/2600] -> Loss: 2874.4695
Epoch [ 838/2600] -> Loss: 2864.9592
Epoch [ 839/2600] -> Loss: 2880.2920
Epoch [ 840/2600] -> Loss: 2876.3987
Epoch [ 841/2600] -> Loss: 2867.3744
Epoch [ 842/2600] -> Loss: 2866.8623
Epoch [ 843/2600] -> Loss: 2872.3854
Epoch [ 844/2600] -> Loss: 2869.5267
Epoch [ 845/2600] -> Loss: 2874.4892
Epoch [ 846/2600] -> Loss: 2878.0775
Epoch [ 847/2600] -> Loss: 2882.5740
Epoch [ 848/2600] -> Loss: 2867.9728
Epoch [ 849/2600] -> Loss: 2874.7530
Epoch [ 850/2600] -> Loss: 2881.8971
Epoch [ 851/2600] -> Loss: 2859.0789
Epoch [ 852/2600] -> Loss: 2872.2523
Epoch [ 853/2600] -> Loss: 2883.4807
Epoch [ 854/2600] -> Loss: 2884.1624
Epoch [ 855/2600] -> Loss: 2874.7330
Epoch [ 856/2600] -> Loss: 2877.6742
Epoch [ 857/2600] -> Loss: 2879.1418
Epoch [ 858/2600] -> Loss: 2867.8101
Epoch [ 859/2600] -> Loss: 2877.9793
Epoch [ 860/2600] -> Loss: 2875.0211
Epoch [ 861/2600] -> Loss: 2870.1183
Epoch [ 862/2600] -> Loss: 2870.9563
Epoch [ 863/2600] -> Loss: 2880.0655
Epoch [ 864/2600] -> Loss: 2879.6101
Epoch [ 865/2600] -> Loss: 2890.7964
Epoch [ 866/2600] -> Loss: 2879.2283
Epoch [ 867/2600] -> Loss: 2870.0198
Epoch [ 868/2600] -> Loss: 2882.6066
Epoch [ 869/2600] -> Loss: 2860.2690
Epoch [ 870/2600] -> Loss: 2879.0928
Epoch [ 871/2600] -> Loss: 2869.3493
Epoch [ 872/2600] -> Loss: 2876.0822
Epoch [ 873/2600] -> Loss: 2873.7184
Epoch [ 874/2600] -> Loss: 2859.9206
Epoch [ 875/2600] -> Loss: 2885.7723
Epoch [ 876/2600] -> Loss: 2878.1026
Epoch [ 877/2600] -> Loss: 2868.0009
Epoch [ 878/2600] -> Loss: 2861.4147
Epoch [ 879/2600] -> Loss: 2882.0700
Epoch [ 880/2600] -> Loss: 2864.0317
Epoch [ 881/2600] -> Loss: 2878.0733
Epoch [ 882/2600] -> Loss: 2869.6033
Epoch [ 883/2600] -> Loss: 2879.2563
Epoch [ 884/2600] -> Loss: 2887.6184
Epoch [ 885/2600] -> Loss: 2881.4098
Epoch [ 886/2600] -> Loss: 2867.9627
Epoch [ 887/2600] -> Loss: 2862.6095
Epoch [ 888/2600] -> Loss: 2865.9243
Epoch [ 889/2600] -> Loss: 2877.4917
Epoch [ 890/2600] -> Loss: 2869.5098
Epoch [ 891/2600] -> Loss: 2874.1117
Epoch [ 892/2600] -> Loss: 2869.6897
Epoch [ 893/2600] -> Loss: 2861.3413
Epoch [ 894/2600] -> Loss: 2874.0211
Epoch [ 895/2600] -> Loss: 2878.9917
Epoch [ 896/2600] -> Loss: 2881.2800
Epoch [ 897/2600] -> Loss: 2863.9660
Epoch [ 898/2600] -> Loss: 2863.7080
Epoch [ 899/2600] -> Loss: 2872.2346
Epoch [ 900/2600] -> Loss: 2869.6498
Epoch [ 901/2600] -> Loss: 2869.7692
Epoch [ 902/2600] -> Loss: 2857.9391
Epoch [ 903/2600] -> Loss: 2891.1879
Epoch [ 904/2600] -> Loss: 2884.6673
Epoch [ 905/2600] -> Loss: 2867.7349
Epoch [ 906/2600] -> Loss: 2864.3875
Epoch [ 907/2600] -> Loss: 2868.0472
Epoch [ 908/2600] -> Loss: 2863.3571
Epoch [ 909/2600] -> Loss: 2871.1642
Epoch [ 910/2600] -> Loss: 2861.3926
Epoch [ 911/2600] -> Loss: 2862.7226
Epoch [ 912/2600] -> Loss: 2871.1648
Epoch [ 913/2600] -> Loss: 2862.8457
Epoch [ 914/2600] -> Loss: 2880.2711
Epoch [ 915/2600] -> Loss: 2865.6624
Epoch [ 916/2600] -> Loss: 2868.9197
Epoch [ 917/2600] -> Loss: 2854.2434
Epoch [ 918/2600] -> Loss: 2877.3663
Epoch [ 919/2600] -> Loss: 2877.6547
Epoch [ 920/2600] -> Loss: 2867.4256
Epoch [ 921/2600] -> Loss: 2862.9189
Epoch [ 922/2600] -> Loss: 2866.4028
Epoch [ 923/2600] -> Loss: 2864.3587
Epoch [ 924/2600] -> Loss: 2869.7464
Epoch [ 925/2600] -> Loss: 2863.8473
Epoch [ 926/2600] -> Loss: 2870.0315
Epoch [ 927/2600] -> Loss: 2878.4037
Epoch [ 928/2600] -> Loss: 2868.3779
Epoch [ 929/2600] -> Loss: 2890.5182
Epoch [ 930/2600] -> Loss: 2864.1553
Epoch [ 931/2600] -> Loss: 2866.5768
Epoch [ 932/2600] -> Loss: 2875.9930
Epoch [ 933/2600] -> Loss: 2869.5776
Epoch [ 934/2600] -> Loss: 2874.6739
Epoch [ 935/2600] -> Loss: 2873.8374
Epoch [ 936/2600] -> Loss: 2865.9440
Epoch [ 937/2600] -> Loss: 2864.5534
Epoch [ 938/2600] -> Loss: 2875.7360
Epoch [ 939/2600] -> Loss: 2877.8645
Epoch [ 940/2600] -> Loss: 2868.1476
Epoch [ 941/2600] -> Loss: 2889.2636
Epoch [ 942/2600] -> Loss: 2866.0867
Epoch [ 943/2600] -> Loss: 2884.3551
Epoch [ 944/2600] -> Loss: 2884.7738
Epoch [ 945/2600] -> Loss: 2861.9822
Epoch [ 946/2600] -> Loss: 2855.3938
Epoch [ 947/2600] -> Loss: 2855.5333
Epoch [ 948/2600] -> Loss: 2886.2438
Epoch [ 949/2600] -> Loss: 2882.4834
Epoch [ 950/2600] -> Loss: 2871.9959
Epoch [ 951/2600] -> Loss: 2884.3459
Epoch [ 952/2600] -> Loss: 2889.0694
Epoch [ 953/2600] -> Loss: 2863.3133
Epoch [ 954/2600] -> Loss: 2889.7605
Epoch [ 955/2600] -> Loss: 2861.6460
Epoch [ 956/2600] -> Loss: 2867.2380
Epoch [ 957/2600] -> Loss: 2872.1671
Epoch [ 958/2600] -> Loss: 2864.1604
Epoch [ 959/2600] -> Loss: 2888.4458
Epoch [ 960/2600] -> Loss: 2888.1741
Epoch [ 961/2600] -> Loss: 2863.2261
Epoch [ 962/2600] -> Loss: 2872.6530
Epoch [ 963/2600] -> Loss: 2871.8215
Epoch [ 964/2600] -> Loss: 2865.8888
Epoch [ 965/2600] -> Loss: 2864.2688
Epoch [ 966/2600] -> Loss: 2861.0724
Epoch [ 967/2600] -> Loss: 2867.7103
Epoch [ 968/2600] -> Loss: 2853.0946
Epoch [ 969/2600] -> Loss: 2909.1141
Epoch [ 970/2600] -> Loss: 2864.2264
Epoch [ 971/2600] -> Loss: 2867.1837
Epoch [ 972/2600] -> Loss: 2859.1453
Epoch [ 973/2600] -> Loss: 2878.4607
Epoch [ 974/2600] -> Loss: 2856.8065
Epoch [ 975/2600] -> Loss: 2870.0280
Epoch [ 976/2600] -> Loss: 2884.1414
Epoch [ 977/2600] -> Loss: 2876.5385
Epoch [ 978/2600] -> Loss: 2857.2065
Epoch [ 979/2600] -> Loss: 2874.0836
Epoch [ 980/2600] -> Loss: 2872.8524
Epoch [ 981/2600] -> Loss: 2872.2346
Epoch [ 982/2600] -> Loss: 2850.9123
Epoch [ 983/2600] -> Loss: 2878.2761
Epoch [ 984/2600] -> Loss: 2849.4540
Epoch [ 985/2600] -> Loss: 2865.6381
Epoch [ 986/2600] -> Loss: 2870.8424
Epoch [ 987/2600] -> Loss: 2882.2096
Epoch [ 988/2600] -> Loss: 2859.5954
Epoch [ 989/2600] -> Loss: 2854.0180
Epoch [ 990/2600] -> Loss: 2873.3479
Epoch [ 991/2600] -> Loss: 2853.0522
Epoch [ 992/2600] -> Loss: 2869.7846
Epoch [ 993/2600] -> Loss: 2878.9637
Epoch [ 994/2600] -> Loss: 2870.8715
Epoch [ 995/2600] -> Loss: 2865.0775
Epoch [ 996/2600] -> Loss: 2868.3671
Epoch [ 997/2600] -> Loss: 2854.3086
Epoch [ 998/2600] -> Loss: 2867.2729
Epoch [ 999/2600] -> Loss: 2867.9669
--------------------------------------------------
Model checkpoint saved as FFNN_1000.pth
--------------------------------------------------
Epoch [1000/2600] -> Loss: 2877.9884
Epoch [1001/2600] -> Loss: 2865.2132
Epoch [1002/2600] -> Loss: 2858.6518
Epoch [1003/2600] -> Loss: 2889.0627
Epoch [1004/2600] -> Loss: 2885.4592
Epoch [1005/2600] -> Loss: 2870.4013
Epoch [1006/2600] -> Loss: 2863.5734
Epoch [1007/2600] -> Loss: 2892.2493
Epoch [1008/2600] -> Loss: 2871.8348
Epoch [1009/2600] -> Loss: 2875.1402
Epoch [1010/2600] -> Loss: 2885.2032
Epoch [1011/2600] -> Loss: 2864.6649
Epoch [1012/2600] -> Loss: 2873.5465
Epoch [1013/2600] -> Loss: 2872.4081
Epoch [1014/2600] -> Loss: 2876.3001
Epoch [1015/2600] -> Loss: 2878.5132
Epoch [1016/2600] -> Loss: 2883.3760
Epoch [1017/2600] -> Loss: 2886.6142
Epoch [1018/2600] -> Loss: 2884.8404
Epoch [1019/2600] -> Loss: 2866.1233
Epoch [1020/2600] -> Loss: 2872.7425
Epoch [1021/2600] -> Loss: 2879.4924
