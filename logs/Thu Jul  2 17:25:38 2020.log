Usage: python3 SSN_predictor.py <path_to_ssn_datafile> <path_to_aa_datafile>
----------------------------------------------------------------
Code running on device: cuda
----------------------------------------------------------------
Data loaded from file locations :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
----------------------------------------------------------------
Solar cycle data loaded/saved as: cycle_data.pickle
----------------------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=3, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=3, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
----------------------------------------------------------------
No pre-trained models available, initializing model weights
----------------------------------------------------------------
Training model with: num_epochs=1200, start_lr=0.0002
Epoch [   1/1200] -> Loss: 121.9157
Epoch [   2/1200] -> Loss: 117.0730
Epoch [   3/1200] -> Loss: 107.9370
Epoch [   4/1200] -> Loss: 95.6501
Epoch [   5/1200] -> Loss: 83.1613
Epoch [   6/1200] -> Loss: 70.5711
Epoch [   7/1200] -> Loss: 59.8689
Epoch [   8/1200] -> Loss: 54.4793
Epoch [   9/1200] -> Loss: 52.4162
Epoch [  10/1200] -> Loss: 52.1077
Epoch [  11/1200] -> Loss: 51.8257
Epoch [  12/1200] -> Loss: 52.2113
Epoch [  13/1200] -> Loss: 51.8010
Epoch [  14/1200] -> Loss: 51.9024
Epoch [  15/1200] -> Loss: 52.2691
Epoch [  16/1200] -> Loss: 51.6137
Epoch [  17/1200] -> Loss: 52.2729
Epoch [  18/1200] -> Loss: 51.6763
Epoch [  19/1200] -> Loss: 51.8391
Epoch [  20/1200] -> Loss: 51.8590
Epoch [  21/1200] -> Loss: 51.9013
Epoch [  22/1200] -> Loss: 51.8140
Epoch [  23/1200] -> Loss: 51.6209
Epoch [  24/1200] -> Loss: 51.3867
Epoch [  25/1200] -> Loss: 51.7880
Epoch [  26/1200] -> Loss: 51.4722
Epoch [  27/1200] -> Loss: 51.4676
Epoch [  28/1200] -> Loss: 51.0151
Epoch [  29/1200] -> Loss: 51.4799
Epoch [  30/1200] -> Loss: 51.2572
Epoch [  31/1200] -> Loss: 51.3309
Epoch [  32/1200] -> Loss: 51.1958
Epoch [  33/1200] -> Loss: 51.3619
Epoch [  34/1200] -> Loss: 51.1698
Epoch [  35/1200] -> Loss: 50.8092
Epoch [  36/1200] -> Loss: 50.9683
Epoch [  37/1200] -> Loss: 50.9233
Epoch [  38/1200] -> Loss: 51.0041
Epoch [  39/1200] -> Loss: 51.0714
Epoch [  40/1200] -> Loss: 51.2060
Epoch [  41/1200] -> Loss: 50.9502
Epoch [  42/1200] -> Loss: 51.0357
Epoch [  43/1200] -> Loss: 50.5723
Epoch [  44/1200] -> Loss: 50.9597
Epoch [  45/1200] -> Loss: 50.6197
Epoch [  46/1200] -> Loss: 50.9048
Epoch [  47/1200] -> Loss: 50.3861
Epoch [  48/1200] -> Loss: 50.7176
Epoch [  49/1200] -> Loss: 50.7151
Epoch [  50/1200] -> Loss: 50.7102
Epoch [  51/1200] -> Loss: 50.5196
Epoch [  52/1200] -> Loss: 50.5946
Epoch [  53/1200] -> Loss: 50.6065
Epoch [  54/1200] -> Loss: 50.6662
Epoch [  55/1200] -> Loss: 50.2358
Epoch [  56/1200] -> Loss: 50.0495
Epoch [  57/1200] -> Loss: 50.3339
Epoch [  58/1200] -> Loss: 49.9209
Epoch [  59/1200] -> Loss: 50.2655
Epoch [  60/1200] -> Loss: 49.9505
Epoch [  61/1200] -> Loss: 50.3794
Epoch [  62/1200] -> Loss: 50.2438
Epoch [  63/1200] -> Loss: 50.4769
Epoch [  64/1200] -> Loss: 50.0308
Epoch [  65/1200] -> Loss: 50.3571
Epoch [  66/1200] -> Loss: 49.9239
Epoch [  67/1200] -> Loss: 50.0051
Epoch [  68/1200] -> Loss: 50.1238
Epoch [  69/1200] -> Loss: 49.8541
Epoch [  70/1200] -> Loss: 49.8833
Epoch [  71/1200] -> Loss: 49.1668
Epoch [  72/1200] -> Loss: 49.7602
Epoch [  73/1200] -> Loss: 49.9859
Epoch [  74/1200] -> Loss: 49.5113
Epoch [  75/1200] -> Loss: 49.5079
Epoch [  76/1200] -> Loss: 49.8090
Epoch [  77/1200] -> Loss: 49.3547
Epoch [  78/1200] -> Loss: 49.7074
Epoch [  79/1200] -> Loss: 49.3650
Epoch [  80/1200] -> Loss: 49.5245
Epoch [  81/1200] -> Loss: 49.4394
Epoch    82: reducing learning rate of group 0 to 1.0000e-04.
Epoch [  82/1200] -> Loss: 49.4763
Epoch [  83/1200] -> Loss: 49.3842
Epoch [  84/1200] -> Loss: 49.4488
Epoch [  85/1200] -> Loss: 49.3099
Epoch [  86/1200] -> Loss: 49.3986
Epoch [  87/1200] -> Loss: 49.4702
Epoch [  88/1200] -> Loss: 49.1027
Epoch [  89/1200] -> Loss: 49.4636
Epoch [  90/1200] -> Loss: 49.3887
Epoch [  91/1200] -> Loss: 49.2694
Epoch [  92/1200] -> Loss: 49.3627
Epoch [  93/1200] -> Loss: 48.9418
Epoch [  94/1200] -> Loss: 49.1286
Epoch [  95/1200] -> Loss: 49.3317
Epoch [  96/1200] -> Loss: 49.2431
Epoch [  97/1200] -> Loss: 49.5248
Epoch [  98/1200] -> Loss: 48.7922
Epoch [  99/1200] -> Loss: 49.1396
----------------------------------------------------------------
Model checkpoint saved as FFNN_100.pth
----------------------------------------------------------------
Epoch [ 100/1200] -> Loss: 49.3649
Epoch [ 101/1200] -> Loss: 49.3507
Epoch [ 102/1200] -> Loss: 49.3003
Epoch [ 103/1200] -> Loss: 49.2903
Epoch [ 104/1200] -> Loss: 49.2290
Epoch [ 105/1200] -> Loss: 48.8945
Epoch [ 106/1200] -> Loss: 49.3369
Epoch [ 107/1200] -> Loss: 49.5265
Epoch [ 108/1200] -> Loss: 48.8689
Epoch   109: reducing learning rate of group 0 to 5.0000e-05.
Epoch [ 109/1200] -> Loss: 49.1994
Epoch [ 110/1200] -> Loss: 49.1901
Epoch [ 111/1200] -> Loss: 48.9753
Epoch [ 112/1200] -> Loss: 48.9899
Epoch [ 113/1200] -> Loss: 49.1901
Epoch [ 114/1200] -> Loss: 49.1318
Epoch [ 115/1200] -> Loss: 48.7298
Epoch [ 116/1200] -> Loss: 49.0676
Epoch [ 117/1200] -> Loss: 48.8500
Epoch [ 118/1200] -> Loss: 49.1273
Epoch [ 119/1200] -> Loss: 48.9076
Epoch [ 120/1200] -> Loss: 49.0416
Epoch [ 121/1200] -> Loss: 48.4322
Epoch [ 122/1200] -> Loss: 48.9518
Epoch [ 123/1200] -> Loss: 48.8196
Epoch [ 124/1200] -> Loss: 48.8715
Epoch [ 125/1200] -> Loss: 48.8964
Epoch [ 126/1200] -> Loss: 49.3633
Epoch [ 127/1200] -> Loss: 48.6226
Epoch [ 128/1200] -> Loss: 48.6010
Epoch [ 129/1200] -> Loss: 48.8430
Epoch [ 130/1200] -> Loss: 48.7366
Epoch [ 131/1200] -> Loss: 48.8987
Epoch   132: reducing learning rate of group 0 to 2.5000e-05.
Epoch [ 132/1200] -> Loss: 48.9060
Epoch [ 133/1200] -> Loss: 48.8414
Epoch [ 134/1200] -> Loss: 48.7867
Epoch [ 135/1200] -> Loss: 48.5301
Epoch [ 136/1200] -> Loss: 47.9195
Epoch [ 137/1200] -> Loss: 48.9511
Epoch [ 138/1200] -> Loss: 48.4542
Epoch [ 139/1200] -> Loss: 48.3266
Epoch [ 140/1200] -> Loss: 48.7131
Epoch [ 141/1200] -> Loss: 48.3690
Epoch [ 142/1200] -> Loss: 48.5609
Epoch [ 143/1200] -> Loss: 48.8718
Epoch [ 144/1200] -> Loss: 48.7200
Epoch [ 145/1200] -> Loss: 48.8867
Epoch [ 146/1200] -> Loss: 48.5168
Epoch   147: reducing learning rate of group 0 to 1.2500e-05.
Epoch [ 147/1200] -> Loss: 48.4176
Epoch [ 148/1200] -> Loss: 48.6848
Epoch [ 149/1200] -> Loss: 48.9453
Epoch [ 150/1200] -> Loss: 48.4813
Epoch [ 151/1200] -> Loss: 48.2044
Epoch [ 152/1200] -> Loss: 48.5477
Epoch [ 153/1200] -> Loss: 48.7619
Epoch [ 154/1200] -> Loss: 48.8995
Epoch [ 155/1200] -> Loss: 48.6338
Epoch [ 156/1200] -> Loss: 48.1902
Epoch [ 157/1200] -> Loss: 48.6103
Epoch   158: reducing learning rate of group 0 to 6.2500e-06.
Epoch [ 158/1200] -> Loss: 48.8155
Epoch [ 159/1200] -> Loss: 48.8462
Epoch [ 160/1200] -> Loss: 48.7226
Epoch [ 161/1200] -> Loss: 48.7732
Epoch [ 162/1200] -> Loss: 48.8552
Epoch [ 163/1200] -> Loss: 48.8203
Epoch [ 164/1200] -> Loss: 48.8025
Epoch [ 165/1200] -> Loss: 48.7044
Epoch [ 166/1200] -> Loss: 48.7743
Epoch [ 167/1200] -> Loss: 48.9346
Epoch [ 168/1200] -> Loss: 48.6030
Epoch   169: reducing learning rate of group 0 to 3.1250e-06.
Epoch [ 169/1200] -> Loss: 48.5137
Epoch [ 170/1200] -> Loss: 48.7263
Epoch [ 171/1200] -> Loss: 48.7599
Epoch [ 172/1200] -> Loss: 48.8662
Epoch [ 173/1200] -> Loss: 49.0798
Epoch [ 174/1200] -> Loss: 48.7615
Epoch [ 175/1200] -> Loss: 48.8775
Epoch [ 176/1200] -> Loss: 48.5660
Epoch [ 177/1200] -> Loss: 48.4486
Epoch [ 178/1200] -> Loss: 48.5738
Epoch [ 179/1200] -> Loss: 48.8405
Epoch   180: reducing learning rate of group 0 to 1.5625e-06.
Epoch [ 180/1200] -> Loss: 48.7424
Epoch [ 181/1200] -> Loss: 48.6322
Epoch [ 182/1200] -> Loss: 48.8006
Epoch [ 183/1200] -> Loss: 48.6833
Epoch [ 184/1200] -> Loss: 48.8728
Epoch [ 185/1200] -> Loss: 48.8021
Epoch [ 186/1200] -> Loss: 48.6387
Epoch [ 187/1200] -> Loss: 48.3943
Epoch [ 188/1200] -> Loss: 48.4753
Epoch [ 189/1200] -> Loss: 48.6170
Epoch [ 190/1200] -> Loss: 48.8253
Epoch   191: reducing learning rate of group 0 to 7.8125e-07.
Epoch [ 191/1200] -> Loss: 48.6663
Epoch [ 192/1200] -> Loss: 48.4249
Epoch [ 193/1200] -> Loss: 48.6988
Epoch [ 194/1200] -> Loss: 48.7094
Epoch [ 195/1200] -> Loss: 49.0256
Epoch [ 196/1200] -> Loss: 48.9256
Epoch [ 197/1200] -> Loss: 48.4995
Epoch [ 198/1200] -> Loss: 48.4067
Epoch [ 199/1200] -> Loss: 48.5364
----------------------------------------------------------------
Model checkpoint saved as FFNN_200.pth
----------------------------------------------------------------
Epoch [ 200/1200] -> Loss: 48.8336
Epoch [ 201/1200] -> Loss: 48.5289
Epoch   202: reducing learning rate of group 0 to 3.9063e-07.
Epoch [ 202/1200] -> Loss: 49.0265
Epoch [ 203/1200] -> Loss: 48.2424
Epoch [ 204/1200] -> Loss: 48.6677
Epoch [ 205/1200] -> Loss: 48.5883
Epoch [ 206/1200] -> Loss: 48.5423
Epoch [ 207/1200] -> Loss: 48.9748
Epoch [ 208/1200] -> Loss: 48.5940
Epoch [ 209/1200] -> Loss: 48.6234
Epoch [ 210/1200] -> Loss: 48.8322
Epoch [ 211/1200] -> Loss: 48.6917
Epoch [ 212/1200] -> Loss: 48.8008
Epoch   213: reducing learning rate of group 0 to 1.9531e-07.
Epoch [ 213/1200] -> Loss: 48.7934
Epoch [ 214/1200] -> Loss: 48.6516
Epoch [ 215/1200] -> Loss: 48.7003
Epoch [ 216/1200] -> Loss: 48.7006
Epoch [ 217/1200] -> Loss: 48.6696
Epoch [ 218/1200] -> Loss: 49.0669
Epoch [ 219/1200] -> Loss: 48.1480
Epoch [ 220/1200] -> Loss: 48.7294
Epoch [ 221/1200] -> Loss: 48.3202
Epoch [ 222/1200] -> Loss: 48.6434
Epoch [ 223/1200] -> Loss: 48.5375
Epoch   224: reducing learning rate of group 0 to 9.7656e-08.
Epoch [ 224/1200] -> Loss: 48.7122
Epoch [ 225/1200] -> Loss: 48.9607
Epoch [ 226/1200] -> Loss: 48.4334
Epoch [ 227/1200] -> Loss: 48.2804
Epoch [ 228/1200] -> Loss: 48.6351
Epoch [ 229/1200] -> Loss: 48.7560
Epoch [ 230/1200] -> Loss: 48.8141
Epoch [ 231/1200] -> Loss: 48.7974
Epoch [ 232/1200] -> Loss: 48.5178
Epoch [ 233/1200] -> Loss: 48.5062
Epoch [ 234/1200] -> Loss: 48.6951
Epoch   235: reducing learning rate of group 0 to 4.8828e-08.
Epoch [ 235/1200] -> Loss: 48.8394
Epoch [ 236/1200] -> Loss: 48.7516
Epoch [ 237/1200] -> Loss: 48.4394
Epoch [ 238/1200] -> Loss: 48.6423
Epoch [ 239/1200] -> Loss: 48.9272
Epoch [ 240/1200] -> Loss: 48.4619
Epoch [ 241/1200] -> Loss: 48.6296
Epoch [ 242/1200] -> Loss: 48.8687
Epoch [ 243/1200] -> Loss: 48.3168
Epoch [ 244/1200] -> Loss: 48.6742
Epoch [ 245/1200] -> Loss: 48.7857
Epoch   246: reducing learning rate of group 0 to 2.4414e-08.
Epoch [ 246/1200] -> Loss: 48.8583
Epoch [ 247/1200] -> Loss: 48.5404
Epoch [ 248/1200] -> Loss: 48.8668
Epoch [ 249/1200] -> Loss: 48.5409
Epoch [ 250/1200] -> Loss: 48.4163
Epoch [ 251/1200] -> Loss: 48.4311
Epoch [ 252/1200] -> Loss: 48.7772
Epoch [ 253/1200] -> Loss: 48.0690
Epoch [ 254/1200] -> Loss: 48.7427
Epoch [ 255/1200] -> Loss: 48.9505
Epoch [ 256/1200] -> Loss: 48.4250
Epoch   257: reducing learning rate of group 0 to 1.2207e-08.
Epoch [ 257/1200] -> Loss: 48.6945
Epoch [ 258/1200] -> Loss: 48.4885
Epoch [ 259/1200] -> Loss: 48.6214
Epoch [ 260/1200] -> Loss: 48.6973
Epoch [ 261/1200] -> Loss: 48.7768
Epoch [ 262/1200] -> Loss: 48.3751
Epoch [ 263/1200] -> Loss: 49.0003
Epoch [ 264/1200] -> Loss: 48.3924
Epoch [ 265/1200] -> Loss: 48.9822
Epoch [ 266/1200] -> Loss: 48.6594
Epoch [ 267/1200] -> Loss: 48.9019
Epoch [ 268/1200] -> Loss: 48.9153
Epoch [ 269/1200] -> Loss: 48.3144
Epoch [ 270/1200] -> Loss: 48.5921
Epoch [ 271/1200] -> Loss: 48.2739
Epoch [ 272/1200] -> Loss: 48.4437
Epoch [ 273/1200] -> Loss: 48.4044
Epoch [ 274/1200] -> Loss: 48.7827
Epoch [ 275/1200] -> Loss: 48.7694
Epoch [ 276/1200] -> Loss: 48.8895
Epoch [ 277/1200] -> Loss: 48.7818
Epoch [ 278/1200] -> Loss: 48.3315
Epoch [ 279/1200] -> Loss: 48.3702
Epoch [ 280/1200] -> Loss: 48.5766
Epoch [ 281/1200] -> Loss: 48.9429
Epoch [ 282/1200] -> Loss: 48.4899
Epoch [ 283/1200] -> Loss: 48.5160
Epoch [ 284/1200] -> Loss: 48.8112
Epoch [ 285/1200] -> Loss: 48.5867
Epoch [ 286/1200] -> Loss: 48.6889
Epoch [ 287/1200] -> Loss: 48.9223
Epoch [ 288/1200] -> Loss: 48.3865
Epoch [ 289/1200] -> Loss: 48.8271
Epoch [ 290/1200] -> Loss: 48.8729
Epoch [ 291/1200] -> Loss: 48.5654
Epoch [ 292/1200] -> Loss: 48.7105
Epoch [ 293/1200] -> Loss: 49.0690
Epoch [ 294/1200] -> Loss: 48.6745
Epoch [ 295/1200] -> Loss: 48.3632
Epoch [ 296/1200] -> Loss: 48.8524
Epoch [ 297/1200] -> Loss: 49.0308
Epoch [ 298/1200] -> Loss: 48.4621
Epoch [ 299/1200] -> Loss: 48.8349
----------------------------------------------------------------
Model checkpoint saved as FFNN_300.pth
----------------------------------------------------------------
Epoch [ 300/1200] -> Loss: 48.8651
Epoch [ 301/1200] -> Loss: 48.6143
Epoch [ 302/1200] -> Loss: 48.7713
Epoch [ 303/1200] -> Loss: 48.4441
Epoch [ 304/1200] -> Loss: 48.2748
Epoch [ 305/1200] -> Loss: 48.4334
Epoch [ 306/1200] -> Loss: 48.7040
Epoch [ 307/1200] -> Loss: 48.3833
Epoch [ 308/1200] -> Loss: 48.5389
Epoch [ 309/1200] -> Loss: 48.4725
Epoch [ 310/1200] -> Loss: 48.7455
Epoch [ 311/1200] -> Loss: 48.4998
Epoch [ 312/1200] -> Loss: 48.7803
Epoch [ 313/1200] -> Loss: 48.4411
Epoch [ 314/1200] -> Loss: 48.5446
Epoch [ 315/1200] -> Loss: 48.8097
Epoch [ 316/1200] -> Loss: 48.5123
Epoch [ 317/1200] -> Loss: 48.7629
Epoch [ 318/1200] -> Loss: 48.6238
Epoch [ 319/1200] -> Loss: 48.9870
Epoch [ 320/1200] -> Loss: 48.3200
Epoch [ 321/1200] -> Loss: 48.8844
Epoch [ 322/1200] -> Loss: 48.3673
Epoch [ 323/1200] -> Loss: 48.9178
Epoch [ 324/1200] -> Loss: 48.5025
Epoch [ 325/1200] -> Loss: 48.2955
Epoch [ 326/1200] -> Loss: 48.4470
Epoch [ 327/1200] -> Loss: 48.4520
Epoch [ 328/1200] -> Loss: 48.6340
Epoch [ 329/1200] -> Loss: 48.3879
Epoch [ 330/1200] -> Loss: 48.4965
Epoch [ 331/1200] -> Loss: 48.5119
Epoch [ 332/1200] -> Loss: 48.7917
Epoch [ 333/1200] -> Loss: 48.5852
Epoch [ 334/1200] -> Loss: 48.4128
Epoch [ 335/1200] -> Loss: 48.5972
Epoch [ 336/1200] -> Loss: 48.6564
Epoch [ 337/1200] -> Loss: 48.5086
Epoch [ 338/1200] -> Loss: 48.6427
Epoch [ 339/1200] -> Loss: 48.6169
Epoch [ 340/1200] -> Loss: 48.9788
Epoch [ 341/1200] -> Loss: 48.6543
Epoch [ 342/1200] -> Loss: 48.6189
Epoch [ 343/1200] -> Loss: 48.6438
Epoch [ 344/1200] -> Loss: 48.5591
Epoch [ 345/1200] -> Loss: 48.4389
Epoch [ 346/1200] -> Loss: 49.0343
Epoch [ 347/1200] -> Loss: 48.8219
Epoch [ 348/1200] -> Loss: 48.7343
Epoch [ 349/1200] -> Loss: 48.6137
Epoch [ 350/1200] -> Loss: 48.6710
Epoch [ 351/1200] -> Loss: 48.6964
Epoch [ 352/1200] -> Loss: 48.3434
Epoch [ 353/1200] -> Loss: 48.8493
Epoch [ 354/1200] -> Loss: 48.4778
Epoch [ 355/1200] -> Loss: 48.6276
Epoch [ 356/1200] -> Loss: 48.8104
Epoch [ 357/1200] -> Loss: 48.5652
Epoch [ 358/1200] -> Loss: 48.9053
Epoch [ 359/1200] -> Loss: 48.8138
Epoch [ 360/1200] -> Loss: 48.5552
Epoch [ 361/1200] -> Loss: 48.5319
Epoch [ 362/1200] -> Loss: 48.9485
Epoch [ 363/1200] -> Loss: 48.2101
Epoch [ 364/1200] -> Loss: 48.7235
Epoch [ 365/1200] -> Loss: 48.3853
Epoch [ 366/1200] -> Loss: 48.6333
Epoch [ 367/1200] -> Loss: 48.5322
Epoch [ 368/1200] -> Loss: 48.1305
Epoch [ 369/1200] -> Loss: 48.5606
Epoch [ 370/1200] -> Loss: 48.7750
Epoch [ 371/1200] -> Loss: 48.8691
Epoch [ 372/1200] -> Loss: 48.7583
Epoch [ 373/1200] -> Loss: 48.7807
Epoch [ 374/1200] -> Loss: 48.3251
Epoch [ 375/1200] -> Loss: 48.5769
Epoch [ 376/1200] -> Loss: 48.2682
Epoch [ 377/1200] -> Loss: 48.7063
Epoch [ 378/1200] -> Loss: 48.6706
Epoch [ 379/1200] -> Loss: 48.7516
Epoch [ 380/1200] -> Loss: 48.6879
Epoch [ 381/1200] -> Loss: 48.2566
Epoch [ 382/1200] -> Loss: 48.7479
Epoch [ 383/1200] -> Loss: 48.9914
Epoch [ 384/1200] -> Loss: 48.7201
Epoch [ 385/1200] -> Loss: 48.7397
Epoch [ 386/1200] -> Loss: 48.6621
Epoch [ 387/1200] -> Loss: 48.3924
Epoch [ 388/1200] -> Loss: 48.8604
Epoch [ 389/1200] -> Loss: 48.5644
Epoch [ 390/1200] -> Loss: 48.4243
Epoch [ 391/1200] -> Loss: 48.0499
Epoch [ 392/1200] -> Loss: 48.5149
Epoch [ 393/1200] -> Loss: 48.7476
Epoch [ 394/1200] -> Loss: 48.6213
Epoch [ 395/1200] -> Loss: 48.9123
Epoch [ 396/1200] -> Loss: 48.9453
Epoch [ 397/1200] -> Loss: 48.5077
Epoch [ 398/1200] -> Loss: 48.9331
Epoch [ 399/1200] -> Loss: 48.6111
----------------------------------------------------------------
Model checkpoint saved as FFNN_400.pth
----------------------------------------------------------------
Epoch [ 400/1200] -> Loss: 48.5726
Epoch [ 401/1200] -> Loss: 49.0554
Epoch [ 402/1200] -> Loss: 48.7137
Epoch [ 403/1200] -> Loss: 48.8288
Epoch [ 404/1200] -> Loss: 49.1001
Epoch [ 405/1200] -> Loss: 48.3517
Epoch [ 406/1200] -> Loss: 48.4327
Epoch [ 407/1200] -> Loss: 48.7595
Epoch [ 408/1200] -> Loss: 48.5964
Epoch [ 409/1200] -> Loss: 49.0058
Epoch [ 410/1200] -> Loss: 48.3174
Epoch [ 411/1200] -> Loss: 48.4354
Epoch [ 412/1200] -> Loss: 48.5465
Epoch [ 413/1200] -> Loss: 48.6202
Epoch [ 414/1200] -> Loss: 48.5074
Epoch [ 415/1200] -> Loss: 48.6731
Epoch [ 416/1200] -> Loss: 48.8264
Epoch [ 417/1200] -> Loss: 48.8746
Epoch [ 418/1200] -> Loss: 48.8409
Epoch [ 419/1200] -> Loss: 48.6665
Epoch [ 420/1200] -> Loss: 48.3740
Epoch [ 421/1200] -> Loss: 48.4834
Epoch [ 422/1200] -> Loss: 48.5038
Epoch [ 423/1200] -> Loss: 48.6814
Epoch [ 424/1200] -> Loss: 48.6818
Epoch [ 425/1200] -> Loss: 48.6970
Epoch [ 426/1200] -> Loss: 48.6964
Epoch [ 427/1200] -> Loss: 48.5175
Epoch [ 428/1200] -> Loss: 48.7392
Epoch [ 429/1200] -> Loss: 48.5364
Epoch [ 430/1200] -> Loss: 48.7068
Epoch [ 431/1200] -> Loss: 48.8025
Epoch [ 432/1200] -> Loss: 48.6462
Epoch [ 433/1200] -> Loss: 48.6506
Epoch [ 434/1200] -> Loss: 48.7726
Epoch [ 435/1200] -> Loss: 48.5590
Epoch [ 436/1200] -> Loss: 48.6313
Epoch [ 437/1200] -> Loss: 48.5008
Epoch [ 438/1200] -> Loss: 47.9439
Epoch [ 439/1200] -> Loss: 48.8033
Epoch [ 440/1200] -> Loss: 48.6595
Epoch [ 441/1200] -> Loss: 48.4963
Epoch [ 442/1200] -> Loss: 48.8597
Epoch [ 443/1200] -> Loss: 48.5230
Epoch [ 444/1200] -> Loss: 48.9568
Epoch [ 445/1200] -> Loss: 48.5589
Epoch [ 446/1200] -> Loss: 48.7221
Epoch [ 447/1200] -> Loss: 48.6861
Epoch [ 448/1200] -> Loss: 48.6027
Epoch [ 449/1200] -> Loss: 48.8671
Epoch [ 450/1200] -> Loss: 48.4420
Epoch [ 451/1200] -> Loss: 48.4806
Epoch [ 452/1200] -> Loss: 48.4092
Epoch [ 453/1200] -> Loss: 49.1343
Epoch [ 454/1200] -> Loss: 48.8414
Epoch [ 455/1200] -> Loss: 48.2798
Epoch [ 456/1200] -> Loss: 48.2584
Epoch [ 457/1200] -> Loss: 48.4775
Epoch [ 458/1200] -> Loss: 48.6946
Epoch [ 459/1200] -> Loss: 48.8674
Epoch [ 460/1200] -> Loss: 48.6318
Epoch [ 461/1200] -> Loss: 48.7646
Epoch [ 462/1200] -> Loss: 48.4074
Epoch [ 463/1200] -> Loss: 48.7162
Epoch [ 464/1200] -> Loss: 48.4124
Epoch [ 465/1200] -> Loss: 48.9502
Epoch [ 466/1200] -> Loss: 48.6923
Epoch [ 467/1200] -> Loss: 48.5348
Epoch [ 468/1200] -> Loss: 48.3710
Epoch [ 469/1200] -> Loss: 48.5894
Epoch [ 470/1200] -> Loss: 48.5014
Epoch [ 471/1200] -> Loss: 49.1359
Epoch [ 472/1200] -> Loss: 48.1674
Epoch [ 473/1200] -> Loss: 48.9357
Epoch [ 474/1200] -> Loss: 48.2998
Epoch [ 475/1200] -> Loss: 48.8460
Epoch [ 476/1200] -> Loss: 48.9651
Epoch [ 477/1200] -> Loss: 48.5363
Epoch [ 478/1200] -> Loss: 48.6812
Epoch [ 479/1200] -> Loss: 48.6970
Epoch [ 480/1200] -> Loss: 48.4236
Epoch [ 481/1200] -> Loss: 48.6172
Epoch [ 482/1200] -> Loss: 48.7614
Epoch [ 483/1200] -> Loss: 48.3649
Epoch [ 484/1200] -> Loss: 48.1425
Epoch [ 485/1200] -> Loss: 48.4457
Epoch [ 486/1200] -> Loss: 48.7694
Epoch [ 487/1200] -> Loss: 48.6490
Epoch [ 488/1200] -> Loss: 48.7708
Epoch [ 489/1200] -> Loss: 48.8364
Epoch [ 490/1200] -> Loss: 48.9293
Epoch [ 491/1200] -> Loss: 48.7779
Epoch [ 492/1200] -> Loss: 48.5512
Epoch [ 493/1200] -> Loss: 48.7220
Epoch [ 494/1200] -> Loss: 48.6440
Epoch [ 495/1200] -> Loss: 48.6335
Epoch [ 496/1200] -> Loss: 48.8275
Epoch [ 497/1200] -> Loss: 48.7985
Epoch [ 498/1200] -> Loss: 48.4232
Epoch [ 499/1200] -> Loss: 48.4210
----------------------------------------------------------------
Model checkpoint saved as FFNN_500.pth
----------------------------------------------------------------
Epoch [ 500/1200] -> Loss: 48.6126
Epoch [ 501/1200] -> Loss: 48.8065
Epoch [ 502/1200] -> Loss: 48.4435
Epoch [ 503/1200] -> Loss: 48.2390
Epoch [ 504/1200] -> Loss: 48.4751
Epoch [ 505/1200] -> Loss: 48.5568
Epoch [ 506/1200] -> Loss: 48.3426
Epoch [ 507/1200] -> Loss: 48.6671
Epoch [ 508/1200] -> Loss: 47.9858
Epoch [ 509/1200] -> Loss: 48.4118
Epoch [ 510/1200] -> Loss: 48.8563
Epoch [ 511/1200] -> Loss: 48.3894
Epoch [ 512/1200] -> Loss: 48.7835
Epoch [ 513/1200] -> Loss: 48.6447
Epoch [ 514/1200] -> Loss: 49.0532
Epoch [ 515/1200] -> Loss: 48.4458
Epoch [ 516/1200] -> Loss: 48.8551
Epoch [ 517/1200] -> Loss: 48.4292
Epoch [ 518/1200] -> Loss: 48.9362
Epoch [ 519/1200] -> Loss: 48.7230
Epoch [ 520/1200] -> Loss: 48.5297
Epoch [ 521/1200] -> Loss: 48.8277
Epoch [ 522/1200] -> Loss: 48.3146
Epoch [ 523/1200] -> Loss: 48.2426
Epoch [ 524/1200] -> Loss: 48.4524
Epoch [ 525/1200] -> Loss: 48.5047
Epoch [ 526/1200] -> Loss: 48.3958
Epoch [ 527/1200] -> Loss: 49.0616
Epoch [ 528/1200] -> Loss: 48.8170
Epoch [ 529/1200] -> Loss: 48.6656
Epoch [ 530/1200] -> Loss: 48.4913
Epoch [ 531/1200] -> Loss: 48.6152
Epoch [ 532/1200] -> Loss: 48.4550
Epoch [ 533/1200] -> Loss: 48.7882
Epoch [ 534/1200] -> Loss: 48.7758
Epoch [ 535/1200] -> Loss: 49.0009
Epoch [ 536/1200] -> Loss: 48.4332
Epoch [ 537/1200] -> Loss: 48.9009
Epoch [ 538/1200] -> Loss: 48.7970
Epoch [ 539/1200] -> Loss: 48.7610
Epoch [ 540/1200] -> Loss: 48.4147
Epoch [ 541/1200] -> Loss: 48.5155
Epoch [ 542/1200] -> Loss: 48.6637
Epoch [ 543/1200] -> Loss: 48.1562
Epoch [ 544/1200] -> Loss: 48.5491
Epoch [ 545/1200] -> Loss: 48.6895
Epoch [ 546/1200] -> Loss: 48.3427
Epoch [ 547/1200] -> Loss: 48.9245
Epoch [ 548/1200] -> Loss: 48.6216
Epoch [ 549/1200] -> Loss: 48.7435
Epoch [ 550/1200] -> Loss: 48.7986
Epoch [ 551/1200] -> Loss: 48.3332
Epoch [ 552/1200] -> Loss: 48.3852
Epoch [ 553/1200] -> Loss: 48.8084
Epoch [ 554/1200] -> Loss: 48.4265
Epoch [ 555/1200] -> Loss: 48.9057
Epoch [ 556/1200] -> Loss: 48.5597
Epoch [ 557/1200] -> Loss: 48.8501
Epoch [ 558/1200] -> Loss: 48.6163
Epoch [ 559/1200] -> Loss: 48.3243
Epoch [ 560/1200] -> Loss: 48.7886
Epoch [ 561/1200] -> Loss: 48.3507
Epoch [ 562/1200] -> Loss: 48.6570
Epoch [ 563/1200] -> Loss: 48.3145
Epoch [ 564/1200] -> Loss: 48.4163
Epoch [ 565/1200] -> Loss: 48.7976
Epoch [ 566/1200] -> Loss: 48.7502
Epoch [ 567/1200] -> Loss: 48.8198
Epoch [ 568/1200] -> Loss: 48.4904
Epoch [ 569/1200] -> Loss: 48.7361
Epoch [ 570/1200] -> Loss: 48.6374
Epoch [ 571/1200] -> Loss: 48.8068
Epoch [ 572/1200] -> Loss: 49.0767
Epoch [ 573/1200] -> Loss: 48.7360
Epoch [ 574/1200] -> Loss: 48.9063
Epoch [ 575/1200] -> Loss: 48.9738
Epoch [ 576/1200] -> Loss: 48.3981
Epoch [ 577/1200] -> Loss: 48.9959
Epoch [ 578/1200] -> Loss: 48.6760
Epoch [ 579/1200] -> Loss: 48.4552
Epoch [ 580/1200] -> Loss: 48.3118
Epoch [ 581/1200] -> Loss: 48.3797
Epoch [ 582/1200] -> Loss: 48.5771
Epoch [ 583/1200] -> Loss: 49.0686
Epoch [ 584/1200] -> Loss: 48.8973
Epoch [ 585/1200] -> Loss: 48.5709
Epoch [ 586/1200] -> Loss: 48.3513
Epoch [ 587/1200] -> Loss: 48.4747
Epoch [ 588/1200] -> Loss: 48.2234
Epoch [ 589/1200] -> Loss: 48.2590
Epoch [ 590/1200] -> Loss: 48.5698
Epoch [ 591/1200] -> Loss: 48.0568
Epoch [ 592/1200] -> Loss: 48.3112
Epoch [ 593/1200] -> Loss: 48.5875
Epoch [ 594/1200] -> Loss: 48.8177
Epoch [ 595/1200] -> Loss: 48.5221
Epoch [ 596/1200] -> Loss: 48.3748
Epoch [ 597/1200] -> Loss: 48.6135
Epoch [ 598/1200] -> Loss: 48.5746
Epoch [ 599/1200] -> Loss: 48.7338
----------------------------------------------------------------
Model checkpoint saved as FFNN_600.pth
----------------------------------------------------------------
Epoch [ 600/1200] -> Loss: 48.5115
Epoch [ 601/1200] -> Loss: 48.9866
Epoch [ 602/1200] -> Loss: 48.8245
Epoch [ 603/1200] -> Loss: 48.9834
Epoch [ 604/1200] -> Loss: 48.7365
Epoch [ 605/1200] -> Loss: 48.7845
Epoch [ 606/1200] -> Loss: 48.4058
Epoch [ 607/1200] -> Loss: 48.4768
Epoch [ 608/1200] -> Loss: 48.3687
Epoch [ 609/1200] -> Loss: 48.6894
