--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
File location :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
No pre-trained models available, initializing model weights
--------------------------------------------------
Training model with: num_epochs=1800, start_lr=0.0005
Epoch [   1/1800] -> Loss: 13266.1610
Epoch [   2/1800] -> Loss: 11883.2233
Epoch [   3/1800] -> Loss: 9104.4858
Epoch [   4/1800] -> Loss: 5746.3203
Epoch [   5/1800] -> Loss: 3721.6147
Epoch [   6/1800] -> Loss: 3160.3232
Epoch [   7/1800] -> Loss: 3081.2226
Epoch [   8/1800] -> Loss: 3069.4696
Epoch [   9/1800] -> Loss: 3070.8513
Epoch [  10/1800] -> Loss: 3068.6143
Epoch [  11/1800] -> Loss: 3061.7226
Epoch [  12/1800] -> Loss: 3053.3045
Epoch [  13/1800] -> Loss: 3042.7728
Epoch [  14/1800] -> Loss: 3039.8941
Epoch [  15/1800] -> Loss: 3033.5003
Epoch [  16/1800] -> Loss: 3027.6738
Epoch [  17/1800] -> Loss: 3021.6536
Epoch [  18/1800] -> Loss: 3011.2899
Epoch [  19/1800] -> Loss: 3013.9688
Epoch [  20/1800] -> Loss: 3005.9993
Epoch [  21/1800] -> Loss: 3010.9422
Epoch [  22/1800] -> Loss: 2987.0960
Epoch [  23/1800] -> Loss: 2981.7134
Epoch [  24/1800] -> Loss: 2980.9089
Epoch [  25/1800] -> Loss: 2967.0559
Epoch [  26/1800] -> Loss: 2962.7754
Epoch [  27/1800] -> Loss: 2954.8873
Epoch [  28/1800] -> Loss: 2941.4447
Epoch [  29/1800] -> Loss: 2937.1997
Epoch [  30/1800] -> Loss: 2928.3261
Epoch [  31/1800] -> Loss: 2913.0827
Epoch [  32/1800] -> Loss: 2913.5777
Epoch [  33/1800] -> Loss: 2897.6884
Epoch [  34/1800] -> Loss: 2887.6063
Epoch [  35/1800] -> Loss: 2879.2162
Epoch [  36/1800] -> Loss: 2863.7637
Epoch [  37/1800] -> Loss: 2847.2805
Epoch [  38/1800] -> Loss: 2845.1001
Epoch [  39/1800] -> Loss: 2814.6140
Epoch [  40/1800] -> Loss: 2820.7162
Epoch [  41/1800] -> Loss: 2791.7555
Epoch [  42/1800] -> Loss: 2776.2002
Epoch [  43/1800] -> Loss: 2759.0307
Epoch [  44/1800] -> Loss: 2744.5224
Epoch [  45/1800] -> Loss: 2725.5917
Epoch [  46/1800] -> Loss: 2708.1359
Epoch [  47/1800] -> Loss: 2694.5902
Epoch [  48/1800] -> Loss: 2673.3851
Epoch [  49/1800] -> Loss: 2659.3371
Epoch [  50/1800] -> Loss: 2641.2559
Epoch [  51/1800] -> Loss: 2620.4435
Epoch [  52/1800] -> Loss: 2605.9718
Epoch [  53/1800] -> Loss: 2580.7965
Epoch [  54/1800] -> Loss: 2566.4911
Epoch [  55/1800] -> Loss: 2541.7757
Epoch [  56/1800] -> Loss: 2525.2009
Epoch [  57/1800] -> Loss: 2500.6233
Epoch [  58/1800] -> Loss: 2491.2891
Epoch [  59/1800] -> Loss: 2475.5605
Epoch [  60/1800] -> Loss: 2445.0981
Epoch [  61/1800] -> Loss: 2424.3411
Epoch [  62/1800] -> Loss: 2408.1068
Epoch [  63/1800] -> Loss: 2387.7880
Epoch [  64/1800] -> Loss: 2364.1164
Epoch [  65/1800] -> Loss: 2350.6049
Epoch [  66/1800] -> Loss: 2336.9037
Epoch [  67/1800] -> Loss: 2307.9812
Epoch [  68/1800] -> Loss: 2286.7881
Epoch [  69/1800] -> Loss: 2278.6286
Epoch [  70/1800] -> Loss: 2255.2853
Epoch [  71/1800] -> Loss: 2235.2606
Epoch [  72/1800] -> Loss: 2218.6592
Epoch [  73/1800] -> Loss: 2198.7497
Epoch [  74/1800] -> Loss: 2174.5700
Epoch [  75/1800] -> Loss: 2169.6390
Epoch [  76/1800] -> Loss: 2147.1862
Epoch [  77/1800] -> Loss: 2139.1520
Epoch [  78/1800] -> Loss: 2117.9597
Epoch [  79/1800] -> Loss: 2103.9284
Epoch [  80/1800] -> Loss: 2091.5352
Epoch [  81/1800] -> Loss: 2071.0476
Epoch [  82/1800] -> Loss: 2061.4960
Epoch [  83/1800] -> Loss: 2048.8166
Epoch [  84/1800] -> Loss: 2032.0241
Epoch [  85/1800] -> Loss: 2033.3393
Epoch [  86/1800] -> Loss: 2003.9359
Epoch [  87/1800] -> Loss: 1992.3437
Epoch [  88/1800] -> Loss: 1982.9706
Epoch [  89/1800] -> Loss: 1970.5758
Epoch [  90/1800] -> Loss: 1971.4598
Epoch [  91/1800] -> Loss: 1939.8860
Epoch [  92/1800] -> Loss: 1938.2935
Epoch [  93/1800] -> Loss: 1922.8994
Epoch [  94/1800] -> Loss: 1914.3113
Epoch [  95/1800] -> Loss: 1908.0065
Epoch [  96/1800] -> Loss: 1895.6488
Epoch [  97/1800] -> Loss: 1887.1355
Epoch [  98/1800] -> Loss: 1873.4029
Epoch [  99/1800] -> Loss: 1868.6854
--------------------------------------------------
Model checkpoint saved as FFNN_100.pth
--------------------------------------------------
Epoch [ 100/1800] -> Loss: 1868.1586
Epoch [ 101/1800] -> Loss: 1855.3594
Epoch [ 102/1800] -> Loss: 1835.7805
Epoch [ 103/1800] -> Loss: 1836.6419
Epoch [ 104/1800] -> Loss: 1829.2840
Epoch [ 105/1800] -> Loss: 1813.6955
Epoch [ 106/1800] -> Loss: 1815.7474
Epoch [ 107/1800] -> Loss: 1801.4061
Epoch [ 108/1800] -> Loss: 1798.8647
Epoch [ 109/1800] -> Loss: 1787.7031
Epoch [ 110/1800] -> Loss: 1777.1600
Epoch [ 111/1800] -> Loss: 1774.6123
Epoch [ 112/1800] -> Loss: 1767.0721
Epoch [ 113/1800] -> Loss: 1765.6413
Epoch [ 114/1800] -> Loss: 1754.6215
Epoch [ 115/1800] -> Loss: 1744.0355
Epoch [ 116/1800] -> Loss: 1742.9432
Epoch [ 117/1800] -> Loss: 1730.8370
Epoch [ 118/1800] -> Loss: 1726.1320
Epoch [ 119/1800] -> Loss: 1720.5814
Epoch [ 120/1800] -> Loss: 1726.3721
Epoch [ 121/1800] -> Loss: 1718.9941
Epoch [ 122/1800] -> Loss: 1695.3320
Epoch [ 123/1800] -> Loss: 1698.0997
Epoch [ 124/1800] -> Loss: 1691.9444
Epoch [ 125/1800] -> Loss: 1699.0419
Epoch [ 126/1800] -> Loss: 1681.4136
Epoch [ 127/1800] -> Loss: 1676.1673
Epoch [ 128/1800] -> Loss: 1671.8078
Epoch [ 129/1800] -> Loss: 1666.8635
Epoch [ 130/1800] -> Loss: 1667.2847
Epoch [ 131/1800] -> Loss: 1657.6292
Epoch [ 132/1800] -> Loss: 1661.5254
Epoch [ 133/1800] -> Loss: 1649.9385
Epoch [ 134/1800] -> Loss: 1643.5799
Epoch [ 135/1800] -> Loss: 1642.4243
Epoch [ 136/1800] -> Loss: 1639.4002
Epoch [ 137/1800] -> Loss: 1630.9322
Epoch [ 138/1800] -> Loss: 1626.3523
Epoch [ 139/1800] -> Loss: 1625.2362
Epoch [ 140/1800] -> Loss: 1622.8219
Epoch [ 141/1800] -> Loss: 1618.0415
Epoch [ 142/1800] -> Loss: 1620.6573
Epoch [ 143/1800] -> Loss: 1610.0200
Epoch [ 144/1800] -> Loss: 1606.6424
Epoch [ 145/1800] -> Loss: 1605.6450
Epoch [ 146/1800] -> Loss: 1605.6477
Epoch [ 147/1800] -> Loss: 1602.3557
Epoch [ 148/1800] -> Loss: 1594.5306
Epoch [ 149/1800] -> Loss: 1601.0955
Epoch [ 150/1800] -> Loss: 1590.5315
Epoch [ 151/1800] -> Loss: 1594.4612
Epoch [ 152/1800] -> Loss: 1587.2242
Epoch [ 153/1800] -> Loss: 1586.4062
Epoch [ 154/1800] -> Loss: 1593.1960
Epoch [ 155/1800] -> Loss: 1578.9200
Epoch [ 156/1800] -> Loss: 1587.6651
Epoch [ 157/1800] -> Loss: 1579.8940
Epoch [ 158/1800] -> Loss: 1576.5498
Epoch [ 159/1800] -> Loss: 1588.9728
Epoch [ 160/1800] -> Loss: 1574.0847
Epoch [ 161/1800] -> Loss: 1577.8971
Epoch [ 162/1800] -> Loss: 1571.0705
Epoch [ 163/1800] -> Loss: 1571.7426
Epoch [ 164/1800] -> Loss: 1574.4046
Epoch [ 165/1800] -> Loss: 1574.6452
Epoch [ 166/1800] -> Loss: 1561.8038
Epoch [ 167/1800] -> Loss: 1573.8616
Epoch [ 168/1800] -> Loss: 1564.3950
Epoch [ 169/1800] -> Loss: 1561.3432
Epoch [ 170/1800] -> Loss: 1562.3748
Epoch [ 171/1800] -> Loss: 1553.2471
Epoch [ 172/1800] -> Loss: 1561.3450
Epoch [ 173/1800] -> Loss: 1556.2896
Epoch [ 174/1800] -> Loss: 1566.4920
Epoch [ 175/1800] -> Loss: 1561.6510
Epoch [ 176/1800] -> Loss: 1561.0251
Epoch [ 177/1800] -> Loss: 1558.1208
Epoch [ 178/1800] -> Loss: 1559.7482
Epoch [ 179/1800] -> Loss: 1559.3854
Epoch [ 180/1800] -> Loss: 1556.5368
Epoch [ 181/1800] -> Loss: 1550.2670
Epoch [ 182/1800] -> Loss: 1552.3838
Epoch [ 183/1800] -> Loss: 1547.7653
Epoch [ 184/1800] -> Loss: 1546.7943
Epoch [ 185/1800] -> Loss: 1554.3238
Epoch [ 186/1800] -> Loss: 1553.5919
Epoch [ 187/1800] -> Loss: 1547.9492
Epoch [ 188/1800] -> Loss: 1562.5870
Epoch [ 189/1800] -> Loss: 1548.9135
Epoch [ 190/1800] -> Loss: 1542.2352
Epoch [ 191/1800] -> Loss: 1558.6035
Epoch [ 192/1800] -> Loss: 1554.8156
Epoch [ 193/1800] -> Loss: 1544.6045
Epoch [ 194/1800] -> Loss: 1547.4377
Epoch [ 195/1800] -> Loss: 1542.8764
Epoch [ 196/1800] -> Loss: 1550.6276
Epoch [ 197/1800] -> Loss: 1541.4117
Epoch [ 198/1800] -> Loss: 1553.6069
Epoch [ 199/1800] -> Loss: 1540.7539
--------------------------------------------------
Model checkpoint saved as FFNN_200.pth
--------------------------------------------------
Epoch [ 200/1800] -> Loss: 1542.5639
Epoch [ 201/1800] -> Loss: 1548.3754
Epoch [ 202/1800] -> Loss: 1546.9663
Epoch [ 203/1800] -> Loss: 1540.3885
Epoch [ 204/1800] -> Loss: 1548.2807
Epoch [ 205/1800] -> Loss: 1536.9458
Epoch [ 206/1800] -> Loss: 1539.3902
Epoch [ 207/1800] -> Loss: 1542.3832
Epoch [ 208/1800] -> Loss: 1534.8411
Epoch [ 209/1800] -> Loss: 1538.4879
Epoch [ 210/1800] -> Loss: 1536.8756
Epoch [ 211/1800] -> Loss: 1536.6914
Epoch [ 212/1800] -> Loss: 1547.6022
Epoch [ 213/1800] -> Loss: 1540.3731
Epoch [ 214/1800] -> Loss: 1532.7073
Epoch [ 215/1800] -> Loss: 1538.0561
Epoch [ 216/1800] -> Loss: 1530.3326
Epoch [ 217/1800] -> Loss: 1534.8898
Epoch [ 218/1800] -> Loss: 1537.6960
Epoch [ 219/1800] -> Loss: 1533.0967
Epoch [ 220/1800] -> Loss: 1526.5235
Epoch [ 221/1800] -> Loss: 1523.4708
Epoch [ 222/1800] -> Loss: 1536.9063
Epoch [ 223/1800] -> Loss: 1529.9267
Epoch [ 224/1800] -> Loss: 1535.9176
Epoch [ 225/1800] -> Loss: 1531.9617
Epoch [ 226/1800] -> Loss: 1534.1469
Epoch [ 227/1800] -> Loss: 1531.7956
Epoch [ 228/1800] -> Loss: 1534.6378
Epoch [ 229/1800] -> Loss: 1541.5282
Epoch [ 230/1800] -> Loss: 1523.2139
Epoch [ 231/1800] -> Loss: 1528.8309
Epoch [ 232/1800] -> Loss: 1527.5548
Epoch [ 233/1800] -> Loss: 1524.4576
Epoch [ 234/1800] -> Loss: 1532.4286
Epoch [ 235/1800] -> Loss: 1534.2602
Epoch [ 236/1800] -> Loss: 1527.7258
Epoch [ 237/1800] -> Loss: 1527.7456
Epoch [ 238/1800] -> Loss: 1526.1917
Epoch [ 239/1800] -> Loss: 1533.7115
Epoch [ 240/1800] -> Loss: 1533.9476
Epoch   241: reducing learning rate of group 0 to 2.5000e-04.
Epoch [ 241/1800] -> Loss: 1535.5496
Epoch [ 242/1800] -> Loss: 1525.2618
Epoch [ 243/1800] -> Loss: 1523.5221
Epoch [ 244/1800] -> Loss: 1525.2978
Epoch [ 245/1800] -> Loss: 1521.0967
Epoch [ 246/1800] -> Loss: 1521.5839
Epoch [ 247/1800] -> Loss: 1515.2986
Epoch [ 248/1800] -> Loss: 1526.7304
Epoch [ 249/1800] -> Loss: 1520.8429
Epoch [ 250/1800] -> Loss: 1524.4921
Epoch [ 251/1800] -> Loss: 1522.5546
Epoch [ 252/1800] -> Loss: 1523.9018
Epoch [ 253/1800] -> Loss: 1521.8861
Epoch [ 254/1800] -> Loss: 1521.9203
Epoch [ 255/1800] -> Loss: 1525.1588
Epoch [ 256/1800] -> Loss: 1522.2038
Epoch [ 257/1800] -> Loss: 1523.5847
Epoch   258: reducing learning rate of group 0 to 1.2500e-04.
Epoch [ 258/1800] -> Loss: 1519.6661
Epoch [ 259/1800] -> Loss: 1523.9021
Epoch [ 260/1800] -> Loss: 1515.4874
Epoch [ 261/1800] -> Loss: 1516.5483
Epoch [ 262/1800] -> Loss: 1515.7794
Epoch [ 263/1800] -> Loss: 1513.2653
Epoch [ 264/1800] -> Loss: 1519.9475
Epoch [ 265/1800] -> Loss: 1518.2156
Epoch [ 266/1800] -> Loss: 1515.7645
Epoch [ 267/1800] -> Loss: 1514.2879
Epoch [ 268/1800] -> Loss: 1514.2389
Epoch [ 269/1800] -> Loss: 1520.1207
Epoch [ 270/1800] -> Loss: 1514.8392
Epoch [ 271/1800] -> Loss: 1516.6510
Epoch [ 272/1800] -> Loss: 1518.5455
Epoch [ 273/1800] -> Loss: 1520.5104
Epoch   274: reducing learning rate of group 0 to 6.2500e-05.
Epoch [ 274/1800] -> Loss: 1519.9828
Epoch [ 275/1800] -> Loss: 1514.7477
Epoch [ 276/1800] -> Loss: 1515.6706
Epoch [ 277/1800] -> Loss: 1517.9702
Epoch [ 278/1800] -> Loss: 1515.3089
Epoch [ 279/1800] -> Loss: 1517.9443
Epoch [ 280/1800] -> Loss: 1514.3544
Epoch [ 281/1800] -> Loss: 1513.5641
Epoch [ 282/1800] -> Loss: 1519.4933
Epoch [ 283/1800] -> Loss: 1518.0453
Epoch [ 284/1800] -> Loss: 1516.6574
Epoch   285: reducing learning rate of group 0 to 3.1250e-05.
Epoch [ 285/1800] -> Loss: 1516.2724
Epoch [ 286/1800] -> Loss: 1514.8576
Epoch [ 287/1800] -> Loss: 1518.5546
Epoch [ 288/1800] -> Loss: 1514.3750
Epoch [ 289/1800] -> Loss: 1513.2704
Epoch [ 290/1800] -> Loss: 1514.3211
Epoch [ 291/1800] -> Loss: 1515.1324
Epoch [ 292/1800] -> Loss: 1512.3726
Epoch [ 293/1800] -> Loss: 1513.9267
Epoch [ 294/1800] -> Loss: 1513.5054
Epoch [ 295/1800] -> Loss: 1514.2920
Epoch [ 296/1800] -> Loss: 1511.2038
Epoch [ 297/1800] -> Loss: 1513.4173
Epoch [ 298/1800] -> Loss: 1512.7662
Epoch [ 299/1800] -> Loss: 1512.9747
--------------------------------------------------
Model checkpoint saved as FFNN_300.pth
--------------------------------------------------
Epoch [ 300/1800] -> Loss: 1512.8948
Epoch [ 301/1800] -> Loss: 1513.5861
Epoch [ 302/1800] -> Loss: 1513.6471
Epoch [ 303/1800] -> Loss: 1511.7080
Epoch [ 304/1800] -> Loss: 1515.7160
Epoch [ 305/1800] -> Loss: 1517.1879
Epoch [ 306/1800] -> Loss: 1511.8237
Epoch   307: reducing learning rate of group 0 to 1.5625e-05.
Epoch [ 307/1800] -> Loss: 1518.9436
Epoch [ 308/1800] -> Loss: 1511.5312
Epoch [ 309/1800] -> Loss: 1512.4048
Epoch [ 310/1800] -> Loss: 1511.1286
Epoch [ 311/1800] -> Loss: 1513.7874
Epoch [ 312/1800] -> Loss: 1512.5223
Epoch [ 313/1800] -> Loss: 1512.6080
Epoch [ 314/1800] -> Loss: 1511.1508
Epoch [ 315/1800] -> Loss: 1511.6186
Epoch [ 316/1800] -> Loss: 1512.4423
Epoch [ 317/1800] -> Loss: 1515.3192
Epoch   318: reducing learning rate of group 0 to 7.8125e-06.
Epoch [ 318/1800] -> Loss: 1513.2646
Epoch [ 319/1800] -> Loss: 1511.8015
Epoch [ 320/1800] -> Loss: 1511.6170
Epoch [ 321/1800] -> Loss: 1514.1574
Epoch [ 322/1800] -> Loss: 1514.2040
Epoch [ 323/1800] -> Loss: 1512.6017
Epoch [ 324/1800] -> Loss: 1511.9871
Epoch [ 325/1800] -> Loss: 1510.5896
Epoch [ 326/1800] -> Loss: 1517.9228
Epoch [ 327/1800] -> Loss: 1517.8486
Epoch [ 328/1800] -> Loss: 1511.8087
Epoch [ 329/1800] -> Loss: 1511.8739
Epoch [ 330/1800] -> Loss: 1513.7467
Epoch [ 331/1800] -> Loss: 1511.5092
Epoch [ 332/1800] -> Loss: 1512.6874
Epoch [ 333/1800] -> Loss: 1515.0617
Epoch [ 334/1800] -> Loss: 1513.8715
Epoch [ 335/1800] -> Loss: 1522.6007
Epoch   336: reducing learning rate of group 0 to 3.9063e-06.
Epoch [ 336/1800] -> Loss: 1513.6185
Epoch [ 337/1800] -> Loss: 1510.9785
Epoch [ 338/1800] -> Loss: 1512.8234
Epoch [ 339/1800] -> Loss: 1512.8889
Epoch [ 340/1800] -> Loss: 1512.5242
Epoch [ 341/1800] -> Loss: 1511.7402
Epoch [ 342/1800] -> Loss: 1515.7285
Epoch [ 343/1800] -> Loss: 1514.4351
Epoch [ 344/1800] -> Loss: 1510.0245
Epoch [ 345/1800] -> Loss: 1513.5717
Epoch [ 346/1800] -> Loss: 1514.9753
Epoch [ 347/1800] -> Loss: 1513.7558
Epoch [ 348/1800] -> Loss: 1517.3180
Epoch [ 349/1800] -> Loss: 1510.9718
Epoch [ 350/1800] -> Loss: 1512.6479
Epoch [ 351/1800] -> Loss: 1514.9393
Epoch [ 352/1800] -> Loss: 1511.1352
Epoch [ 353/1800] -> Loss: 1511.6019
Epoch [ 354/1800] -> Loss: 1513.6234
Epoch   355: reducing learning rate of group 0 to 1.9531e-06.
Epoch [ 355/1800] -> Loss: 1511.1911
Epoch [ 356/1800] -> Loss: 1513.7937
Epoch [ 357/1800] -> Loss: 1512.5279
Epoch [ 358/1800] -> Loss: 1511.1080
Epoch [ 359/1800] -> Loss: 1512.4143
Epoch [ 360/1800] -> Loss: 1512.5628
Epoch [ 361/1800] -> Loss: 1514.7862
Epoch [ 362/1800] -> Loss: 1511.1644
Epoch [ 363/1800] -> Loss: 1512.0224
Epoch [ 364/1800] -> Loss: 1512.2391
Epoch [ 365/1800] -> Loss: 1511.5388
Epoch   366: reducing learning rate of group 0 to 9.7656e-07.
Epoch [ 366/1800] -> Loss: 1512.1791
Epoch [ 367/1800] -> Loss: 1512.5193
Epoch [ 368/1800] -> Loss: 1513.6599
Epoch [ 369/1800] -> Loss: 1511.2697
Epoch [ 370/1800] -> Loss: 1510.3485
Epoch [ 371/1800] -> Loss: 1515.6587
Epoch [ 372/1800] -> Loss: 1512.0247
Epoch [ 373/1800] -> Loss: 1513.3466
Epoch [ 374/1800] -> Loss: 1510.9578
Epoch [ 375/1800] -> Loss: 1512.9514
Epoch [ 376/1800] -> Loss: 1510.8925
Epoch   377: reducing learning rate of group 0 to 4.8828e-07.
Epoch [ 377/1800] -> Loss: 1512.9832
Epoch [ 378/1800] -> Loss: 1514.1010
Epoch [ 379/1800] -> Loss: 1511.0358
Epoch [ 380/1800] -> Loss: 1511.9890
Epoch [ 381/1800] -> Loss: 1511.9719
Epoch [ 382/1800] -> Loss: 1512.7479
Epoch [ 383/1800] -> Loss: 1512.4402
Epoch [ 384/1800] -> Loss: 1511.8625
Epoch [ 385/1800] -> Loss: 1512.4680
Epoch [ 386/1800] -> Loss: 1511.2805
Epoch [ 387/1800] -> Loss: 1511.5481
Epoch   388: reducing learning rate of group 0 to 2.4414e-07.
Epoch [ 388/1800] -> Loss: 1513.6548
Epoch [ 389/1800] -> Loss: 1512.5148
Epoch [ 390/1800] -> Loss: 1514.5679
Epoch [ 391/1800] -> Loss: 1512.6542
Epoch [ 392/1800] -> Loss: 1513.4201
Epoch [ 393/1800] -> Loss: 1514.8681
Epoch [ 394/1800] -> Loss: 1511.5246
Epoch [ 395/1800] -> Loss: 1515.0065
Epoch [ 396/1800] -> Loss: 1511.1693
Epoch [ 397/1800] -> Loss: 1513.3484
Epoch [ 398/1800] -> Loss: 1522.7569
Epoch   399: reducing learning rate of group 0 to 1.2207e-07.
Epoch [ 399/1800] -> Loss: 1514.5039
--------------------------------------------------
Model checkpoint saved as FFNN_400.pth
--------------------------------------------------
Epoch [ 400/1800] -> Loss: 1512.1540
Epoch [ 401/1800] -> Loss: 1513.4088
Epoch [ 402/1800] -> Loss: 1513.6581
Epoch [ 403/1800] -> Loss: 1512.0954
Epoch [ 404/1800] -> Loss: 1514.1343
Epoch [ 405/1800] -> Loss: 1513.0318
Epoch [ 406/1800] -> Loss: 1514.1042
Epoch [ 407/1800] -> Loss: 1516.0567
Epoch [ 408/1800] -> Loss: 1511.3589
Epoch [ 409/1800] -> Loss: 1511.4011
Epoch   410: reducing learning rate of group 0 to 6.1035e-08.
Epoch [ 410/1800] -> Loss: 1512.2482
Epoch [ 411/1800] -> Loss: 1511.6868
Epoch [ 412/1800] -> Loss: 1513.1824
Epoch [ 413/1800] -> Loss: 1512.8922
Epoch [ 414/1800] -> Loss: 1511.0854
Epoch [ 415/1800] -> Loss: 1512.4169
Epoch [ 416/1800] -> Loss: 1513.3396
Epoch [ 417/1800] -> Loss: 1512.6457
Epoch [ 418/1800] -> Loss: 1510.7112
Epoch [ 419/1800] -> Loss: 1511.7955
Epoch [ 420/1800] -> Loss: 1510.4261
Epoch   421: reducing learning rate of group 0 to 3.0518e-08.
Epoch [ 421/1800] -> Loss: 1510.8246
Epoch [ 422/1800] -> Loss: 1511.2716
Epoch [ 423/1800] -> Loss: 1512.8027
Epoch [ 424/1800] -> Loss: 1512.0209
Epoch [ 425/1800] -> Loss: 1512.1551
Epoch [ 426/1800] -> Loss: 1513.0077
Epoch [ 427/1800] -> Loss: 1512.9481
Epoch [ 428/1800] -> Loss: 1510.8657
Epoch [ 429/1800] -> Loss: 1512.8858
Epoch [ 430/1800] -> Loss: 1511.7066
Epoch [ 431/1800] -> Loss: 1510.8519
Epoch   432: reducing learning rate of group 0 to 1.5259e-08.
Epoch [ 432/1800] -> Loss: 1511.9135
Epoch [ 433/1800] -> Loss: 1511.8344
Epoch [ 434/1800] -> Loss: 1511.6069
Epoch [ 435/1800] -> Loss: 1511.3485
Epoch [ 436/1800] -> Loss: 1512.0162
Epoch [ 437/1800] -> Loss: 1511.8001
Epoch [ 438/1800] -> Loss: 1512.1150
Epoch [ 439/1800] -> Loss: 1512.4735
Epoch [ 440/1800] -> Loss: 1512.7541
Epoch [ 441/1800] -> Loss: 1515.0028
Epoch [ 442/1800] -> Loss: 1513.1627
Epoch [ 443/1800] -> Loss: 1511.9861
Epoch [ 444/1800] -> Loss: 1515.3789
Epoch [ 445/1800] -> Loss: 1511.2873
Epoch [ 446/1800] -> Loss: 1511.8519
Epoch [ 447/1800] -> Loss: 1515.8179
Epoch [ 448/1800] -> Loss: 1512.4611
Epoch [ 449/1800] -> Loss: 1510.7509
Epoch [ 450/1800] -> Loss: 1515.0259
Epoch [ 451/1800] -> Loss: 1519.4856
Epoch [ 452/1800] -> Loss: 1513.0842
Epoch [ 453/1800] -> Loss: 1511.4365
Epoch [ 454/1800] -> Loss: 1516.2074
Epoch [ 455/1800] -> Loss: 1510.6184
Epoch [ 456/1800] -> Loss: 1514.6734
Epoch [ 457/1800] -> Loss: 1512.3057
Epoch [ 458/1800] -> Loss: 1512.4553
Epoch [ 459/1800] -> Loss: 1514.6420
Epoch [ 460/1800] -> Loss: 1510.7516
Epoch [ 461/1800] -> Loss: 1512.6834
Epoch [ 462/1800] -> Loss: 1510.6871
Epoch [ 463/1800] -> Loss: 1516.8784
Epoch [ 464/1800] -> Loss: 1510.6182
Epoch [ 465/1800] -> Loss: 1515.8482
Epoch [ 466/1800] -> Loss: 1512.6774
Epoch [ 467/1800] -> Loss: 1510.3317
Epoch [ 468/1800] -> Loss: 1511.3324
Epoch [ 469/1800] -> Loss: 1512.9181
Epoch [ 470/1800] -> Loss: 1511.7143
Epoch [ 471/1800] -> Loss: 1512.9644
Epoch [ 472/1800] -> Loss: 1513.6690
Epoch [ 473/1800] -> Loss: 1512.3365
Epoch [ 474/1800] -> Loss: 1512.0514
Epoch [ 475/1800] -> Loss: 1512.5595
Epoch [ 476/1800] -> Loss: 1513.3838
Epoch [ 477/1800] -> Loss: 1510.1847
Epoch [ 478/1800] -> Loss: 1514.9839
Epoch [ 479/1800] -> Loss: 1510.8555
Epoch [ 480/1800] -> Loss: 1511.3702
Epoch [ 481/1800] -> Loss: 1511.9608
Epoch [ 482/1800] -> Loss: 1512.4566
Epoch [ 483/1800] -> Loss: 1512.2021
Epoch [ 484/1800] -> Loss: 1512.5060
Epoch [ 485/1800] -> Loss: 1511.7214
Epoch [ 486/1800] -> Loss: 1513.1975
Epoch [ 487/1800] -> Loss: 1513.3470
Epoch [ 488/1800] -> Loss: 1512.3195
Epoch [ 489/1800] -> Loss: 1511.1848
Epoch [ 490/1800] -> Loss: 1509.9180
Epoch [ 491/1800] -> Loss: 1512.5167
Epoch [ 492/1800] -> Loss: 1512.5887
Epoch [ 493/1800] -> Loss: 1511.9590
Epoch [ 494/1800] -> Loss: 1510.7069
Epoch [ 495/1800] -> Loss: 1511.0724
Epoch [ 496/1800] -> Loss: 1513.7790
Epoch [ 497/1800] -> Loss: 1513.4152
Epoch [ 498/1800] -> Loss: 1510.4654
Epoch [ 499/1800] -> Loss: 1511.5975
--------------------------------------------------
Model checkpoint saved as FFNN_500.pth
--------------------------------------------------
Epoch [ 500/1800] -> Loss: 1511.4468
Epoch [ 501/1800] -> Loss: 1511.1079
Epoch [ 502/1800] -> Loss: 1511.3833
Epoch [ 503/1800] -> Loss: 1511.2850
Epoch [ 504/1800] -> Loss: 1514.3058
Epoch [ 505/1800] -> Loss: 1513.7935
Epoch [ 506/1800] -> Loss: 1515.6984
Epoch [ 507/1800] -> Loss: 1516.9536
Epoch [ 508/1800] -> Loss: 1511.3987
Epoch [ 509/1800] -> Loss: 1511.0481
Epoch [ 510/1800] -> Loss: 1515.9287
Epoch [ 511/1800] -> Loss: 1519.0186
Epoch [ 512/1800] -> Loss: 1511.1347
Epoch [ 513/1800] -> Loss: 1512.1508
Epoch [ 514/1800] -> Loss: 1511.3134
Epoch [ 515/1800] -> Loss: 1510.9002
Epoch [ 516/1800] -> Loss: 1511.2586
Epoch [ 517/1800] -> Loss: 1511.4619
Epoch [ 518/1800] -> Loss: 1517.3966
Epoch [ 519/1800] -> Loss: 1511.2643
Epoch [ 520/1800] -> Loss: 1511.5434
Epoch [ 521/1800] -> Loss: 1511.7387
Epoch [ 522/1800] -> Loss: 1513.2264
Epoch [ 523/1800] -> Loss: 1511.1195
Epoch [ 524/1800] -> Loss: 1513.0116
Epoch [ 525/1800] -> Loss: 1515.3728
Epoch [ 526/1800] -> Loss: 1518.3166
Epoch [ 527/1800] -> Loss: 1519.7543
Epoch [ 528/1800] -> Loss: 1515.5273
Epoch [ 529/1800] -> Loss: 1511.5788
Epoch [ 530/1800] -> Loss: 1514.2844
Epoch [ 531/1800] -> Loss: 1512.2590
Epoch [ 532/1800] -> Loss: 1512.1495
Epoch [ 533/1800] -> Loss: 1512.5416
Epoch [ 534/1800] -> Loss: 1513.9829
Epoch [ 535/1800] -> Loss: 1510.4887
Epoch [ 536/1800] -> Loss: 1511.9920
Epoch [ 537/1800] -> Loss: 1511.9863
Epoch [ 538/1800] -> Loss: 1511.4973
Epoch [ 539/1800] -> Loss: 1510.4311
Epoch [ 540/1800] -> Loss: 1511.9270
Epoch [ 541/1800] -> Loss: 1510.0675
Epoch [ 542/1800] -> Loss: 1516.2059
Epoch [ 543/1800] -> Loss: 1512.4612
Epoch [ 544/1800] -> Loss: 1515.0713
Epoch [ 545/1800] -> Loss: 1510.4695
Epoch [ 546/1800] -> Loss: 1512.4873
Epoch [ 547/1800] -> Loss: 1512.7352
Epoch [ 548/1800] -> Loss: 1511.0054
Epoch [ 549/1800] -> Loss: 1513.2074
Epoch [ 550/1800] -> Loss: 1512.2611
Epoch [ 551/1800] -> Loss: 1514.7916
Epoch [ 552/1800] -> Loss: 1509.8472
Epoch [ 553/1800] -> Loss: 1511.2507
Epoch [ 554/1800] -> Loss: 1513.8999
Epoch [ 555/1800] -> Loss: 1512.6213
Epoch [ 556/1800] -> Loss: 1512.6227
Epoch [ 557/1800] -> Loss: 1512.2660
Epoch [ 558/1800] -> Loss: 1514.2697
Epoch [ 559/1800] -> Loss: 1514.9240
Epoch [ 560/1800] -> Loss: 1510.3888
Epoch [ 561/1800] -> Loss: 1515.2256
Epoch [ 562/1800] -> Loss: 1512.2195
Epoch [ 563/1800] -> Loss: 1519.6126
Epoch [ 564/1800] -> Loss: 1511.3110
Epoch [ 565/1800] -> Loss: 1513.2191
Epoch [ 566/1800] -> Loss: 1510.9689
Epoch [ 567/1800] -> Loss: 1514.0056
Epoch [ 568/1800] -> Loss: 1514.3203
Epoch [ 569/1800] -> Loss: 1515.7596
Epoch [ 570/1800] -> Loss: 1512.1762
Epoch [ 571/1800] -> Loss: 1516.5176
Epoch [ 572/1800] -> Loss: 1509.8541
Epoch [ 573/1800] -> Loss: 1512.1313
Epoch [ 574/1800] -> Loss: 1511.7267
Epoch [ 575/1800] -> Loss: 1511.4938
Epoch [ 576/1800] -> Loss: 1510.9182
Epoch [ 577/1800] -> Loss: 1511.3661
Epoch [ 578/1800] -> Loss: 1511.3445
Epoch [ 579/1800] -> Loss: 1510.5617
Epoch [ 580/1800] -> Loss: 1511.2639
Epoch [ 581/1800] -> Loss: 1516.2112
Epoch [ 582/1800] -> Loss: 1518.1421
Epoch [ 583/1800] -> Loss: 1518.8848
Epoch [ 584/1800] -> Loss: 1513.7131
Epoch [ 585/1800] -> Loss: 1512.0532
Epoch [ 586/1800] -> Loss: 1511.0195
Epoch [ 587/1800] -> Loss: 1513.3367
Epoch [ 588/1800] -> Loss: 1510.2297
Epoch [ 589/1800] -> Loss: 1511.2364
Epoch [ 590/1800] -> Loss: 1512.0494
Epoch [ 591/1800] -> Loss: 1510.8752
Epoch [ 592/1800] -> Loss: 1511.4517
Epoch [ 593/1800] -> Loss: 1511.8720
Epoch [ 594/1800] -> Loss: 1514.8990
Epoch [ 595/1800] -> Loss: 1514.8533
Epoch [ 596/1800] -> Loss: 1512.9122
Epoch [ 597/1800] -> Loss: 1516.1507
Epoch [ 598/1800] -> Loss: 1512.6216
Epoch [ 599/1800] -> Loss: 1510.2690
--------------------------------------------------
Model checkpoint saved as FFNN_600.pth
--------------------------------------------------
Epoch [ 600/1800] -> Loss: 1512.0641
Epoch [ 601/1800] -> Loss: 1510.7223
Epoch [ 602/1800] -> Loss: 1512.4191
Epoch [ 603/1800] -> Loss: 1515.2539
Epoch [ 604/1800] -> Loss: 1511.6661
Epoch [ 605/1800] -> Loss: 1513.3895
Epoch [ 606/1800] -> Loss: 1511.9150
Epoch [ 607/1800] -> Loss: 1510.1150
Epoch [ 608/1800] -> Loss: 1515.2146
Epoch [ 609/1800] -> Loss: 1511.9817
Epoch [ 610/1800] -> Loss: 1513.1863
Epoch [ 611/1800] -> Loss: 1512.3901
Epoch [ 612/1800] -> Loss: 1512.4204
Epoch [ 613/1800] -> Loss: 1510.3971
Epoch [ 614/1800] -> Loss: 1510.8099
Epoch [ 615/1800] -> Loss: 1516.4662
Epoch [ 616/1800] -> Loss: 1511.5988
Epoch [ 617/1800] -> Loss: 1512.4521
Epoch [ 618/1800] -> Loss: 1514.2635
Epoch [ 619/1800] -> Loss: 1516.9674
Epoch [ 620/1800] -> Loss: 1512.5672
Epoch [ 621/1800] -> Loss: 1511.8707
Epoch [ 622/1800] -> Loss: 1513.3501
Epoch [ 623/1800] -> Loss: 1510.2124
Epoch [ 624/1800] -> Loss: 1515.2389
Epoch [ 625/1800] -> Loss: 1510.7688
Epoch [ 626/1800] -> Loss: 1512.2455
Epoch [ 627/1800] -> Loss: 1516.0200
Epoch [ 628/1800] -> Loss: 1516.6186
Epoch [ 629/1800] -> Loss: 1514.9430
Epoch [ 630/1800] -> Loss: 1515.3012
Epoch [ 631/1800] -> Loss: 1511.9675
Epoch [ 632/1800] -> Loss: 1518.1530
Epoch [ 633/1800] -> Loss: 1512.4415
Epoch [ 634/1800] -> Loss: 1513.5896
Epoch [ 635/1800] -> Loss: 1516.5170
Epoch [ 636/1800] -> Loss: 1515.8913
Epoch [ 637/1800] -> Loss: 1514.4338
Epoch [ 638/1800] -> Loss: 1514.1832
Epoch [ 639/1800] -> Loss: 1511.4694
Epoch [ 640/1800] -> Loss: 1510.8134
Epoch [ 641/1800] -> Loss: 1515.0463
Epoch [ 642/1800] -> Loss: 1512.7475
Epoch [ 643/1800] -> Loss: 1510.3592
Epoch [ 644/1800] -> Loss: 1515.0467
Epoch [ 645/1800] -> Loss: 1512.5784
Epoch [ 646/1800] -> Loss: 1515.2296
Epoch [ 647/1800] -> Loss: 1511.9837
Epoch [ 648/1800] -> Loss: 1513.1423
Epoch [ 649/1800] -> Loss: 1511.3589
Epoch [ 650/1800] -> Loss: 1511.0810
Epoch [ 651/1800] -> Loss: 1512.5350
Epoch [ 652/1800] -> Loss: 1513.3105
Epoch [ 653/1800] -> Loss: 1511.6139
Epoch [ 654/1800] -> Loss: 1511.0150
Epoch [ 655/1800] -> Loss: 1510.8452
Epoch [ 656/1800] -> Loss: 1512.2938
Epoch [ 657/1800] -> Loss: 1510.4314
Epoch [ 658/1800] -> Loss: 1513.7140
Epoch [ 659/1800] -> Loss: 1510.7130
Epoch [ 660/1800] -> Loss: 1511.7337
Epoch [ 661/1800] -> Loss: 1513.1861
Epoch [ 662/1800] -> Loss: 1510.5015
Epoch [ 663/1800] -> Loss: 1512.1213
Epoch [ 664/1800] -> Loss: 1512.6954
Epoch [ 665/1800] -> Loss: 1511.8881
Epoch [ 666/1800] -> Loss: 1514.5992
Epoch [ 667/1800] -> Loss: 1511.2846
Epoch [ 668/1800] -> Loss: 1511.8715
Epoch [ 669/1800] -> Loss: 1512.7637
Epoch [ 670/1800] -> Loss: 1514.7000
Epoch [ 671/1800] -> Loss: 1511.7875
Epoch [ 672/1800] -> Loss: 1512.7778
Epoch [ 673/1800] -> Loss: 1513.4633
Epoch [ 674/1800] -> Loss: 1512.5567
Epoch [ 675/1800] -> Loss: 1512.4094
Epoch [ 676/1800] -> Loss: 1513.2032
Epoch [ 677/1800] -> Loss: 1514.5679
Epoch [ 678/1800] -> Loss: 1512.0056
Epoch [ 679/1800] -> Loss: 1512.1505
Epoch [ 680/1800] -> Loss: 1513.1940
Epoch [ 681/1800] -> Loss: 1514.3216
Epoch [ 682/1800] -> Loss: 1511.8413
Epoch [ 683/1800] -> Loss: 1512.8382
Epoch [ 684/1800] -> Loss: 1518.0068
Epoch [ 685/1800] -> Loss: 1512.8381
Epoch [ 686/1800] -> Loss: 1510.1066
Epoch [ 687/1800] -> Loss: 1512.8808
Epoch [ 688/1800] -> Loss: 1511.1589
Epoch [ 689/1800] -> Loss: 1511.0576
Epoch [ 690/1800] -> Loss: 1514.1303
Epoch [ 691/1800] -> Loss: 1511.2606
Epoch [ 692/1800] -> Loss: 1511.3907
Epoch [ 693/1800] -> Loss: 1513.1432
Epoch [ 694/1800] -> Loss: 1511.8361
Epoch [ 695/1800] -> Loss: 1512.8695
Epoch [ 696/1800] -> Loss: 1511.9280
Epoch [ 697/1800] -> Loss: 1512.3599
Epoch [ 698/1800] -> Loss: 1512.0392
Epoch [ 699/1800] -> Loss: 1512.7535
--------------------------------------------------
Model checkpoint saved as FFNN_700.pth
--------------------------------------------------
Epoch [ 700/1800] -> Loss: 1510.4596
Epoch [ 701/1800] -> Loss: 1514.6136
Epoch [ 702/1800] -> Loss: 1511.4377
Epoch [ 703/1800] -> Loss: 1511.7443
Epoch [ 704/1800] -> Loss: 1512.2390
Epoch [ 705/1800] -> Loss: 1513.6447
Epoch [ 706/1800] -> Loss: 1515.2490
Epoch [ 707/1800] -> Loss: 1511.6260
Epoch [ 708/1800] -> Loss: 1510.8533
Epoch [ 709/1800] -> Loss: 1519.9105
Epoch [ 710/1800] -> Loss: 1510.3899
Epoch [ 711/1800] -> Loss: 1517.8669
Epoch [ 712/1800] -> Loss: 1511.8838
Epoch [ 713/1800] -> Loss: 1516.9765
Epoch [ 714/1800] -> Loss: 1512.2231
Epoch [ 715/1800] -> Loss: 1514.2465
Epoch [ 716/1800] -> Loss: 1511.8832
Epoch [ 717/1800] -> Loss: 1510.6223
Epoch [ 718/1800] -> Loss: 1511.3923
Epoch [ 719/1800] -> Loss: 1513.6416
Epoch [ 720/1800] -> Loss: 1511.9413
Epoch [ 721/1800] -> Loss: 1511.8223
Epoch [ 722/1800] -> Loss: 1513.6206
Epoch [ 723/1800] -> Loss: 1513.3418
Epoch [ 724/1800] -> Loss: 1511.4185
Epoch [ 725/1800] -> Loss: 1512.2896
Epoch [ 726/1800] -> Loss: 1511.2877
Epoch [ 727/1800] -> Loss: 1511.6915
Epoch [ 728/1800] -> Loss: 1512.0564
Epoch [ 729/1800] -> Loss: 1513.7455
Epoch [ 730/1800] -> Loss: 1511.1846
Epoch [ 731/1800] -> Loss: 1512.1822
Epoch [ 732/1800] -> Loss: 1514.0121
Epoch [ 733/1800] -> Loss: 1513.7083
Epoch [ 734/1800] -> Loss: 1515.3661
Epoch [ 735/1800] -> Loss: 1513.3800
Epoch [ 736/1800] -> Loss: 1513.9606
Epoch [ 737/1800] -> Loss: 1512.9946
Epoch [ 738/1800] -> Loss: 1515.5170
Epoch [ 739/1800] -> Loss: 1513.3970
Epoch [ 740/1800] -> Loss: 1511.9082
Epoch [ 741/1800] -> Loss: 1511.0875
Epoch [ 742/1800] -> Loss: 1513.9362
Epoch [ 743/1800] -> Loss: 1513.4256
Epoch [ 744/1800] -> Loss: 1514.8782
Epoch [ 745/1800] -> Loss: 1510.3874
Epoch [ 746/1800] -> Loss: 1510.7013
Epoch [ 747/1800] -> Loss: 1513.3540
Epoch [ 748/1800] -> Loss: 1510.5203
Epoch [ 749/1800] -> Loss: 1517.7240
Epoch [ 750/1800] -> Loss: 1512.2193
Epoch [ 751/1800] -> Loss: 1511.7454
Epoch [ 752/1800] -> Loss: 1516.3396
Epoch [ 753/1800] -> Loss: 1512.1617
Epoch [ 754/1800] -> Loss: 1510.9297
Epoch [ 755/1800] -> Loss: 1512.3671
Epoch [ 756/1800] -> Loss: 1513.5537
Epoch [ 757/1800] -> Loss: 1514.1569
Epoch [ 758/1800] -> Loss: 1510.5016
Epoch [ 759/1800] -> Loss: 1514.0427
Epoch [ 760/1800] -> Loss: 1511.3774
Epoch [ 761/1800] -> Loss: 1515.5063
Epoch [ 762/1800] -> Loss: 1512.6034
Epoch [ 763/1800] -> Loss: 1512.0480
Epoch [ 764/1800] -> Loss: 1512.0944
Epoch [ 765/1800] -> Loss: 1510.0622
Epoch [ 766/1800] -> Loss: 1511.3625
Epoch [ 767/1800] -> Loss: 1511.5600
Epoch [ 768/1800] -> Loss: 1511.9047
Epoch [ 769/1800] -> Loss: 1511.0547
Epoch [ 770/1800] -> Loss: 1513.7864
Epoch [ 771/1800] -> Loss: 1512.6372
Epoch [ 772/1800] -> Loss: 1511.9990
Epoch [ 773/1800] -> Loss: 1512.5287
Epoch [ 774/1800] -> Loss: 1513.9671
Epoch [ 775/1800] -> Loss: 1512.6969
Epoch [ 776/1800] -> Loss: 1515.0215
Epoch [ 777/1800] -> Loss: 1511.3155
Epoch [ 778/1800] -> Loss: 1513.7942
Epoch [ 779/1800] -> Loss: 1511.2946
Epoch [ 780/1800] -> Loss: 1514.7762
Epoch [ 781/1800] -> Loss: 1511.7668
Epoch [ 782/1800] -> Loss: 1511.3885
Epoch [ 783/1800] -> Loss: 1515.7433
Epoch [ 784/1800] -> Loss: 1512.3052
Epoch [ 785/1800] -> Loss: 1511.0268
Epoch [ 786/1800] -> Loss: 1513.5405
Epoch [ 787/1800] -> Loss: 1511.7332
Epoch [ 788/1800] -> Loss: 1513.2709
Epoch [ 789/1800] -> Loss: 1512.1471
Epoch [ 790/1800] -> Loss: 1515.1642
Epoch [ 791/1800] -> Loss: 1512.6754
Epoch [ 792/1800] -> Loss: 1510.9589
Epoch [ 793/1800] -> Loss: 1514.4988
Epoch [ 794/1800] -> Loss: 1510.9780
Epoch [ 795/1800] -> Loss: 1513.0385
Epoch [ 796/1800] -> Loss: 1512.3150
Epoch [ 797/1800] -> Loss: 1510.7756
Epoch [ 798/1800] -> Loss: 1511.3857
Epoch [ 799/1800] -> Loss: 1510.3241
--------------------------------------------------
Model checkpoint saved as FFNN_800.pth
--------------------------------------------------
Epoch [ 800/1800] -> Loss: 1512.8882
Epoch [ 801/1800] -> Loss: 1517.7935
Epoch [ 802/1800] -> Loss: 1512.5187
Epoch [ 803/1800] -> Loss: 1511.7333
Epoch [ 804/1800] -> Loss: 1514.2028
Epoch [ 805/1800] -> Loss: 1515.7702
Epoch [ 806/1800] -> Loss: 1513.0495
Epoch [ 807/1800] -> Loss: 1512.3071
Epoch [ 808/1800] -> Loss: 1513.7262
Epoch [ 809/1800] -> Loss: 1511.3337
Epoch [ 810/1800] -> Loss: 1512.3509
Epoch [ 811/1800] -> Loss: 1515.1168
Epoch [ 812/1800] -> Loss: 1513.3727
Epoch [ 813/1800] -> Loss: 1511.3529
Epoch [ 814/1800] -> Loss: 1512.9886
Epoch [ 815/1800] -> Loss: 1510.5959
Epoch [ 816/1800] -> Loss: 1511.9292
Epoch [ 817/1800] -> Loss: 1511.6749
Epoch [ 818/1800] -> Loss: 1512.4135
