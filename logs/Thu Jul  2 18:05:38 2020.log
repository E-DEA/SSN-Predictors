Usage: python3 SSN_predictor.py <path_to_ssn_datafile> <path_to_aa_datafile>
----------------------------------------------------------------
Code running on device: cuda
----------------------------------------------------------------
Data loaded from file locations :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
----------------------------------------------------------------
Solar cycle data loaded/saved as: cycle_data.pickle
----------------------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=3, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=3, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
----------------------------------------------------------------
No pre-trained models available, initializing model weights
----------------------------------------------------------------
Training model with: num_epochs=1200, start_lr=0.0005
Epoch [   1/1200] -> Loss: 123.3825
Epoch [   2/1200] -> Loss: 116.3641
Epoch [   3/1200] -> Loss: 102.0922
Epoch [   4/1200] -> Loss: 85.0835
Epoch [   5/1200] -> Loss: 66.4338
Epoch [   6/1200] -> Loss: 55.9428
Epoch [   7/1200] -> Loss: 53.7502
Epoch [   8/1200] -> Loss: 53.4478
Epoch [   9/1200] -> Loss: 53.4926
Epoch [  10/1200] -> Loss: 53.2889
Epoch [  11/1200] -> Loss: 53.1255
Epoch [  12/1200] -> Loss: 53.3183
Epoch [  13/1200] -> Loss: 53.2733
Epoch [  14/1200] -> Loss: 53.0240
Epoch [  15/1200] -> Loss: 53.5785
Epoch [  16/1200] -> Loss: 53.0161
Epoch [  17/1200] -> Loss: 53.3120
Epoch [  18/1200] -> Loss: 53.0692
Epoch [  19/1200] -> Loss: 52.9778
Epoch [  20/1200] -> Loss: 53.2417
Epoch [  21/1200] -> Loss: 52.9686
Epoch [  22/1200] -> Loss: 52.7020
Epoch [  23/1200] -> Loss: 52.7557
Epoch [  24/1200] -> Loss: 52.6533
Epoch [  25/1200] -> Loss: 53.0999
Epoch [  26/1200] -> Loss: 52.4226
Epoch [  27/1200] -> Loss: 52.5377
Epoch [  28/1200] -> Loss: 52.2397
Epoch [  29/1200] -> Loss: 52.4390
Epoch [  30/1200] -> Loss: 52.4331
Epoch [  31/1200] -> Loss: 52.3401
Epoch [  32/1200] -> Loss: 52.2071
Epoch [  33/1200] -> Loss: 52.4725
Epoch [  34/1200] -> Loss: 52.1797
Epoch [  35/1200] -> Loss: 52.0375
Epoch [  36/1200] -> Loss: 51.8063
Epoch [  37/1200] -> Loss: 51.8370
Epoch [  38/1200] -> Loss: 52.0184
Epoch [  39/1200] -> Loss: 51.8132
Epoch [  40/1200] -> Loss: 52.0232
Epoch [  41/1200] -> Loss: 51.7749
Epoch [  42/1200] -> Loss: 51.7523
Epoch [  43/1200] -> Loss: 51.3705
Epoch [  44/1200] -> Loss: 51.6364
Epoch [  45/1200] -> Loss: 51.6656
Epoch [  46/1200] -> Loss: 51.5632
Epoch [  47/1200] -> Loss: 51.0933
Epoch [  48/1200] -> Loss: 51.4392
Epoch [  49/1200] -> Loss: 51.3471
Epoch [  50/1200] -> Loss: 51.2845
Epoch [  51/1200] -> Loss: 51.3458
Epoch [  52/1200] -> Loss: 51.2430
Epoch [  53/1200] -> Loss: 51.1493
Epoch [  54/1200] -> Loss: 51.1792
Epoch [  55/1200] -> Loss: 50.9148
Epoch [  56/1200] -> Loss: 50.5595
Epoch [  57/1200] -> Loss: 50.7327
Epoch [  58/1200] -> Loss: 50.6873
Epoch [  59/1200] -> Loss: 50.8239
Epoch [  60/1200] -> Loss: 50.5121
Epoch [  61/1200] -> Loss: 50.7723
Epoch [  62/1200] -> Loss: 50.8052
Epoch [  63/1200] -> Loss: 50.7410
Epoch [  64/1200] -> Loss: 50.4578
Epoch [  65/1200] -> Loss: 50.4553
Epoch [  66/1200] -> Loss: 50.1599
Epoch [  67/1200] -> Loss: 50.2991
Epoch [  68/1200] -> Loss: 50.1609
Epoch [  69/1200] -> Loss: 50.1848
Epoch [  70/1200] -> Loss: 49.9807
Epoch [  71/1200] -> Loss: 49.7272
Epoch [  72/1200] -> Loss: 49.9280
Epoch [  73/1200] -> Loss: 50.1184
Epoch [  74/1200] -> Loss: 49.6592
Epoch [  75/1200] -> Loss: 49.5704
Epoch [  76/1200] -> Loss: 49.6459
Epoch [  77/1200] -> Loss: 49.3595
Epoch [  78/1200] -> Loss: 49.5710
Epoch [  79/1200] -> Loss: 49.2953
Epoch [  80/1200] -> Loss: 49.1035
Epoch [  81/1200] -> Loss: 49.4386
Epoch [  82/1200] -> Loss: 49.2967
Epoch [  83/1200] -> Loss: 49.1680
Epoch [  84/1200] -> Loss: 48.7997
Epoch [  85/1200] -> Loss: 48.9778
Epoch [  86/1200] -> Loss: 48.9627
Epoch [  87/1200] -> Loss: 48.4556
Epoch [  88/1200] -> Loss: 48.6144
Epoch [  89/1200] -> Loss: 48.6296
Epoch [  90/1200] -> Loss: 48.3925
Epoch [  91/1200] -> Loss: 48.3009
Epoch [  92/1200] -> Loss: 48.1986
Epoch [  93/1200] -> Loss: 48.0014
Epoch [  94/1200] -> Loss: 47.8556
Epoch [  95/1200] -> Loss: 47.7959
Epoch [  96/1200] -> Loss: 47.7909
Epoch [  97/1200] -> Loss: 47.7177
Epoch [  98/1200] -> Loss: 47.6606
Epoch [  99/1200] -> Loss: 47.5950
----------------------------------------------------------------
Model checkpoint saved as FFNN_100.pth
----------------------------------------------------------------
Epoch [ 100/1200] -> Loss: 47.4710
Epoch [ 101/1200] -> Loss: 47.2591
Epoch [ 102/1200] -> Loss: 47.0134
Epoch [ 103/1200] -> Loss: 47.2505
Epoch [ 104/1200] -> Loss: 46.8994
Epoch [ 105/1200] -> Loss: 46.2898
Epoch [ 106/1200] -> Loss: 46.9621
Epoch [ 107/1200] -> Loss: 47.2261
Epoch [ 108/1200] -> Loss: 46.1982
Epoch [ 109/1200] -> Loss: 46.4688
Epoch [ 110/1200] -> Loss: 46.4919
Epoch [ 111/1200] -> Loss: 46.2953
Epoch [ 112/1200] -> Loss: 46.0131
Epoch [ 113/1200] -> Loss: 46.2605
Epoch [ 114/1200] -> Loss: 45.9274
Epoch [ 115/1200] -> Loss: 45.5723
Epoch [ 116/1200] -> Loss: 45.4939
Epoch [ 117/1200] -> Loss: 45.4850
Epoch [ 118/1200] -> Loss: 45.3620
Epoch [ 119/1200] -> Loss: 45.0797
Epoch [ 120/1200] -> Loss: 45.0466
Epoch [ 121/1200] -> Loss: 44.4226
Epoch [ 122/1200] -> Loss: 44.6844
Epoch [ 123/1200] -> Loss: 44.6810
Epoch [ 124/1200] -> Loss: 44.4880
Epoch [ 125/1200] -> Loss: 44.3078
Epoch [ 126/1200] -> Loss: 44.5801
Epoch [ 127/1200] -> Loss: 44.3854
Epoch [ 128/1200] -> Loss: 43.9568
Epoch [ 129/1200] -> Loss: 43.9450
Epoch [ 130/1200] -> Loss: 44.1372
Epoch [ 131/1200] -> Loss: 43.3323
Epoch [ 132/1200] -> Loss: 43.9014
Epoch [ 133/1200] -> Loss: 43.5601
Epoch [ 134/1200] -> Loss: 43.5442
Epoch [ 135/1200] -> Loss: 43.2488
Epoch [ 136/1200] -> Loss: 42.6227
Epoch [ 137/1200] -> Loss: 43.1894
Epoch [ 138/1200] -> Loss: 42.9612
Epoch [ 139/1200] -> Loss: 42.7893
Epoch [ 140/1200] -> Loss: 42.9167
Epoch [ 141/1200] -> Loss: 42.6446
Epoch [ 142/1200] -> Loss: 42.6241
Epoch [ 143/1200] -> Loss: 42.5450
Epoch [ 144/1200] -> Loss: 42.3540
Epoch [ 145/1200] -> Loss: 42.2350
Epoch [ 146/1200] -> Loss: 42.2172
Epoch [ 147/1200] -> Loss: 42.2462
Epoch [ 148/1200] -> Loss: 42.1402
Epoch [ 149/1200] -> Loss: 41.9997
Epoch [ 150/1200] -> Loss: 41.7502
Epoch [ 151/1200] -> Loss: 41.3430
Epoch [ 152/1200] -> Loss: 41.7099
Epoch [ 153/1200] -> Loss: 41.3831
Epoch [ 154/1200] -> Loss: 41.4078
Epoch [ 155/1200] -> Loss: 41.1910
Epoch [ 156/1200] -> Loss: 41.0016
Epoch [ 157/1200] -> Loss: 40.7683
Epoch [ 158/1200] -> Loss: 41.2359
Epoch [ 159/1200] -> Loss: 41.0071
Epoch [ 160/1200] -> Loss: 40.8603
Epoch [ 161/1200] -> Loss: 40.7420
Epoch [ 162/1200] -> Loss: 40.5922
Epoch [ 163/1200] -> Loss: 40.7773
Epoch [ 164/1200] -> Loss: 40.7711
Epoch [ 165/1200] -> Loss: 40.5125
Epoch [ 166/1200] -> Loss: 40.5559
Epoch [ 167/1200] -> Loss: 40.5158
Epoch [ 168/1200] -> Loss: 40.3330
Epoch [ 169/1200] -> Loss: 40.1004
Epoch [ 170/1200] -> Loss: 40.1291
Epoch [ 171/1200] -> Loss: 40.1155
Epoch [ 172/1200] -> Loss: 40.1803
Epoch [ 173/1200] -> Loss: 40.1788
Epoch [ 174/1200] -> Loss: 39.8413
Epoch [ 175/1200] -> Loss: 39.8647
Epoch [ 176/1200] -> Loss: 39.8609
Epoch [ 177/1200] -> Loss: 40.0471
Epoch [ 178/1200] -> Loss: 39.7202
Epoch [ 179/1200] -> Loss: 39.7247
Epoch [ 180/1200] -> Loss: 39.6982
Epoch [ 181/1200] -> Loss: 39.8576
Epoch [ 182/1200] -> Loss: 39.7551
Epoch [ 183/1200] -> Loss: 39.3445
Epoch [ 184/1200] -> Loss: 39.2580
Epoch [ 185/1200] -> Loss: 39.4012
Epoch [ 186/1200] -> Loss: 39.3831
Epoch [ 187/1200] -> Loss: 39.2038
Epoch [ 188/1200] -> Loss: 39.5001
Epoch [ 189/1200] -> Loss: 39.4062
Epoch [ 190/1200] -> Loss: 39.4858
Epoch [ 191/1200] -> Loss: 38.9269
Epoch [ 192/1200] -> Loss: 39.0162
Epoch [ 193/1200] -> Loss: 38.8977
Epoch [ 194/1200] -> Loss: 38.9754
Epoch [ 195/1200] -> Loss: 39.1428
Epoch [ 196/1200] -> Loss: 39.0358
Epoch [ 197/1200] -> Loss: 39.0514
Epoch [ 198/1200] -> Loss: 39.0043
Epoch [ 199/1200] -> Loss: 38.9783
----------------------------------------------------------------
Model checkpoint saved as FFNN_200.pth
----------------------------------------------------------------
Epoch [ 200/1200] -> Loss: 38.9635
Epoch [ 201/1200] -> Loss: 38.8891
Epoch [ 202/1200] -> Loss: 38.8687
Epoch [ 203/1200] -> Loss: 38.8553
Epoch [ 204/1200] -> Loss: 38.7230
Epoch [ 205/1200] -> Loss: 38.4790
Epoch [ 206/1200] -> Loss: 38.7899
Epoch [ 207/1200] -> Loss: 38.6860
Epoch [ 208/1200] -> Loss: 38.6031
Epoch [ 209/1200] -> Loss: 38.3726
Epoch [ 210/1200] -> Loss: 38.5403
Epoch [ 211/1200] -> Loss: 38.5282
Epoch [ 212/1200] -> Loss: 38.4127
Epoch [ 213/1200] -> Loss: 38.4206
Epoch [ 214/1200] -> Loss: 38.7942
Epoch [ 215/1200] -> Loss: 38.2370
Epoch [ 216/1200] -> Loss: 38.6053
Epoch [ 217/1200] -> Loss: 38.5979
Epoch [ 218/1200] -> Loss: 38.3451
Epoch [ 219/1200] -> Loss: 38.3967
Epoch [ 220/1200] -> Loss: 38.1774
Epoch [ 221/1200] -> Loss: 38.1745
Epoch [ 222/1200] -> Loss: 38.3197
Epoch [ 223/1200] -> Loss: 38.1145
Epoch [ 224/1200] -> Loss: 38.3141
Epoch [ 225/1200] -> Loss: 38.4295
Epoch [ 226/1200] -> Loss: 38.1232
Epoch [ 227/1200] -> Loss: 38.1665
Epoch [ 228/1200] -> Loss: 38.2484
Epoch [ 229/1200] -> Loss: 38.2882
Epoch [ 230/1200] -> Loss: 38.1703
Epoch [ 231/1200] -> Loss: 38.1647
Epoch [ 232/1200] -> Loss: 38.2366
Epoch [ 233/1200] -> Loss: 38.0744
Epoch [ 234/1200] -> Loss: 38.2625
Epoch [ 235/1200] -> Loss: 38.1925
Epoch [ 236/1200] -> Loss: 38.0689
Epoch [ 237/1200] -> Loss: 38.1786
Epoch [ 238/1200] -> Loss: 37.8294
Epoch [ 239/1200] -> Loss: 38.0526
Epoch [ 240/1200] -> Loss: 38.0149
Epoch [ 241/1200] -> Loss: 37.8549
Epoch [ 242/1200] -> Loss: 38.0189
Epoch [ 243/1200] -> Loss: 37.8989
Epoch [ 244/1200] -> Loss: 37.7692
Epoch [ 245/1200] -> Loss: 38.1066
Epoch [ 246/1200] -> Loss: 37.9913
Epoch [ 247/1200] -> Loss: 37.9766
Epoch [ 248/1200] -> Loss: 38.0609
Epoch [ 249/1200] -> Loss: 37.6098
Epoch [ 250/1200] -> Loss: 37.7545
Epoch [ 251/1200] -> Loss: 38.0440
Epoch [ 252/1200] -> Loss: 38.0773
Epoch [ 253/1200] -> Loss: 37.6059
Epoch [ 254/1200] -> Loss: 38.2116
Epoch [ 255/1200] -> Loss: 38.0977
Epoch [ 256/1200] -> Loss: 37.6196
Epoch [ 257/1200] -> Loss: 37.9478
Epoch [ 258/1200] -> Loss: 38.0215
Epoch [ 259/1200] -> Loss: 37.8000
Epoch [ 260/1200] -> Loss: 37.6859
Epoch [ 261/1200] -> Loss: 37.9279
Epoch [ 262/1200] -> Loss: 37.8467
Epoch [ 263/1200] -> Loss: 37.8896
Epoch   264: reducing learning rate of group 0 to 2.5000e-04.
Epoch [ 264/1200] -> Loss: 37.6673
Epoch [ 265/1200] -> Loss: 37.8272
Epoch [ 266/1200] -> Loss: 37.6701
Epoch [ 267/1200] -> Loss: 37.8125
Epoch [ 268/1200] -> Loss: 38.0088
Epoch [ 269/1200] -> Loss: 37.7878
Epoch [ 270/1200] -> Loss: 37.5916
Epoch [ 271/1200] -> Loss: 37.6953
Epoch [ 272/1200] -> Loss: 37.6721
Epoch [ 273/1200] -> Loss: 37.7734
Epoch [ 274/1200] -> Loss: 37.6207
Epoch [ 275/1200] -> Loss: 37.6650
Epoch [ 276/1200] -> Loss: 37.9420
Epoch [ 277/1200] -> Loss: 37.5830
Epoch [ 278/1200] -> Loss: 37.7820
Epoch [ 279/1200] -> Loss: 37.5637
Epoch [ 280/1200] -> Loss: 37.8047
Epoch [ 281/1200] -> Loss: 37.7318
Epoch [ 282/1200] -> Loss: 37.7843
Epoch [ 283/1200] -> Loss: 37.5575
Epoch [ 284/1200] -> Loss: 37.7285
Epoch [ 285/1200] -> Loss: 37.7532
Epoch [ 286/1200] -> Loss: 37.5460
Epoch [ 287/1200] -> Loss: 38.0999
Epoch [ 288/1200] -> Loss: 37.7403
Epoch [ 289/1200] -> Loss: 37.8692
Epoch [ 290/1200] -> Loss: 37.6335
Epoch [ 291/1200] -> Loss: 37.7960
Epoch [ 292/1200] -> Loss: 37.7964
Epoch [ 293/1200] -> Loss: 37.6739
Epoch [ 294/1200] -> Loss: 37.4041
Epoch [ 295/1200] -> Loss: 37.7486
Epoch [ 296/1200] -> Loss: 37.7257
Epoch [ 297/1200] -> Loss: 37.6226
Epoch [ 298/1200] -> Loss: 37.5697
Epoch [ 299/1200] -> Loss: 37.5842
----------------------------------------------------------------
Model checkpoint saved as FFNN_300.pth
----------------------------------------------------------------
Epoch [ 300/1200] -> Loss: 37.7764
Epoch [ 301/1200] -> Loss: 37.5367
Epoch [ 302/1200] -> Loss: 37.6556
Epoch [ 303/1200] -> Loss: 37.7244
Epoch [ 304/1200] -> Loss: 37.3603
Epoch [ 305/1200] -> Loss: 37.6820
Epoch [ 306/1200] -> Loss: 37.6922
Epoch [ 307/1200] -> Loss: 37.3463
Epoch [ 308/1200] -> Loss: 37.4820
Epoch [ 309/1200] -> Loss: 37.5939
Epoch [ 310/1200] -> Loss: 37.4917
Epoch [ 311/1200] -> Loss: 37.5489
Epoch [ 312/1200] -> Loss: 37.7108
Epoch [ 313/1200] -> Loss: 37.6228
Epoch [ 314/1200] -> Loss: 37.5278
Epoch [ 315/1200] -> Loss: 37.6318
Epoch [ 316/1200] -> Loss: 37.5974
Epoch [ 317/1200] -> Loss: 37.5961
Epoch   318: reducing learning rate of group 0 to 1.2500e-04.
Epoch [ 318/1200] -> Loss: 37.8303
Epoch [ 319/1200] -> Loss: 37.4929
Epoch [ 320/1200] -> Loss: 37.6582
Epoch [ 321/1200] -> Loss: 37.5393
Epoch [ 322/1200] -> Loss: 37.4164
Epoch [ 323/1200] -> Loss: 37.7978
Epoch [ 324/1200] -> Loss: 37.6658
Epoch [ 325/1200] -> Loss: 37.4460
Epoch [ 326/1200] -> Loss: 37.5501
Epoch [ 327/1200] -> Loss: 37.3579
Epoch [ 328/1200] -> Loss: 37.5359
Epoch   329: reducing learning rate of group 0 to 6.2500e-05.
Epoch [ 329/1200] -> Loss: 37.5679
Epoch [ 330/1200] -> Loss: 37.6997
Epoch [ 331/1200] -> Loss: 37.4536
Epoch [ 332/1200] -> Loss: 37.5901
Epoch [ 333/1200] -> Loss: 37.6747
Epoch [ 334/1200] -> Loss: 37.4087
Epoch [ 335/1200] -> Loss: 37.5257
Epoch [ 336/1200] -> Loss: 37.8080
Epoch [ 337/1200] -> Loss: 37.2512
Epoch [ 338/1200] -> Loss: 37.6157
Epoch [ 339/1200] -> Loss: 37.4997
Epoch [ 340/1200] -> Loss: 37.5917
Epoch [ 341/1200] -> Loss: 37.3199
Epoch [ 342/1200] -> Loss: 37.7390
Epoch [ 343/1200] -> Loss: 37.4656
Epoch [ 344/1200] -> Loss: 37.6001
Epoch [ 345/1200] -> Loss: 37.5957
Epoch [ 346/1200] -> Loss: 37.7140
Epoch [ 347/1200] -> Loss: 37.5265
Epoch   348: reducing learning rate of group 0 to 3.1250e-05.
Epoch [ 348/1200] -> Loss: 37.4530
Epoch [ 349/1200] -> Loss: 37.4344
Epoch [ 350/1200] -> Loss: 37.6323
Epoch [ 351/1200] -> Loss: 37.5279
Epoch [ 352/1200] -> Loss: 37.5662
Epoch [ 353/1200] -> Loss: 37.5240
Epoch [ 354/1200] -> Loss: 37.4656
Epoch [ 355/1200] -> Loss: 37.3717
Epoch [ 356/1200] -> Loss: 37.6522
Epoch [ 357/1200] -> Loss: 37.6105
Epoch [ 358/1200] -> Loss: 37.3777
Epoch   359: reducing learning rate of group 0 to 1.5625e-05.
Epoch [ 359/1200] -> Loss: 37.5304
Epoch [ 360/1200] -> Loss: 37.6298
Epoch [ 361/1200] -> Loss: 37.2832
Epoch [ 362/1200] -> Loss: 37.6933
Epoch [ 363/1200] -> Loss: 37.4296
Epoch [ 364/1200] -> Loss: 37.4866
Epoch [ 365/1200] -> Loss: 37.6673
Epoch [ 366/1200] -> Loss: 37.5404
Epoch [ 367/1200] -> Loss: 37.5497
Epoch [ 368/1200] -> Loss: 37.1214
Epoch [ 369/1200] -> Loss: 37.4296
Epoch [ 370/1200] -> Loss: 37.5356
Epoch [ 371/1200] -> Loss: 37.6811
Epoch [ 372/1200] -> Loss: 37.4412
Epoch [ 373/1200] -> Loss: 37.3426
Epoch [ 374/1200] -> Loss: 37.6570
Epoch [ 375/1200] -> Loss: 37.4234
Epoch [ 376/1200] -> Loss: 37.4351
Epoch [ 377/1200] -> Loss: 37.6259
Epoch [ 378/1200] -> Loss: 37.5346
Epoch   379: reducing learning rate of group 0 to 7.8125e-06.
Epoch [ 379/1200] -> Loss: 37.5927
Epoch [ 380/1200] -> Loss: 37.5792
Epoch [ 381/1200] -> Loss: 37.2247
Epoch [ 382/1200] -> Loss: 37.5966
Epoch [ 383/1200] -> Loss: 37.6895
Epoch [ 384/1200] -> Loss: 37.6345
Epoch [ 385/1200] -> Loss: 37.6835
Epoch [ 386/1200] -> Loss: 37.2062
Epoch [ 387/1200] -> Loss: 37.5803
Epoch [ 388/1200] -> Loss: 37.6975
Epoch [ 389/1200] -> Loss: 37.5815
Epoch   390: reducing learning rate of group 0 to 3.9063e-06.
Epoch [ 390/1200] -> Loss: 37.2846
Epoch [ 391/1200] -> Loss: 37.5037
Epoch [ 392/1200] -> Loss: 36.9689
Epoch [ 393/1200] -> Loss: 37.5494
Epoch [ 394/1200] -> Loss: 37.5720
Epoch [ 395/1200] -> Loss: 37.5396
Epoch [ 396/1200] -> Loss: 37.6841
Epoch [ 397/1200] -> Loss: 37.5614
Epoch [ 398/1200] -> Loss: 37.6311
Epoch [ 399/1200] -> Loss: 37.3005
----------------------------------------------------------------
Model checkpoint saved as FFNN_400.pth
----------------------------------------------------------------
Epoch [ 400/1200] -> Loss: 37.6139
Epoch [ 401/1200] -> Loss: 37.5465
Epoch [ 402/1200] -> Loss: 37.5959
Epoch   403: reducing learning rate of group 0 to 1.9531e-06.
Epoch [ 403/1200] -> Loss: 37.2696
Epoch [ 404/1200] -> Loss: 37.5903
Epoch [ 405/1200] -> Loss: 37.5440
Epoch [ 406/1200] -> Loss: 37.4640
Epoch [ 407/1200] -> Loss: 37.6523
Epoch [ 408/1200] -> Loss: 37.5971
Epoch [ 409/1200] -> Loss: 37.5820
Epoch [ 410/1200] -> Loss: 37.2519
Epoch [ 411/1200] -> Loss: 37.5232
Epoch [ 412/1200] -> Loss: 37.1696
Epoch [ 413/1200] -> Loss: 37.5445
Epoch   414: reducing learning rate of group 0 to 9.7656e-07.
Epoch [ 414/1200] -> Loss: 37.4148
Epoch [ 415/1200] -> Loss: 37.6033
Epoch [ 416/1200] -> Loss: 37.6629
Epoch [ 417/1200] -> Loss: 37.4826
Epoch [ 418/1200] -> Loss: 37.7512
Epoch [ 419/1200] -> Loss: 37.3898
Epoch [ 420/1200] -> Loss: 37.3000
Epoch [ 421/1200] -> Loss: 37.5629
Epoch [ 422/1200] -> Loss: 37.2754
Epoch [ 423/1200] -> Loss: 37.5076
Epoch [ 424/1200] -> Loss: 37.2765
Epoch   425: reducing learning rate of group 0 to 4.8828e-07.
Epoch [ 425/1200] -> Loss: 37.4627
Epoch [ 426/1200] -> Loss: 37.5351
Epoch [ 427/1200] -> Loss: 37.3125
Epoch [ 428/1200] -> Loss: 37.7002
Epoch [ 429/1200] -> Loss: 37.3785
Epoch [ 430/1200] -> Loss: 37.5086
Epoch [ 431/1200] -> Loss: 37.4957
Epoch [ 432/1200] -> Loss: 37.3973
Epoch [ 433/1200] -> Loss: 37.4703
Epoch [ 434/1200] -> Loss: 37.5226
Epoch [ 435/1200] -> Loss: 37.8134
Epoch   436: reducing learning rate of group 0 to 2.4414e-07.
Epoch [ 436/1200] -> Loss: 37.4135
Epoch [ 437/1200] -> Loss: 37.4913
Epoch [ 438/1200] -> Loss: 37.2566
Epoch [ 439/1200] -> Loss: 37.5461
Epoch [ 440/1200] -> Loss: 37.4158
Epoch [ 441/1200] -> Loss: 37.6385
Epoch [ 442/1200] -> Loss: 37.4243
Epoch [ 443/1200] -> Loss: 37.5939
Epoch [ 444/1200] -> Loss: 37.5659
Epoch [ 445/1200] -> Loss: 37.4458
Epoch [ 446/1200] -> Loss: 37.7020
Epoch   447: reducing learning rate of group 0 to 1.2207e-07.
Epoch [ 447/1200] -> Loss: 37.5277
Epoch [ 448/1200] -> Loss: 37.5030
Epoch [ 449/1200] -> Loss: 37.5380
Epoch [ 450/1200] -> Loss: 37.5374
Epoch [ 451/1200] -> Loss: 37.3837
Epoch [ 452/1200] -> Loss: 37.3104
Epoch [ 453/1200] -> Loss: 37.5129
Epoch [ 454/1200] -> Loss: 37.7350
Epoch [ 455/1200] -> Loss: 37.5119
Epoch [ 456/1200] -> Loss: 37.4534
Epoch [ 457/1200] -> Loss: 37.4687
Epoch   458: reducing learning rate of group 0 to 6.1035e-08.
Epoch [ 458/1200] -> Loss: 37.6038
Epoch [ 459/1200] -> Loss: 37.7059
Epoch [ 460/1200] -> Loss: 37.4729
Epoch [ 461/1200] -> Loss: 37.5675
Epoch [ 462/1200] -> Loss: 37.6181
Epoch [ 463/1200] -> Loss: 37.4230
Epoch [ 464/1200] -> Loss: 37.2660
Epoch [ 465/1200] -> Loss: 37.5016
Epoch [ 466/1200] -> Loss: 37.4641
Epoch [ 467/1200] -> Loss: 37.5190
Epoch [ 468/1200] -> Loss: 37.6533
Epoch   469: reducing learning rate of group 0 to 3.0518e-08.
Epoch [ 469/1200] -> Loss: 37.5487
Epoch [ 470/1200] -> Loss: 37.4540
Epoch [ 471/1200] -> Loss: 37.6963
Epoch [ 472/1200] -> Loss: 37.2208
Epoch [ 473/1200] -> Loss: 37.3344
Epoch [ 474/1200] -> Loss: 37.5157
Epoch [ 475/1200] -> Loss: 37.7417
Epoch [ 476/1200] -> Loss: 37.4685
Epoch [ 477/1200] -> Loss: 37.4770
Epoch [ 478/1200] -> Loss: 37.4466
Epoch [ 479/1200] -> Loss: 37.4112
Epoch   480: reducing learning rate of group 0 to 1.5259e-08.
Epoch [ 480/1200] -> Loss: 37.6510
Epoch [ 481/1200] -> Loss: 37.3602
Epoch [ 482/1200] -> Loss: 37.4055
Epoch [ 483/1200] -> Loss: 37.1577
Epoch [ 484/1200] -> Loss: 37.3025
Epoch [ 485/1200] -> Loss: 37.5280
Epoch [ 486/1200] -> Loss: 37.3416
Epoch [ 487/1200] -> Loss: 37.5372
Epoch [ 488/1200] -> Loss: 37.3859
Epoch [ 489/1200] -> Loss: 37.6367
Epoch [ 490/1200] -> Loss: 37.6615
Epoch [ 491/1200] -> Loss: 37.3925
Epoch [ 492/1200] -> Loss: 37.3683
Epoch [ 493/1200] -> Loss: 37.4441
Epoch [ 494/1200] -> Loss: 37.5606
Epoch [ 495/1200] -> Loss: 37.4844
Epoch [ 496/1200] -> Loss: 37.5052
Epoch [ 497/1200] -> Loss: 37.4106
Epoch [ 498/1200] -> Loss: 37.2642
Epoch [ 499/1200] -> Loss: 37.3116
----------------------------------------------------------------
Model checkpoint saved as FFNN_500.pth
----------------------------------------------------------------
Epoch [ 500/1200] -> Loss: 37.3600
Epoch [ 501/1200] -> Loss: 37.7157
Epoch [ 502/1200] -> Loss: 37.5280
Epoch [ 503/1200] -> Loss: 37.5340
Epoch [ 504/1200] -> Loss: 37.6564
Epoch [ 505/1200] -> Loss: 37.3368
Epoch [ 506/1200] -> Loss: 37.4592
Epoch [ 507/1200] -> Loss: 37.1895
Epoch [ 508/1200] -> Loss: 36.9737
Epoch [ 509/1200] -> Loss: 37.3571
Epoch [ 510/1200] -> Loss: 37.6064
Epoch [ 511/1200] -> Loss: 37.4335
Epoch [ 512/1200] -> Loss: 37.6879
Epoch [ 513/1200] -> Loss: 37.2524
Epoch [ 514/1200] -> Loss: 37.5497
Epoch [ 515/1200] -> Loss: 37.5337
Epoch [ 516/1200] -> Loss: 37.4273
Epoch [ 517/1200] -> Loss: 37.5398
Epoch [ 518/1200] -> Loss: 37.4031
Epoch [ 519/1200] -> Loss: 37.5260
Epoch [ 520/1200] -> Loss: 37.5402
Epoch [ 521/1200] -> Loss: 37.6239
Epoch [ 522/1200] -> Loss: 37.4156
Epoch [ 523/1200] -> Loss: 37.4307
Epoch [ 524/1200] -> Loss: 37.5339
Epoch [ 525/1200] -> Loss: 37.6200
Epoch [ 526/1200] -> Loss: 37.4333
Epoch [ 527/1200] -> Loss: 37.3131
Epoch [ 528/1200] -> Loss: 37.4934
Epoch [ 529/1200] -> Loss: 37.5522
Epoch [ 530/1200] -> Loss: 37.3607
Epoch [ 531/1200] -> Loss: 37.4895
Epoch [ 532/1200] -> Loss: 37.3307
Epoch [ 533/1200] -> Loss: 37.5967
Epoch [ 534/1200] -> Loss: 37.5579
Epoch [ 535/1200] -> Loss: 37.4658
Epoch [ 536/1200] -> Loss: 37.4127
Epoch [ 537/1200] -> Loss: 37.3286
Epoch [ 538/1200] -> Loss: 37.6502
Epoch [ 539/1200] -> Loss: 37.5056
Epoch [ 540/1200] -> Loss: 37.4942
Epoch [ 541/1200] -> Loss: 37.4395
Epoch [ 542/1200] -> Loss: 37.3716
Epoch [ 543/1200] -> Loss: 37.2382
Epoch [ 544/1200] -> Loss: 37.5806
Epoch [ 545/1200] -> Loss: 37.4312
Epoch [ 546/1200] -> Loss: 37.2763
Epoch [ 547/1200] -> Loss: 37.4486
Epoch [ 548/1200] -> Loss: 37.3139
Epoch [ 549/1200] -> Loss: 37.4163
Epoch [ 550/1200] -> Loss: 37.4462
Epoch [ 551/1200] -> Loss: 37.1393
Epoch [ 552/1200] -> Loss: 37.3383
Epoch [ 553/1200] -> Loss: 37.5204
Epoch [ 554/1200] -> Loss: 37.3494
Epoch [ 555/1200] -> Loss: 37.7511
Epoch [ 556/1200] -> Loss: 37.6175
Epoch [ 557/1200] -> Loss: 37.5406
Epoch [ 558/1200] -> Loss: 37.5634
Epoch [ 559/1200] -> Loss: 37.2719
Epoch [ 560/1200] -> Loss: 37.4607
Epoch [ 561/1200] -> Loss: 37.5627
Epoch [ 562/1200] -> Loss: 37.3724
Epoch [ 563/1200] -> Loss: 37.4689
Epoch [ 564/1200] -> Loss: 37.4314
Epoch [ 565/1200] -> Loss: 37.3609
Epoch [ 566/1200] -> Loss: 37.7927
Epoch [ 567/1200] -> Loss: 37.3537
Epoch [ 568/1200] -> Loss: 37.4627
Epoch [ 569/1200] -> Loss: 37.3564
Epoch [ 570/1200] -> Loss: 37.2924
Epoch [ 571/1200] -> Loss: 37.5185
Epoch [ 572/1200] -> Loss: 37.7775
Epoch [ 573/1200] -> Loss: 37.5080
Epoch [ 574/1200] -> Loss: 37.4686
Epoch [ 575/1200] -> Loss: 37.2762
Epoch [ 576/1200] -> Loss: 37.7485
Epoch [ 577/1200] -> Loss: 37.4601
Epoch [ 578/1200] -> Loss: 37.6418
Epoch [ 579/1200] -> Loss: 37.4767
Epoch [ 580/1200] -> Loss: 37.2930
Epoch [ 581/1200] -> Loss: 37.6651
Epoch [ 582/1200] -> Loss: 37.5063
Epoch [ 583/1200] -> Loss: 37.3677
Epoch [ 584/1200] -> Loss: 37.5068
Epoch [ 585/1200] -> Loss: 37.5438
Epoch [ 586/1200] -> Loss: 37.4245
Epoch [ 587/1200] -> Loss: 37.5052
Epoch [ 588/1200] -> Loss: 37.4674
Epoch [ 589/1200] -> Loss: 37.3599
Epoch [ 590/1200] -> Loss: 37.4831
Epoch [ 591/1200] -> Loss: 37.3585
Epoch [ 592/1200] -> Loss: 37.5246
Epoch [ 593/1200] -> Loss: 37.6047
Epoch [ 594/1200] -> Loss: 37.5323
Epoch [ 595/1200] -> Loss: 37.5218
Epoch [ 596/1200] -> Loss: 37.3710
Epoch [ 597/1200] -> Loss: 37.4159
Epoch [ 598/1200] -> Loss: 37.2787
Epoch [ 599/1200] -> Loss: 37.4594
----------------------------------------------------------------
Model checkpoint saved as FFNN_600.pth
----------------------------------------------------------------
Epoch [ 600/1200] -> Loss: 37.5574
Epoch [ 601/1200] -> Loss: 37.5915
Epoch [ 602/1200] -> Loss: 37.4775
Epoch [ 603/1200] -> Loss: 37.5664
Epoch [ 604/1200] -> Loss: 37.5462
Epoch [ 605/1200] -> Loss: 37.6879
Epoch [ 606/1200] -> Loss: 37.4709
Epoch [ 607/1200] -> Loss: 37.3624
Epoch [ 608/1200] -> Loss: 37.4496
Epoch [ 609/1200] -> Loss: 37.4045
Epoch [ 610/1200] -> Loss: 37.7189
Epoch [ 611/1200] -> Loss: 37.5244
Epoch [ 612/1200] -> Loss: 37.5100
Epoch [ 613/1200] -> Loss: 37.5723
Epoch [ 614/1200] -> Loss: 37.0837
Epoch [ 615/1200] -> Loss: 37.5024
Epoch [ 616/1200] -> Loss: 37.5738
Epoch [ 617/1200] -> Loss: 37.2376
Epoch [ 618/1200] -> Loss: 37.5593
Epoch [ 619/1200] -> Loss: 37.5027
Epoch [ 620/1200] -> Loss: 37.4534
Epoch [ 621/1200] -> Loss: 37.7091
Epoch [ 622/1200] -> Loss: 37.7042
Epoch [ 623/1200] -> Loss: 37.2998
Epoch [ 624/1200] -> Loss: 37.4733
Epoch [ 625/1200] -> Loss: 37.5685
Epoch [ 626/1200] -> Loss: 37.4811
Epoch [ 627/1200] -> Loss: 37.3113
Epoch [ 628/1200] -> Loss: 37.6613
Epoch [ 629/1200] -> Loss: 37.5026
Epoch [ 630/1200] -> Loss: 37.5142
Epoch [ 631/1200] -> Loss: 37.6063
Epoch [ 632/1200] -> Loss: 37.5927
Epoch [ 633/1200] -> Loss: 37.4810
Epoch [ 634/1200] -> Loss: 37.5264
Epoch [ 635/1200] -> Loss: 37.5527
Epoch [ 636/1200] -> Loss: 37.4661
Epoch [ 637/1200] -> Loss: 37.6863
Epoch [ 638/1200] -> Loss: 37.7098
Epoch [ 639/1200] -> Loss: 37.6187
Epoch [ 640/1200] -> Loss: 37.5766
Epoch [ 641/1200] -> Loss: 37.5716
Epoch [ 642/1200] -> Loss: 37.4722
Epoch [ 643/1200] -> Loss: 37.4894
Epoch [ 644/1200] -> Loss: 37.2264
Epoch [ 645/1200] -> Loss: 37.5718
Epoch [ 646/1200] -> Loss: 37.6271
Epoch [ 647/1200] -> Loss: 37.2580
Epoch [ 648/1200] -> Loss: 37.3487
Epoch [ 649/1200] -> Loss: 37.4177
Epoch [ 650/1200] -> Loss: 37.5999
Epoch [ 651/1200] -> Loss: 37.4795
Epoch [ 652/1200] -> Loss: 37.4465
Epoch [ 653/1200] -> Loss: 37.2356
Epoch [ 654/1200] -> Loss: 37.7593
Epoch [ 655/1200] -> Loss: 37.3931
Epoch [ 656/1200] -> Loss: 37.5633
Epoch [ 657/1200] -> Loss: 37.3761
Epoch [ 658/1200] -> Loss: 37.4690
Epoch [ 659/1200] -> Loss: 37.2701
Epoch [ 660/1200] -> Loss: 37.5531
Epoch [ 661/1200] -> Loss: 37.3715
Epoch [ 662/1200] -> Loss: 37.6938
Epoch [ 663/1200] -> Loss: 37.2405
Epoch [ 664/1200] -> Loss: 37.4832
Epoch [ 665/1200] -> Loss: 37.7079
Epoch [ 666/1200] -> Loss: 37.7282
Epoch [ 667/1200] -> Loss: 37.7075
Epoch [ 668/1200] -> Loss: 37.4777
Epoch [ 669/1200] -> Loss: 37.4378
Epoch [ 670/1200] -> Loss: 37.2297
Epoch [ 671/1200] -> Loss: 37.5808
Epoch [ 672/1200] -> Loss: 37.5201
Epoch [ 673/1200] -> Loss: 37.4454
Epoch [ 674/1200] -> Loss: 37.6073
Epoch [ 675/1200] -> Loss: 37.1885
Epoch [ 676/1200] -> Loss: 37.6352
Epoch [ 677/1200] -> Loss: 37.4342
Epoch [ 678/1200] -> Loss: 37.3417
Epoch [ 679/1200] -> Loss: 37.5481
Epoch [ 680/1200] -> Loss: 37.4708
Epoch [ 681/1200] -> Loss: 37.4477
Epoch [ 682/1200] -> Loss: 37.4445
Epoch [ 683/1200] -> Loss: 37.3535
Epoch [ 684/1200] -> Loss: 37.5480
Epoch [ 685/1200] -> Loss: 37.4645
Epoch [ 686/1200] -> Loss: 37.6694
Epoch [ 687/1200] -> Loss: 37.5220
Epoch [ 688/1200] -> Loss: 37.5877
Epoch [ 689/1200] -> Loss: 37.2802
Epoch [ 690/1200] -> Loss: 37.3744
Epoch [ 691/1200] -> Loss: 37.4508
Epoch [ 692/1200] -> Loss: 37.7740
Epoch [ 693/1200] -> Loss: 37.2381
Epoch [ 694/1200] -> Loss: 37.6876
Epoch [ 695/1200] -> Loss: 37.4675
Epoch [ 696/1200] -> Loss: 37.5352
Epoch [ 697/1200] -> Loss: 37.6256
Epoch [ 698/1200] -> Loss: 37.4672
Epoch [ 699/1200] -> Loss: 37.5069
----------------------------------------------------------------
Model checkpoint saved as FFNN_700.pth
----------------------------------------------------------------
Epoch [ 700/1200] -> Loss: 37.5788
Epoch [ 701/1200] -> Loss: 37.4246
Epoch [ 702/1200] -> Loss: 37.6357
Epoch [ 703/1200] -> Loss: 37.4056
Epoch [ 704/1200] -> Loss: 37.3724
Epoch [ 705/1200] -> Loss: 37.5326
Epoch [ 706/1200] -> Loss: 37.4133
Epoch [ 707/1200] -> Loss: 37.4245
Epoch [ 708/1200] -> Loss: 37.6419
Epoch [ 709/1200] -> Loss: 37.2954
Epoch [ 710/1200] -> Loss: 37.5898
Epoch [ 711/1200] -> Loss: 37.3054
Epoch [ 712/1200] -> Loss: 37.6384
Epoch [ 713/1200] -> Loss: 37.2753
Epoch [ 714/1200] -> Loss: 37.4303
Epoch [ 715/1200] -> Loss: 37.5515
Epoch [ 716/1200] -> Loss: 37.5008
Epoch [ 717/1200] -> Loss: 37.4008
Epoch [ 718/1200] -> Loss: 37.4758
Epoch [ 719/1200] -> Loss: 37.5861
Epoch [ 720/1200] -> Loss: 37.4260
Epoch [ 721/1200] -> Loss: 37.5639
Epoch [ 722/1200] -> Loss: 37.4794
Epoch [ 723/1200] -> Loss: 37.6491
Epoch [ 724/1200] -> Loss: 37.6046
Epoch [ 725/1200] -> Loss: 37.3387
Epoch [ 726/1200] -> Loss: 37.4587
Epoch [ 727/1200] -> Loss: 37.4083
Epoch [ 728/1200] -> Loss: 37.5966
Epoch [ 729/1200] -> Loss: 37.4354
Epoch [ 730/1200] -> Loss: 37.5886
Epoch [ 731/1200] -> Loss: 37.6613
Epoch [ 732/1200] -> Loss: 37.3371
Epoch [ 733/1200] -> Loss: 37.2762
Epoch [ 734/1200] -> Loss: 37.4445
Epoch [ 735/1200] -> Loss: 37.4449
Epoch [ 736/1200] -> Loss: 37.3221
Epoch [ 737/1200] -> Loss: 37.4196
Epoch [ 738/1200] -> Loss: 37.6808
Epoch [ 739/1200] -> Loss: 37.4457
Epoch [ 740/1200] -> Loss: 37.3950
Epoch [ 741/1200] -> Loss: 37.4095
Epoch [ 742/1200] -> Loss: 37.4687
Epoch [ 743/1200] -> Loss: 37.1667
Epoch [ 744/1200] -> Loss: 37.3798
Epoch [ 745/1200] -> Loss: 37.5511
Epoch [ 746/1200] -> Loss: 37.3193
Epoch [ 747/1200] -> Loss: 37.5893
Epoch [ 748/1200] -> Loss: 37.6011
Epoch [ 749/1200] -> Loss: 37.3688
Epoch [ 750/1200] -> Loss: 37.5958
Epoch [ 751/1200] -> Loss: 37.5355
Epoch [ 752/1200] -> Loss: 37.3891
Epoch [ 753/1200] -> Loss: 37.3332
Epoch [ 754/1200] -> Loss: 37.5543
Epoch [ 755/1200] -> Loss: 37.4227
Epoch [ 756/1200] -> Loss: 37.2076
Epoch [ 757/1200] -> Loss: 37.5937
Epoch [ 758/1200] -> Loss: 37.2215
Epoch [ 759/1200] -> Loss: 37.3264
Epoch [ 760/1200] -> Loss: 37.6126
Epoch [ 761/1200] -> Loss: 37.6844
Epoch [ 762/1200] -> Loss: 37.4988
Epoch [ 763/1200] -> Loss: 37.5973
Epoch [ 764/1200] -> Loss: 37.4355
Epoch [ 765/1200] -> Loss: 37.4308
Epoch [ 766/1200] -> Loss: 37.3578
Epoch [ 767/1200] -> Loss: 37.4379
Epoch [ 768/1200] -> Loss: 37.4202
Epoch [ 769/1200] -> Loss: 37.3586
Epoch [ 770/1200] -> Loss: 37.3673
Epoch [ 771/1200] -> Loss: 37.5857
Epoch [ 772/1200] -> Loss: 37.3249
Epoch [ 773/1200] -> Loss: 37.5341
Epoch [ 774/1200] -> Loss: 37.4498
Epoch [ 775/1200] -> Loss: 37.5914
Epoch [ 776/1200] -> Loss: 37.4107
Epoch [ 777/1200] -> Loss: 37.3407
Epoch [ 778/1200] -> Loss: 37.3574
Epoch [ 779/1200] -> Loss: 37.4287
Epoch [ 780/1200] -> Loss: 37.5387
Epoch [ 781/1200] -> Loss: 37.5315
Epoch [ 782/1200] -> Loss: 37.3815
Epoch [ 783/1200] -> Loss: 37.6726
Epoch [ 784/1200] -> Loss: 37.6323
Epoch [ 785/1200] -> Loss: 37.5422
Epoch [ 786/1200] -> Loss: 37.6070
Epoch [ 787/1200] -> Loss: 37.3119
Epoch [ 788/1200] -> Loss: 37.5796
Epoch [ 789/1200] -> Loss: 37.1298
Epoch [ 790/1200] -> Loss: 37.6959
Epoch [ 791/1200] -> Loss: 37.4729
Epoch [ 792/1200] -> Loss: 37.5224
Epoch [ 793/1200] -> Loss: 37.4497
Epoch [ 794/1200] -> Loss: 37.5426
Epoch [ 795/1200] -> Loss: 37.5466
Epoch [ 796/1200] -> Loss: 37.4561
Epoch [ 797/1200] -> Loss: 37.5051
Epoch [ 798/1200] -> Loss: 37.5819
Epoch [ 799/1200] -> Loss: 37.5320
----------------------------------------------------------------
Model checkpoint saved as FFNN_800.pth
----------------------------------------------------------------
Epoch [ 800/1200] -> Loss: 37.3151
Epoch [ 801/1200] -> Loss: 37.6003
Epoch [ 802/1200] -> Loss: 37.3157
Epoch [ 803/1200] -> Loss: 37.5670
Epoch [ 804/1200] -> Loss: 37.4667
Epoch [ 805/1200] -> Loss: 37.5263
Epoch [ 806/1200] -> Loss: 37.6000
Epoch [ 807/1200] -> Loss: 37.5492
Epoch [ 808/1200] -> Loss: 37.4628
Epoch [ 809/1200] -> Loss: 37.4533
Epoch [ 810/1200] -> Loss: 37.2015
Epoch [ 811/1200] -> Loss: 37.5434
Epoch [ 812/1200] -> Loss: 37.4219
Epoch [ 813/1200] -> Loss: 37.6312
Epoch [ 814/1200] -> Loss: 37.6981
Epoch [ 815/1200] -> Loss: 37.5902
Epoch [ 816/1200] -> Loss: 37.4812
Epoch [ 817/1200] -> Loss: 37.6098
Epoch [ 818/1200] -> Loss: 37.3640
Epoch [ 819/1200] -> Loss: 37.2126
Epoch [ 820/1200] -> Loss: 37.5776
Epoch [ 821/1200] -> Loss: 37.5278
Epoch [ 822/1200] -> Loss: 37.7682
Epoch [ 823/1200] -> Loss: 37.5594
Epoch [ 824/1200] -> Loss: 37.6848
Epoch [ 825/1200] -> Loss: 37.6228
Epoch [ 826/1200] -> Loss: 37.5990
Epoch [ 827/1200] -> Loss: 37.6879
Epoch [ 828/1200] -> Loss: 37.3793
Epoch [ 829/1200] -> Loss: 37.2920
Epoch [ 830/1200] -> Loss: 37.5758
Epoch [ 831/1200] -> Loss: 37.0023
Epoch [ 832/1200] -> Loss: 37.5595
Epoch [ 833/1200] -> Loss: 37.5784
Epoch [ 834/1200] -> Loss: 37.6054
Epoch [ 835/1200] -> Loss: 37.5544
Epoch [ 836/1200] -> Loss: 37.5035
Epoch [ 837/1200] -> Loss: 37.4859
Epoch [ 838/1200] -> Loss: 37.4384
Epoch [ 839/1200] -> Loss: 37.6469
Epoch [ 840/1200] -> Loss: 37.3826
Epoch [ 841/1200] -> Loss: 37.5386
Epoch [ 842/1200] -> Loss: 37.4079
Epoch [ 843/1200] -> Loss: 37.0615
Epoch [ 844/1200] -> Loss: 37.6055
Epoch [ 845/1200] -> Loss: 37.3807
Epoch [ 846/1200] -> Loss: 37.3806
Epoch [ 847/1200] -> Loss: 37.3945
Epoch [ 848/1200] -> Loss: 37.5146
Epoch [ 849/1200] -> Loss: 37.5713
Epoch [ 850/1200] -> Loss: 37.3397
Epoch [ 851/1200] -> Loss: 37.4053
Epoch [ 852/1200] -> Loss: 37.5391
Epoch [ 853/1200] -> Loss: 37.4931
Epoch [ 854/1200] -> Loss: 37.5885
Epoch [ 855/1200] -> Loss: 37.4678
Epoch [ 856/1200] -> Loss: 37.1745
Epoch [ 857/1200] -> Loss: 37.5405
Epoch [ 858/1200] -> Loss: 37.5372
Epoch [ 859/1200] -> Loss: 37.3808
Epoch [ 860/1200] -> Loss: 37.4836
Epoch [ 861/1200] -> Loss: 37.5545
Epoch [ 862/1200] -> Loss: 37.4420
Epoch [ 863/1200] -> Loss: 37.6287
Epoch [ 864/1200] -> Loss: 37.5570
Epoch [ 865/1200] -> Loss: 37.5696
Epoch [ 866/1200] -> Loss: 37.7791
Epoch [ 867/1200] -> Loss: 37.3648
Epoch [ 868/1200] -> Loss: 37.5536
Epoch [ 869/1200] -> Loss: 37.5028
Epoch [ 870/1200] -> Loss: 37.4138
Epoch [ 871/1200] -> Loss: 37.6377
Epoch [ 872/1200] -> Loss: 37.4814
Epoch [ 873/1200] -> Loss: 37.5942
Epoch [ 874/1200] -> Loss: 37.2873
Epoch [ 875/1200] -> Loss: 37.6136
Epoch [ 876/1200] -> Loss: 37.3824
Epoch [ 877/1200] -> Loss: 37.7228
Epoch [ 878/1200] -> Loss: 37.6009
Epoch [ 879/1200] -> Loss: 37.5996
Epoch [ 880/1200] -> Loss: 37.6278
Epoch [ 881/1200] -> Loss: 37.6466
Epoch [ 882/1200] -> Loss: 37.5615
Epoch [ 883/1200] -> Loss: 37.4067
Epoch [ 884/1200] -> Loss: 37.6886
Epoch [ 885/1200] -> Loss: 37.5301
Epoch [ 886/1200] -> Loss: 37.7182
Epoch [ 887/1200] -> Loss: 37.4494
Epoch [ 888/1200] -> Loss: 37.4414
Epoch [ 889/1200] -> Loss: 37.4390
Epoch [ 890/1200] -> Loss: 37.4612
Epoch [ 891/1200] -> Loss: 37.5093
Epoch [ 892/1200] -> Loss: 37.2863
Epoch [ 893/1200] -> Loss: 37.6021
Epoch [ 894/1200] -> Loss: 37.6155
Epoch [ 895/1200] -> Loss: 37.3901
Epoch [ 896/1200] -> Loss: 37.4221
Epoch [ 897/1200] -> Loss: 37.4482
Epoch [ 898/1200] -> Loss: 37.5445
Epoch [ 899/1200] -> Loss: 37.5843
----------------------------------------------------------------
Model checkpoint saved as FFNN_900.pth
----------------------------------------------------------------
Epoch [ 900/1200] -> Loss: 37.3906
Epoch [ 901/1200] -> Loss: 37.4640
Epoch [ 902/1200] -> Loss: 37.5825
Epoch [ 903/1200] -> Loss: 37.4376
Epoch [ 904/1200] -> Loss: 37.5356
Epoch [ 905/1200] -> Loss: 37.5739
Epoch [ 906/1200] -> Loss: 37.4414
Epoch [ 907/1200] -> Loss: 37.3877
Epoch [ 908/1200] -> Loss: 37.5991
Epoch [ 909/1200] -> Loss: 37.5147
Epoch [ 910/1200] -> Loss: 37.2764
Epoch [ 911/1200] -> Loss: 37.5133
Epoch [ 912/1200] -> Loss: 37.7419
Epoch [ 913/1200] -> Loss: 37.4711
Epoch [ 914/1200] -> Loss: 37.4937
Epoch [ 915/1200] -> Loss: 37.6341
Epoch [ 916/1200] -> Loss: 37.4980
Epoch [ 917/1200] -> Loss: 37.6614
Epoch [ 918/1200] -> Loss: 37.4020
Epoch [ 919/1200] -> Loss: 37.5997
Epoch [ 920/1200] -> Loss: 37.5218
Epoch [ 921/1200] -> Loss: 37.2570
Epoch [ 922/1200] -> Loss: 37.4825
Epoch [ 923/1200] -> Loss: 37.6237
Epoch [ 924/1200] -> Loss: 37.4134
Epoch [ 925/1200] -> Loss: 37.3714
Epoch [ 926/1200] -> Loss: 37.7606
Epoch [ 927/1200] -> Loss: 37.5930
Epoch [ 928/1200] -> Loss: 37.6351
Epoch [ 929/1200] -> Loss: 37.4856
Epoch [ 930/1200] -> Loss: 37.7365
Epoch [ 931/1200] -> Loss: 37.3649
Epoch [ 932/1200] -> Loss: 37.4940
Epoch [ 933/1200] -> Loss: 37.6904
Epoch [ 934/1200] -> Loss: 37.5935
Epoch [ 935/1200] -> Loss: 37.4506
Epoch [ 936/1200] -> Loss: 37.2885
Epoch [ 937/1200] -> Loss: 37.5469
Epoch [ 938/1200] -> Loss: 37.4259
Epoch [ 939/1200] -> Loss: 37.5616
Epoch [ 940/1200] -> Loss: 37.5189
Epoch [ 941/1200] -> Loss: 37.6487
Epoch [ 942/1200] -> Loss: 37.4482
Epoch [ 943/1200] -> Loss: 37.6168
Epoch [ 944/1200] -> Loss: 37.6108
Epoch [ 945/1200] -> Loss: 37.3094
Epoch [ 946/1200] -> Loss: 37.4932
Epoch [ 947/1200] -> Loss: 37.3627
Epoch [ 948/1200] -> Loss: 37.4302
Epoch [ 949/1200] -> Loss: 37.7037
Epoch [ 950/1200] -> Loss: 37.6741
Epoch [ 951/1200] -> Loss: 37.4018
Epoch [ 952/1200] -> Loss: 37.5569
Epoch [ 953/1200] -> Loss: 37.5540
Epoch [ 954/1200] -> Loss: 37.4212
Epoch [ 955/1200] -> Loss: 37.5139
Epoch [ 956/1200] -> Loss: 37.4037
Epoch [ 957/1200] -> Loss: 37.5059
Epoch [ 958/1200] -> Loss: 37.5147
Epoch [ 959/1200] -> Loss: 37.4136
Epoch [ 960/1200] -> Loss: 37.3410
Epoch [ 961/1200] -> Loss: 37.4931
Epoch [ 962/1200] -> Loss: 37.4041
Epoch [ 963/1200] -> Loss: 37.4793
Epoch [ 964/1200] -> Loss: 37.5694
Epoch [ 965/1200] -> Loss: 37.5727
Epoch [ 966/1200] -> Loss: 37.6913
Epoch [ 967/1200] -> Loss: 37.4607
Epoch [ 968/1200] -> Loss: 37.5837
Epoch [ 969/1200] -> Loss: 37.4759
Epoch [ 970/1200] -> Loss: 37.0178
Epoch [ 971/1200] -> Loss: 37.4129
Epoch [ 972/1200] -> Loss: 37.6305
Epoch [ 973/1200] -> Loss: 37.4351
Epoch [ 974/1200] -> Loss: 37.5896
Epoch [ 975/1200] -> Loss: 37.4903
Epoch [ 976/1200] -> Loss: 37.3443
Epoch [ 977/1200] -> Loss: 37.5196
Epoch [ 978/1200] -> Loss: 37.5606
Epoch [ 979/1200] -> Loss: 37.3743
Epoch [ 980/1200] -> Loss: 37.4160
Epoch [ 981/1200] -> Loss: 37.3953
Epoch [ 982/1200] -> Loss: 37.2472
Epoch [ 983/1200] -> Loss: 37.5011
Epoch [ 984/1200] -> Loss: 37.7601
Epoch [ 985/1200] -> Loss: 37.5932
Epoch [ 986/1200] -> Loss: 37.6809
Epoch [ 987/1200] -> Loss: 37.4891
Epoch [ 988/1200] -> Loss: 37.8101
Epoch [ 989/1200] -> Loss: 37.4374
Epoch [ 990/1200] -> Loss: 37.4618
Epoch [ 991/1200] -> Loss: 37.4089
Epoch [ 992/1200] -> Loss: 37.3027
Epoch [ 993/1200] -> Loss: 37.6295
Epoch [ 994/1200] -> Loss: 37.5720
Epoch [ 995/1200] -> Loss: 37.5056
Epoch [ 996/1200] -> Loss: 37.4382
Epoch [ 997/1200] -> Loss: 37.5772
Epoch [ 998/1200] -> Loss: 37.3856
Epoch [ 999/1200] -> Loss: 37.4831
----------------------------------------------------------------
Model checkpoint saved as FFNN_1000.pth
----------------------------------------------------------------
Epoch [1000/1200] -> Loss: 37.6052
Epoch [1001/1200] -> Loss: 37.6935
Epoch [1002/1200] -> Loss: 37.6759
Epoch [1003/1200] -> Loss: 37.4638
Epoch [1004/1200] -> Loss: 37.7906
Epoch [1005/1200] -> Loss: 37.5613
Epoch [1006/1200] -> Loss: 37.3159
Epoch [1007/1200] -> Loss: 37.4259
Epoch [1008/1200] -> Loss: 37.5945
Epoch [1009/1200] -> Loss: 37.4987
Epoch [1010/1200] -> Loss: 37.3470
Epoch [1011/1200] -> Loss: 37.6505
Epoch [1012/1200] -> Loss: 37.4629
Epoch [1013/1200] -> Loss: 37.6503
Epoch [1014/1200] -> Loss: 37.5379
Epoch [1015/1200] -> Loss: 37.6375
Epoch [1016/1200] -> Loss: 37.4937
Epoch [1017/1200] -> Loss: 37.4762
Epoch [1018/1200] -> Loss: 37.6075
Epoch [1019/1200] -> Loss: 37.6051
Epoch [1020/1200] -> Loss: 37.3708
Epoch [1021/1200] -> Loss: 37.3840
Epoch [1022/1200] -> Loss: 37.3550
Epoch [1023/1200] -> Loss: 37.4751
Epoch [1024/1200] -> Loss: 37.5485
Epoch [1025/1200] -> Loss: 37.5081
Epoch [1026/1200] -> Loss: 37.4001
Epoch [1027/1200] -> Loss: 37.3377
Epoch [1028/1200] -> Loss: 37.6131
Epoch [1029/1200] -> Loss: 37.5866
Epoch [1030/1200] -> Loss: 37.4412
Epoch [1031/1200] -> Loss: 37.3386
Epoch [1032/1200] -> Loss: 37.3492
Epoch [1033/1200] -> Loss: 37.5147
Epoch [1034/1200] -> Loss: 37.6026
Epoch [1035/1200] -> Loss: 37.6935
Epoch [1036/1200] -> Loss: 37.4753
Epoch [1037/1200] -> Loss: 37.6455
Epoch [1038/1200] -> Loss: 37.7402
Epoch [1039/1200] -> Loss: 37.3508
Epoch [1040/1200] -> Loss: 37.5782
Epoch [1041/1200] -> Loss: 37.6583
Epoch [1042/1200] -> Loss: 37.9087
Epoch [1043/1200] -> Loss: 37.7674
Epoch [1044/1200] -> Loss: 37.3544
Epoch [1045/1200] -> Loss: 37.2977
Epoch [1046/1200] -> Loss: 37.5867
Epoch [1047/1200] -> Loss: 37.4415
Epoch [1048/1200] -> Loss: 37.5500
Epoch [1049/1200] -> Loss: 37.3976
Epoch [1050/1200] -> Loss: 37.5060
Epoch [1051/1200] -> Loss: 37.3968
Epoch [1052/1200] -> Loss: 37.2680
Epoch [1053/1200] -> Loss: 37.1726
Epoch [1054/1200] -> Loss: 37.7012
Epoch [1055/1200] -> Loss: 37.5507
Epoch [1056/1200] -> Loss: 37.5962
Epoch [1057/1200] -> Loss: 37.3628
Epoch [1058/1200] -> Loss: 37.5241
Epoch [1059/1200] -> Loss: 37.6267
Epoch [1060/1200] -> Loss: 37.6223
Epoch [1061/1200] -> Loss: 37.5340
Epoch [1062/1200] -> Loss: 37.3559
Epoch [1063/1200] -> Loss: 37.6190
Epoch [1064/1200] -> Loss: 37.3281
Epoch [1065/1200] -> Loss: 37.6072
Epoch [1066/1200] -> Loss: 37.3989
Epoch [1067/1200] -> Loss: 37.4375
Epoch [1068/1200] -> Loss: 37.4241
Epoch [1069/1200] -> Loss: 37.4415
Epoch [1070/1200] -> Loss: 37.5762
Epoch [1071/1200] -> Loss: 37.4750
Epoch [1072/1200] -> Loss: 37.3466
Epoch [1073/1200] -> Loss: 37.5418
Epoch [1074/1200] -> Loss: 37.6066
Epoch [1075/1200] -> Loss: 37.4600
Epoch [1076/1200] -> Loss: 37.6191
Epoch [1077/1200] -> Loss: 37.4948
Epoch [1078/1200] -> Loss: 37.5155
Epoch [1079/1200] -> Loss: 37.3221
Epoch [1080/1200] -> Loss: 37.5731
Epoch [1081/1200] -> Loss: 37.5018
Epoch [1082/1200] -> Loss: 37.4737
Epoch [1083/1200] -> Loss: 37.5117
Epoch [1084/1200] -> Loss: 37.3283
Epoch [1085/1200] -> Loss: 37.3432
Epoch [1086/1200] -> Loss: 37.5972
Epoch [1087/1200] -> Loss: 37.5457
Epoch [1088/1200] -> Loss: 37.5791
Epoch [1089/1200] -> Loss: 37.3372
Epoch [1090/1200] -> Loss: 37.6632
Epoch [1091/1200] -> Loss: 37.6411
Epoch [1092/1200] -> Loss: 37.4995
Epoch [1093/1200] -> Loss: 37.5744
Epoch [1094/1200] -> Loss: 37.2702
Epoch [1095/1200] -> Loss: 37.4916
Epoch [1096/1200] -> Loss: 37.5048
Epoch [1097/1200] -> Loss: 37.4477
Epoch [1098/1200] -> Loss: 37.4531
Epoch [1099/1200] -> Loss: 37.6530
----------------------------------------------------------------
Model checkpoint saved as FFNN_1100.pth
----------------------------------------------------------------
Epoch [1100/1200] -> Loss: 37.3943
Epoch [1101/1200] -> Loss: 37.4433
Epoch [1102/1200] -> Loss: 37.5356
Epoch [1103/1200] -> Loss: 37.7362
Epoch [1104/1200] -> Loss: 37.5972
Epoch [1105/1200] -> Loss: 37.3591
Epoch [1106/1200] -> Loss: 37.4902
Epoch [1107/1200] -> Loss: 37.5240
Epoch [1108/1200] -> Loss: 37.5354
Epoch [1109/1200] -> Loss: 37.5175
Epoch [1110/1200] -> Loss: 37.5386
Epoch [1111/1200] -> Loss: 37.5046
Epoch [1112/1200] -> Loss: 37.4628
Epoch [1113/1200] -> Loss: 37.3625
Epoch [1114/1200] -> Loss: 37.5074
Epoch [1115/1200] -> Loss: 37.5695
Epoch [1116/1200] -> Loss: 37.7451
Epoch [1117/1200] -> Loss: 37.5755
Epoch [1118/1200] -> Loss: 37.5743
Epoch [1119/1200] -> Loss: 37.5173
Epoch [1120/1200] -> Loss: 37.5550
Epoch [1121/1200] -> Loss: 37.4396
Epoch [1122/1200] -> Loss: 37.3424
Epoch [1123/1200] -> Loss: 37.6230
Epoch [1124/1200] -> Loss: 37.5348
Epoch [1125/1200] -> Loss: 37.6041
Epoch [1126/1200] -> Loss: 37.4373
Epoch [1127/1200] -> Loss: 37.5262
Epoch [1128/1200] -> Loss: 37.6223
Epoch [1129/1200] -> Loss: 37.4349
Epoch [1130/1200] -> Loss: 37.3799
Epoch [1131/1200] -> Loss: 37.4297
Epoch [1132/1200] -> Loss: 37.5821
Epoch [1133/1200] -> Loss: 37.3966
Epoch [1134/1200] -> Loss: 37.4038
Epoch [1135/1200] -> Loss: 37.4843
Epoch [1136/1200] -> Loss: 37.3791
Epoch [1137/1200] -> Loss: 37.7273
Epoch [1138/1200] -> Loss: 37.5470
Epoch [1139/1200] -> Loss: 37.1055
Epoch [1140/1200] -> Loss: 37.5075
Epoch [1141/1200] -> Loss: 37.3218
Epoch [1142/1200] -> Loss: 37.4131
Epoch [1143/1200] -> Loss: 37.5803
Epoch [1144/1200] -> Loss: 37.6681
Epoch [1145/1200] -> Loss: 37.3272
Epoch [1146/1200] -> Loss: 37.2293
Epoch [1147/1200] -> Loss: 37.7559
Epoch [1148/1200] -> Loss: 37.3658
Epoch [1149/1200] -> Loss: 37.3357
Epoch [1150/1200] -> Loss: 37.5121
Epoch [1151/1200] -> Loss: 37.2597
Epoch [1152/1200] -> Loss: 37.2841
Epoch [1153/1200] -> Loss: 37.4096
Epoch [1154/1200] -> Loss: 37.5548
Epoch [1155/1200] -> Loss: 37.5376
Epoch [1156/1200] -> Loss: 37.4273
Epoch [1157/1200] -> Loss: 37.6204
Epoch [1158/1200] -> Loss: 37.4621
Epoch [1159/1200] -> Loss: 37.5403
Epoch [1160/1200] -> Loss: 37.6291
Epoch [1161/1200] -> Loss: 37.4234
Epoch [1162/1200] -> Loss: 37.4381
Epoch [1163/1200] -> Loss: 37.5198
Epoch [1164/1200] -> Loss: 37.7695
Epoch [1165/1200] -> Loss: 37.5997
Epoch [1166/1200] -> Loss: 37.6484
Epoch [1167/1200] -> Loss: 37.6558
Epoch [1168/1200] -> Loss: 37.4472
Epoch [1169/1200] -> Loss: 37.4778
Epoch [1170/1200] -> Loss: 37.5199
Epoch [1171/1200] -> Loss: 37.4751
Epoch [1172/1200] -> Loss: 37.3996
Epoch [1173/1200] -> Loss: 37.6478
Epoch [1174/1200] -> Loss: 37.4214
Epoch [1175/1200] -> Loss: 37.6058
Epoch [1176/1200] -> Loss: 37.4405
Epoch [1177/1200] -> Loss: 37.4247
Epoch [1178/1200] -> Loss: 37.1831
Epoch [1179/1200] -> Loss: 37.7891
Epoch [1180/1200] -> Loss: 37.2421
Epoch [1181/1200] -> Loss: 37.5034
Epoch [1182/1200] -> Loss: 37.3270
Epoch [1183/1200] -> Loss: 37.5586
Epoch [1184/1200] -> Loss: 37.4437
Epoch [1185/1200] -> Loss: 37.2175
Epoch [1186/1200] -> Loss: 37.5784
Epoch [1187/1200] -> Loss: 37.6499
Epoch [1188/1200] -> Loss: 37.5800
Epoch [1189/1200] -> Loss: 37.5013
Epoch [1190/1200] -> Loss: 37.3409
Epoch [1191/1200] -> Loss: 37.4747
Epoch [1192/1200] -> Loss: 37.4360
Epoch [1193/1200] -> Loss: 37.3872
Epoch [1194/1200] -> Loss: 37.4170
Epoch [1195/1200] -> Loss: 37.5828
Epoch [1196/1200] -> Loss: 37.6337
Epoch [1197/1200] -> Loss: 37.5952
Epoch [1198/1200] -> Loss: 37.6380
Epoch [1199/1200] -> Loss: 37.2834
----------------------------------------------------------------
Model checkpoint saved as FFNN_1200.pth
----------------------------------------------------------------
Epoch [1200/1200] -> Loss: 37.7580
----------------------------------------------------------------
Training finished successfully.
        Saved model checkpoints can be found in: /home/extern/Documents/Research/scripts/models/
        Saved data/loss graphs can be found in: /home/extern/Documents/Research/scripts/graphs/
----------------------------------------------------------------
Plotting data...
