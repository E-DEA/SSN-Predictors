--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
File location :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): ReLU(inplace=True)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
No pre-trained models available, initializing model weights
--------------------------------------------------
Training model with: num_epochs=2600, start_lr=5e-06
Epoch [   1/2600] -> Loss: 12263.7632
Epoch [   2/2600] -> Loss: 7076.4546
Epoch [   3/2600] -> Loss: 3342.9052
Epoch [   4/2600] -> Loss: 3242.9202
Epoch [   5/2600] -> Loss: 3205.8441
Epoch [   6/2600] -> Loss: 3223.2153
Epoch [   7/2600] -> Loss: 3212.0516
Epoch [   8/2600] -> Loss: 3217.6345
Epoch [   9/2600] -> Loss: 3202.7818
Epoch [  10/2600] -> Loss: 3248.6277
Epoch [  11/2600] -> Loss: 3207.7411
Epoch [  12/2600] -> Loss: 3203.4039
Epoch [  13/2600] -> Loss: 3224.8379
Epoch [  14/2600] -> Loss: 3200.9572
Epoch [  15/2600] -> Loss: 3206.8327
Epoch [  16/2600] -> Loss: 3186.5214
Epoch [  17/2600] -> Loss: 3170.6930
Epoch [  18/2600] -> Loss: 3174.2087
Epoch [  19/2600] -> Loss: 3170.7062
Epoch [  20/2600] -> Loss: 3187.4981
Epoch [  21/2600] -> Loss: 3177.4998
Epoch [  22/2600] -> Loss: 3181.8912
Epoch [  23/2600] -> Loss: 3178.2854
Epoch [  24/2600] -> Loss: 3160.0394
Epoch [  25/2600] -> Loss: 3161.6677
Epoch [  26/2600] -> Loss: 3166.3826
Epoch [  27/2600] -> Loss: 3149.8122
Epoch [  28/2600] -> Loss: 3160.4830
Epoch [  29/2600] -> Loss: 3150.9083
Epoch [  30/2600] -> Loss: 3145.7459
Epoch [  31/2600] -> Loss: 3131.9621
Epoch [  32/2600] -> Loss: 3154.6377
Epoch [  33/2600] -> Loss: 3142.5459
Epoch [  34/2600] -> Loss: 3158.8148
Epoch [  35/2600] -> Loss: 3173.5008
Epoch [  36/2600] -> Loss: 3130.3253
Epoch [  37/2600] -> Loss: 3140.9339
Epoch [  38/2600] -> Loss: 3143.7717
Epoch [  39/2600] -> Loss: 3139.4600
Epoch [  40/2600] -> Loss: 3164.5368
Epoch [  41/2600] -> Loss: 3129.0315
Epoch [  42/2600] -> Loss: 3145.5837
Epoch [  43/2600] -> Loss: 3126.9267
Epoch [  44/2600] -> Loss: 3137.6342
Epoch [  45/2600] -> Loss: 3116.3952
Epoch [  46/2600] -> Loss: 3117.6058
Epoch [  47/2600] -> Loss: 3117.3798
Epoch [  48/2600] -> Loss: 3130.5252
Epoch [  49/2600] -> Loss: 3132.8238
Epoch [  50/2600] -> Loss: 3131.9557
Epoch [  51/2600] -> Loss: 3137.7054
Epoch [  52/2600] -> Loss: 3130.9195
Epoch [  53/2600] -> Loss: 3136.4606
Epoch [  54/2600] -> Loss: 3114.2773
Epoch [  55/2600] -> Loss: 3127.2577
Epoch [  56/2600] -> Loss: 3137.9577
Epoch [  57/2600] -> Loss: 3130.5483
Epoch [  58/2600] -> Loss: 3124.0604
Epoch [  59/2600] -> Loss: 3121.9964
Epoch [  60/2600] -> Loss: 3121.1500
Epoch [  61/2600] -> Loss: 3117.4782
Epoch [  62/2600] -> Loss: 3124.2616
Epoch [  63/2600] -> Loss: 3134.6475
Epoch [  64/2600] -> Loss: 3107.9501
Epoch [  65/2600] -> Loss: 3127.6906
Epoch [  66/2600] -> Loss: 3125.8144
Epoch [  67/2600] -> Loss: 3126.3080
Epoch [  68/2600] -> Loss: 3122.6798
Epoch [  69/2600] -> Loss: 3131.0829
Epoch [  70/2600] -> Loss: 3114.9694
Epoch [  71/2600] -> Loss: 3143.5950
Epoch [  72/2600] -> Loss: 3134.1505
Epoch [  73/2600] -> Loss: 3116.4673
Epoch [  74/2600] -> Loss: 3089.9692
Epoch [  75/2600] -> Loss: 3127.2950
Epoch [  76/2600] -> Loss: 3119.8036
Epoch [  77/2600] -> Loss: 3130.9944
Epoch [  78/2600] -> Loss: 3103.1230
Epoch [  79/2600] -> Loss: 3095.5192
Epoch [  80/2600] -> Loss: 3109.8790
Epoch [  81/2600] -> Loss: 3104.9510
Epoch [  82/2600] -> Loss: 3106.2634
Epoch [  83/2600] -> Loss: 3120.4367
Epoch [  84/2600] -> Loss: 3096.0278
Epoch    85: reducing learning rate of group 0 to 2.5000e-06.
Epoch [  85/2600] -> Loss: 3115.7727
Epoch [  86/2600] -> Loss: 3092.1434
Epoch [  87/2600] -> Loss: 3111.7990
Epoch [  88/2600] -> Loss: 3123.9622
Epoch [  89/2600] -> Loss: 3106.6465
Epoch [  90/2600] -> Loss: 3096.5530
Epoch [  91/2600] -> Loss: 3110.4234
Epoch [  92/2600] -> Loss: 3088.3418
Epoch [  93/2600] -> Loss: 3098.0849
Epoch [  94/2600] -> Loss: 3117.9491
Epoch [  95/2600] -> Loss: 3115.4502
Epoch [  96/2600] -> Loss: 3097.7513
Epoch [  97/2600] -> Loss: 3103.9822
Epoch [  98/2600] -> Loss: 3100.1067
Epoch [  99/2600] -> Loss: 3112.4499
Epoch [ 100/2600] -> Loss: 3093.3591
Epoch [ 101/2600] -> Loss: 3091.4140
Epoch [ 102/2600] -> Loss: 3086.1397
Epoch [ 103/2600] -> Loss: 3093.2256
Epoch [ 104/2600] -> Loss: 3110.6861
Epoch [ 105/2600] -> Loss: 3103.6198
Epoch [ 106/2600] -> Loss: 3099.0200
Epoch [ 107/2600] -> Loss: 3121.9395
Epoch [ 108/2600] -> Loss: 3103.5383
Epoch [ 109/2600] -> Loss: 3107.9906
Epoch [ 110/2600] -> Loss: 3102.7515
Epoch [ 111/2600] -> Loss: 3117.0173
Epoch [ 112/2600] -> Loss: 3105.4816
Epoch   113: reducing learning rate of group 0 to 1.2500e-06.
Epoch [ 113/2600] -> Loss: 3106.3834
Epoch [ 114/2600] -> Loss: 3089.4515
Epoch [ 115/2600] -> Loss: 3086.7024
Epoch [ 116/2600] -> Loss: 3104.2394
Epoch [ 117/2600] -> Loss: 3095.9751
Epoch [ 118/2600] -> Loss: 3080.0812
Epoch [ 119/2600] -> Loss: 3099.2146
Epoch [ 120/2600] -> Loss: 3096.5429
Epoch [ 121/2600] -> Loss: 3091.1796
Epoch [ 122/2600] -> Loss: 3118.0939
Epoch [ 123/2600] -> Loss: 3102.0412
Epoch [ 124/2600] -> Loss: 3121.7307
Epoch [ 125/2600] -> Loss: 3118.7750
Epoch [ 126/2600] -> Loss: 3096.7493
Epoch [ 127/2600] -> Loss: 3096.9097
Epoch [ 128/2600] -> Loss: 3097.9448
Epoch   129: reducing learning rate of group 0 to 6.2500e-07.
Epoch [ 129/2600] -> Loss: 3108.2803
Epoch [ 130/2600] -> Loss: 3101.9916
Epoch [ 131/2600] -> Loss: 3089.5057
Epoch [ 132/2600] -> Loss: 3092.8078
Epoch [ 133/2600] -> Loss: 3082.6086
Epoch [ 134/2600] -> Loss: 3094.0642
Epoch [ 135/2600] -> Loss: 3084.7588
Epoch [ 136/2600] -> Loss: 3104.9142
Epoch [ 137/2600] -> Loss: 3080.6560
Epoch [ 138/2600] -> Loss: 3097.5400
Epoch [ 139/2600] -> Loss: 3101.9949
Epoch   140: reducing learning rate of group 0 to 3.1250e-07.
Epoch [ 140/2600] -> Loss: 3104.6271
Epoch [ 141/2600] -> Loss: 3088.1182
Epoch [ 142/2600] -> Loss: 3100.1551
Epoch [ 143/2600] -> Loss: 3098.9452
Epoch [ 144/2600] -> Loss: 3098.7480
Epoch [ 145/2600] -> Loss: 3095.6300
Epoch [ 146/2600] -> Loss: 3098.4617
Epoch [ 147/2600] -> Loss: 3105.7537
Epoch [ 148/2600] -> Loss: 3093.1867
Epoch [ 149/2600] -> Loss: 3096.2088
Epoch [ 150/2600] -> Loss: 3087.9645
Epoch   151: reducing learning rate of group 0 to 1.5625e-07.
Epoch [ 151/2600] -> Loss: 3090.3726
Epoch [ 152/2600] -> Loss: 3091.6463
Epoch [ 153/2600] -> Loss: 3083.7253
Epoch [ 154/2600] -> Loss: 3111.0262
Epoch [ 155/2600] -> Loss: 3092.3626
Epoch [ 156/2600] -> Loss: 3095.2850
Epoch [ 157/2600] -> Loss: 3099.9993
Epoch [ 158/2600] -> Loss: 3083.2254
Epoch [ 159/2600] -> Loss: 3089.4585
Epoch [ 160/2600] -> Loss: 3086.7828
Epoch [ 161/2600] -> Loss: 3078.4933
Epoch [ 162/2600] -> Loss: 3102.0109
Epoch [ 163/2600] -> Loss: 3074.4431
Epoch [ 164/2600] -> Loss: 3086.3255
Epoch [ 165/2600] -> Loss: 3111.6267
Epoch [ 166/2600] -> Loss: 3111.1585
Epoch [ 167/2600] -> Loss: 3092.3108
Epoch [ 168/2600] -> Loss: 3077.6878
Epoch [ 169/2600] -> Loss: 3083.0443
Epoch [ 170/2600] -> Loss: 3090.5866
Epoch [ 171/2600] -> Loss: 3106.0856
Epoch [ 172/2600] -> Loss: 3092.6757
Epoch [ 173/2600] -> Loss: 3079.1635
Epoch   174: reducing learning rate of group 0 to 7.8125e-08.
Epoch [ 174/2600] -> Loss: 3091.8276
Epoch [ 175/2600] -> Loss: 3095.6702
Epoch [ 176/2600] -> Loss: 3091.4802
Epoch [ 177/2600] -> Loss: 3091.9124
Epoch [ 178/2600] -> Loss: 3101.4359
Epoch [ 179/2600] -> Loss: 3091.3952
Epoch [ 180/2600] -> Loss: 3109.3462
Epoch [ 181/2600] -> Loss: 3084.1728
Epoch [ 182/2600] -> Loss: 3090.8148
Epoch [ 183/2600] -> Loss: 3101.6088
Epoch [ 184/2600] -> Loss: 3093.7701
Epoch   185: reducing learning rate of group 0 to 3.9063e-08.
Epoch [ 185/2600] -> Loss: 3081.7484
Epoch [ 186/2600] -> Loss: 3120.5290
Epoch [ 187/2600] -> Loss: 3100.0009
Epoch [ 188/2600] -> Loss: 3092.2389
Epoch [ 189/2600] -> Loss: 3104.5419
Epoch [ 190/2600] -> Loss: 3112.7222
Epoch [ 191/2600] -> Loss: 3091.3019
Epoch [ 192/2600] -> Loss: 3097.4015
Epoch [ 193/2600] -> Loss: 3085.1436
Epoch [ 194/2600] -> Loss: 3106.7994
Epoch [ 195/2600] -> Loss: 3097.3097
Epoch   196: reducing learning rate of group 0 to 1.9531e-08.
Epoch [ 196/2600] -> Loss: 3098.0057
Epoch [ 197/2600] -> Loss: 3102.1751
Epoch [ 198/2600] -> Loss: 3082.8510
Epoch [ 199/2600] -> Loss: 3083.6626
--------------------------------------------------
Model checkpoint saved as FFNN_200.pth
--------------------------------------------------
Epoch [ 200/2600] -> Loss: 3114.7747
Epoch [ 201/2600] -> Loss: 3105.9943
Epoch [ 202/2600] -> Loss: 3091.8812
Epoch [ 203/2600] -> Loss: 3073.7953
Epoch [ 204/2600] -> Loss: 3084.1660
Epoch [ 205/2600] -> Loss: 3100.5305
Epoch [ 206/2600] -> Loss: 3074.0386
Epoch [ 207/2600] -> Loss: 3090.7798
Epoch [ 208/2600] -> Loss: 3082.6911
Epoch [ 209/2600] -> Loss: 3091.6003
Epoch [ 210/2600] -> Loss: 3096.2529
Epoch [ 211/2600] -> Loss: 3095.9281
Epoch [ 212/2600] -> Loss: 3086.5326
Epoch [ 213/2600] -> Loss: 3095.0623
Epoch [ 214/2600] -> Loss: 3092.7966
Epoch [ 215/2600] -> Loss: 3091.4158
Epoch [ 216/2600] -> Loss: 3085.2543
Epoch [ 217/2600] -> Loss: 3085.4796
Epoch [ 218/2600] -> Loss: 3098.0149
Epoch [ 219/2600] -> Loss: 3098.4354
Epoch [ 220/2600] -> Loss: 3096.3572
Epoch [ 221/2600] -> Loss: 3084.3697
Epoch [ 222/2600] -> Loss: 3079.1913
Epoch [ 223/2600] -> Loss: 3084.5590
Epoch [ 224/2600] -> Loss: 3089.7818
Epoch [ 225/2600] -> Loss: 3084.2116
Epoch [ 226/2600] -> Loss: 3128.9487
Epoch [ 227/2600] -> Loss: 3106.9988
Epoch [ 228/2600] -> Loss: 3101.5753
Epoch [ 229/2600] -> Loss: 3096.0394
Epoch [ 230/2600] -> Loss: 3128.9761
Epoch [ 231/2600] -> Loss: 3082.3055
Epoch [ 232/2600] -> Loss: 3077.1760
Epoch [ 233/2600] -> Loss: 3106.8470
Epoch [ 234/2600] -> Loss: 3099.4280
Epoch [ 235/2600] -> Loss: 3103.0332
Epoch [ 236/2600] -> Loss: 3072.2162
Epoch [ 237/2600] -> Loss: 3091.9246
Epoch [ 238/2600] -> Loss: 3088.1250
Epoch [ 239/2600] -> Loss: 3081.0578
Epoch [ 240/2600] -> Loss: 3076.1186
Epoch [ 241/2600] -> Loss: 3088.8376
Epoch [ 242/2600] -> Loss: 3090.1443
Epoch [ 243/2600] -> Loss: 3081.5745
Epoch [ 244/2600] -> Loss: 3087.8618
Epoch [ 245/2600] -> Loss: 3115.1814
Epoch [ 246/2600] -> Loss: 3089.6395
Epoch [ 247/2600] -> Loss: 3097.1117
Epoch [ 248/2600] -> Loss: 3104.7274
Epoch [ 249/2600] -> Loss: 3095.9217
Epoch [ 250/2600] -> Loss: 3078.5519
Epoch [ 251/2600] -> Loss: 3097.0685
Epoch [ 252/2600] -> Loss: 3090.6487
Epoch [ 253/2600] -> Loss: 3092.9253
Epoch [ 254/2600] -> Loss: 3098.3303
Epoch [ 255/2600] -> Loss: 3091.4680
Epoch [ 256/2600] -> Loss: 3112.1220
Epoch [ 257/2600] -> Loss: 3097.6007
Epoch [ 258/2600] -> Loss: 3094.2769
Epoch [ 259/2600] -> Loss: 3105.7384
Epoch [ 260/2600] -> Loss: 3083.5741
Epoch [ 261/2600] -> Loss: 3094.4256
Epoch [ 262/2600] -> Loss: 3078.8458
Epoch [ 263/2600] -> Loss: 3096.4785
Epoch [ 264/2600] -> Loss: 3097.4002
Epoch [ 265/2600] -> Loss: 3094.3362
Epoch [ 266/2600] -> Loss: 3101.7150
Epoch [ 267/2600] -> Loss: 3101.0797
Epoch [ 268/2600] -> Loss: 3096.0894
Epoch [ 269/2600] -> Loss: 3106.1787
Epoch [ 270/2600] -> Loss: 3104.4798
Epoch [ 271/2600] -> Loss: 3090.3376
Epoch [ 272/2600] -> Loss: 3092.7965
Epoch [ 273/2600] -> Loss: 3098.0601
Epoch [ 274/2600] -> Loss: 3090.7804
Epoch [ 275/2600] -> Loss: 3086.0536
Epoch [ 276/2600] -> Loss: 3093.1757
Epoch [ 277/2600] -> Loss: 3099.1965
Epoch [ 278/2600] -> Loss: 3073.3205
Epoch [ 279/2600] -> Loss: 3095.3183
Epoch [ 280/2600] -> Loss: 3085.8739
Epoch [ 281/2600] -> Loss: 3090.5202
Epoch [ 282/2600] -> Loss: 3102.7405
Epoch [ 283/2600] -> Loss: 3087.2500
Epoch [ 284/2600] -> Loss: 3118.0979
Epoch [ 285/2600] -> Loss: 3086.3392
Epoch [ 286/2600] -> Loss: 3100.5349
Epoch [ 287/2600] -> Loss: 3088.0305
Epoch [ 288/2600] -> Loss: 3110.2435
Epoch [ 289/2600] -> Loss: 3093.9835
Epoch [ 290/2600] -> Loss: 3076.1304
Epoch [ 291/2600] -> Loss: 3100.8408
Epoch [ 292/2600] -> Loss: 3099.9406
Epoch [ 293/2600] -> Loss: 3104.0576
Epoch [ 294/2600] -> Loss: 3108.9556
Epoch [ 295/2600] -> Loss: 3089.0555
Epoch [ 296/2600] -> Loss: 3089.6875
Epoch [ 297/2600] -> Loss: 3099.4098
Epoch [ 298/2600] -> Loss: 3078.8480
Epoch [ 299/2600] -> Loss: 3091.3130
Epoch [ 300/2600] -> Loss: 3087.2108
Epoch [ 301/2600] -> Loss: 3090.4653
Epoch [ 302/2600] -> Loss: 3094.8754
Epoch [ 303/2600] -> Loss: 3097.3636
Epoch [ 304/2600] -> Loss: 3099.0946
Epoch [ 305/2600] -> Loss: 3115.9387
Epoch [ 306/2600] -> Loss: 3084.6756
Epoch [ 307/2600] -> Loss: 3089.8853
Epoch [ 308/2600] -> Loss: 3104.8119
Epoch [ 309/2600] -> Loss: 3116.6210
Epoch [ 310/2600] -> Loss: 3091.4354
Epoch [ 311/2600] -> Loss: 3092.6979
Epoch [ 312/2600] -> Loss: 3081.8809
Epoch [ 313/2600] -> Loss: 3100.7138
Epoch [ 314/2600] -> Loss: 3095.6218
Epoch [ 315/2600] -> Loss: 3090.9715
Epoch [ 316/2600] -> Loss: 3096.5804
Epoch [ 317/2600] -> Loss: 3092.7799
Epoch [ 318/2600] -> Loss: 3095.6180
Epoch [ 319/2600] -> Loss: 3088.2926
Epoch [ 320/2600] -> Loss: 3115.9095
Epoch [ 321/2600] -> Loss: 3092.5860
Epoch [ 322/2600] -> Loss: 3091.7050
Epoch [ 323/2600] -> Loss: 3079.9161
Epoch [ 324/2600] -> Loss: 3112.2679
Epoch [ 325/2600] -> Loss: 3071.4894
Epoch [ 326/2600] -> Loss: 3110.6011
Epoch [ 327/2600] -> Loss: 3098.4871
Epoch [ 328/2600] -> Loss: 3098.9467
Epoch [ 329/2600] -> Loss: 3078.5206
Epoch [ 330/2600] -> Loss: 3087.3632
Epoch [ 331/2600] -> Loss: 3093.1973
Epoch [ 332/2600] -> Loss: 3091.2993
Epoch [ 333/2600] -> Loss: 3098.3260
Epoch [ 334/2600] -> Loss: 3097.4492
Epoch [ 335/2600] -> Loss: 3114.0095
Epoch [ 336/2600] -> Loss: 3089.3199
Epoch [ 337/2600] -> Loss: 3097.9895
Epoch [ 338/2600] -> Loss: 3086.3757
Epoch [ 339/2600] -> Loss: 3106.4330
Epoch [ 340/2600] -> Loss: 3089.4327
Epoch [ 341/2600] -> Loss: 3085.0621
Epoch [ 342/2600] -> Loss: 3111.7015
Epoch [ 343/2600] -> Loss: 3095.1787
Epoch [ 344/2600] -> Loss: 3091.9941
Epoch [ 345/2600] -> Loss: 3084.6758
Epoch [ 346/2600] -> Loss: 3099.2557
Epoch [ 347/2600] -> Loss: 3102.9259
Epoch [ 348/2600] -> Loss: 3089.7747
Epoch [ 349/2600] -> Loss: 3102.0411
Epoch [ 350/2600] -> Loss: 3093.3340
Epoch [ 351/2600] -> Loss: 3090.6870
Epoch [ 352/2600] -> Loss: 3084.7408
Epoch [ 353/2600] -> Loss: 3093.4202
Epoch [ 354/2600] -> Loss: 3109.7644
Epoch [ 355/2600] -> Loss: 3094.7740
Epoch [ 356/2600] -> Loss: 3096.8005
Epoch [ 357/2600] -> Loss: 3095.0384
Epoch [ 358/2600] -> Loss: 3104.5699
Epoch [ 359/2600] -> Loss: 3085.8459
Epoch [ 360/2600] -> Loss: 3095.1365
Epoch [ 361/2600] -> Loss: 3090.4021
Epoch [ 362/2600] -> Loss: 3092.2010
Epoch [ 363/2600] -> Loss: 3106.8013
Epoch [ 364/2600] -> Loss: 3101.3468
Epoch [ 365/2600] -> Loss: 3072.9294
Epoch [ 366/2600] -> Loss: 3078.8378
Epoch [ 367/2600] -> Loss: 3088.6490
Epoch [ 368/2600] -> Loss: 3094.3072
Epoch [ 369/2600] -> Loss: 3085.3677
Epoch [ 370/2600] -> Loss: 3098.4207
Epoch [ 371/2600] -> Loss: 3094.6834
Epoch [ 372/2600] -> Loss: 3102.0253
Epoch [ 373/2600] -> Loss: 3109.0440
Epoch [ 374/2600] -> Loss: 3091.5622
Epoch [ 375/2600] -> Loss: 3092.6252
Epoch [ 376/2600] -> Loss: 3111.5880
Epoch [ 377/2600] -> Loss: 3084.5109
Epoch [ 378/2600] -> Loss: 3107.3789
Epoch [ 379/2600] -> Loss: 3101.7469
Epoch [ 380/2600] -> Loss: 3090.5089
Epoch [ 381/2600] -> Loss: 3090.2620
Epoch [ 382/2600] -> Loss: 3085.6405
Epoch [ 383/2600] -> Loss: 3091.6723
Epoch [ 384/2600] -> Loss: 3088.5632
Epoch [ 385/2600] -> Loss: 3085.7318
Epoch [ 386/2600] -> Loss: 3097.5493
Epoch [ 387/2600] -> Loss: 3101.9037
Epoch [ 388/2600] -> Loss: 3103.6931
Epoch [ 389/2600] -> Loss: 3085.9103
Epoch [ 390/2600] -> Loss: 3090.9490
Epoch [ 391/2600] -> Loss: 3083.1543
Epoch [ 392/2600] -> Loss: 3099.4016
Epoch [ 393/2600] -> Loss: 3091.0056
Epoch [ 394/2600] -> Loss: 3090.2561
Epoch [ 395/2600] -> Loss: 3094.4956
Epoch [ 396/2600] -> Loss: 3088.6007
Epoch [ 397/2600] -> Loss: 3088.2007
Epoch [ 398/2600] -> Loss: 3112.1704
Epoch [ 399/2600] -> Loss: 3105.8432
--------------------------------------------------
Model checkpoint saved as FFNN_400.pth
--------------------------------------------------
Epoch [ 400/2600] -> Loss: 3095.8211
Epoch [ 401/2600] -> Loss: 3091.1602
Epoch [ 402/2600] -> Loss: 3099.9963
Epoch [ 403/2600] -> Loss: 3093.9475
Epoch [ 404/2600] -> Loss: 3114.4903
Epoch [ 405/2600] -> Loss: 3098.8643
Epoch [ 406/2600] -> Loss: 3108.7721
Epoch [ 407/2600] -> Loss: 3102.7826
Epoch [ 408/2600] -> Loss: 3113.7022
Epoch [ 409/2600] -> Loss: 3096.0885
Epoch [ 410/2600] -> Loss: 3100.4306
Epoch [ 411/2600] -> Loss: 3103.7210
Epoch [ 412/2600] -> Loss: 3105.7252
Epoch [ 413/2600] -> Loss: 3094.3917
Epoch [ 414/2600] -> Loss: 3096.1271
Epoch [ 415/2600] -> Loss: 3093.8803
Epoch [ 416/2600] -> Loss: 3093.7168
Epoch [ 417/2600] -> Loss: 3086.1601
Epoch [ 418/2600] -> Loss: 3088.3920
Epoch [ 419/2600] -> Loss: 3105.0842
Epoch [ 420/2600] -> Loss: 3096.1990
Epoch [ 421/2600] -> Loss: 3086.9824
Epoch [ 422/2600] -> Loss: 3083.0860
Epoch [ 423/2600] -> Loss: 3080.1820
Epoch [ 424/2600] -> Loss: 3090.2657
Epoch [ 425/2600] -> Loss: 3104.2951
Epoch [ 426/2600] -> Loss: 3092.3944
Epoch [ 427/2600] -> Loss: 3113.7644
Epoch [ 428/2600] -> Loss: 3097.1491
Epoch [ 429/2600] -> Loss: 3092.5951
Epoch [ 430/2600] -> Loss: 3102.3352
Epoch [ 431/2600] -> Loss: 3084.1056
Epoch [ 432/2600] -> Loss: 3085.3848
Epoch [ 433/2600] -> Loss: 3095.2677
Epoch [ 434/2600] -> Loss: 3086.8850
Epoch [ 435/2600] -> Loss: 3096.3462
Epoch [ 436/2600] -> Loss: 3090.7174
Epoch [ 437/2600] -> Loss: 3087.0287
Epoch [ 438/2600] -> Loss: 3083.8749
Epoch [ 439/2600] -> Loss: 3098.0939
Epoch [ 440/2600] -> Loss: 3094.4796
Epoch [ 441/2600] -> Loss: 3093.5649
Epoch [ 442/2600] -> Loss: 3090.0076
Epoch [ 443/2600] -> Loss: 3100.9937
Epoch [ 444/2600] -> Loss: 3093.1055
Epoch [ 445/2600] -> Loss: 3085.6821
Epoch [ 446/2600] -> Loss: 3106.0315
Epoch [ 447/2600] -> Loss: 3097.9769
Epoch [ 448/2600] -> Loss: 3100.1142
Epoch [ 449/2600] -> Loss: 3089.3648
Epoch [ 450/2600] -> Loss: 3103.3474
Epoch [ 451/2600] -> Loss: 3097.0510
Epoch [ 452/2600] -> Loss: 3087.8261
Epoch [ 453/2600] -> Loss: 3092.5159
Epoch [ 454/2600] -> Loss: 3132.8653
Epoch [ 455/2600] -> Loss: 3075.0487
Epoch [ 456/2600] -> Loss: 3087.8511
Epoch [ 457/2600] -> Loss: 3090.3146
Epoch [ 458/2600] -> Loss: 3095.9644
Epoch [ 459/2600] -> Loss: 3099.4903
Epoch [ 460/2600] -> Loss: 3087.6942
Epoch [ 461/2600] -> Loss: 3089.3594
Epoch [ 462/2600] -> Loss: 3093.9299
Epoch [ 463/2600] -> Loss: 3099.3740
Epoch [ 464/2600] -> Loss: 3084.8023
Epoch [ 465/2600] -> Loss: 3113.7320
Epoch [ 466/2600] -> Loss: 3085.7372
Epoch [ 467/2600] -> Loss: 3075.1024
Epoch [ 468/2600] -> Loss: 3096.6631
Epoch [ 469/2600] -> Loss: 3087.1165
Epoch [ 470/2600] -> Loss: 3085.6768
Epoch [ 471/2600] -> Loss: 3095.4551
Epoch [ 472/2600] -> Loss: 3102.8989
Epoch [ 473/2600] -> Loss: 3102.0095
Epoch [ 474/2600] -> Loss: 3104.2282
Epoch [ 475/2600] -> Loss: 3108.4439
Epoch [ 476/2600] -> Loss: 3104.9777
Epoch [ 477/2600] -> Loss: 3093.5903
Epoch [ 478/2600] -> Loss: 3082.3291
Epoch [ 479/2600] -> Loss: 3085.5590
Epoch [ 480/2600] -> Loss: 3100.6419
Epoch [ 481/2600] -> Loss: 3084.3745
Epoch [ 482/2600] -> Loss: 3090.2265
Epoch [ 483/2600] -> Loss: 3081.3555
Epoch [ 484/2600] -> Loss: 3088.7987
Epoch [ 485/2600] -> Loss: 3084.3678
Epoch [ 486/2600] -> Loss: 3099.4871
Epoch [ 487/2600] -> Loss: 3090.9272
Epoch [ 488/2600] -> Loss: 3078.4699
Epoch [ 489/2600] -> Loss: 3090.7661
Epoch [ 490/2600] -> Loss: 3108.6784
Epoch [ 491/2600] -> Loss: 3105.2299
Epoch [ 492/2600] -> Loss: 3103.1952
Epoch [ 493/2600] -> Loss: 3090.7310
Epoch [ 494/2600] -> Loss: 3089.3708
Epoch [ 495/2600] -> Loss: 3098.7190
Epoch [ 496/2600] -> Loss: 3097.0889
Epoch [ 497/2600] -> Loss: 3082.2500
Epoch [ 498/2600] -> Loss: 3095.7396
Epoch [ 499/2600] -> Loss: 3088.3140
Epoch [ 500/2600] -> Loss: 3101.2336
Epoch [ 501/2600] -> Loss: 3092.4023
Epoch [ 502/2600] -> Loss: 3093.4658
Epoch [ 503/2600] -> Loss: 3082.3966
Epoch [ 504/2600] -> Loss: 3081.4107
Epoch [ 505/2600] -> Loss: 3104.8105
Epoch [ 506/2600] -> Loss: 3092.1721
Epoch [ 507/2600] -> Loss: 3092.9324
Epoch [ 508/2600] -> Loss: 3096.7307
Epoch [ 509/2600] -> Loss: 3095.6772
Epoch [ 510/2600] -> Loss: 3102.0435
Epoch [ 511/2600] -> Loss: 3119.6603
Epoch [ 512/2600] -> Loss: 3110.2818
Epoch [ 513/2600] -> Loss: 3084.5226
Epoch [ 514/2600] -> Loss: 3079.7019
Epoch [ 515/2600] -> Loss: 3081.0425
Epoch [ 516/2600] -> Loss: 3090.7391
Epoch [ 517/2600] -> Loss: 3070.8007
Epoch [ 518/2600] -> Loss: 3086.4272
Epoch [ 519/2600] -> Loss: 3101.9652
Epoch [ 520/2600] -> Loss: 3074.0032
Epoch [ 521/2600] -> Loss: 3088.1818
Epoch [ 522/2600] -> Loss: 3074.5841
Epoch [ 523/2600] -> Loss: 3091.5734
Epoch [ 524/2600] -> Loss: 3098.8812
Epoch [ 525/2600] -> Loss: 3088.2940
Epoch [ 526/2600] -> Loss: 3111.7680
Epoch [ 527/2600] -> Loss: 3093.5244
Epoch [ 528/2600] -> Loss: 3096.3738
Epoch [ 529/2600] -> Loss: 3090.8175
Epoch [ 530/2600] -> Loss: 3108.2826
Epoch [ 531/2600] -> Loss: 3093.1430
Epoch [ 532/2600] -> Loss: 3097.7475
Epoch [ 533/2600] -> Loss: 3083.0919
Epoch [ 534/2600] -> Loss: 3119.7315
Epoch [ 535/2600] -> Loss: 3090.3105
Epoch [ 536/2600] -> Loss: 3088.9946
Epoch [ 537/2600] -> Loss: 3091.8252
Epoch [ 538/2600] -> Loss: 3085.4665
Epoch [ 539/2600] -> Loss: 3102.5448
Epoch [ 540/2600] -> Loss: 3083.4820
Epoch [ 541/2600] -> Loss: 3086.1435
Epoch [ 542/2600] -> Loss: 3087.0047
Epoch [ 543/2600] -> Loss: 3082.0737
Epoch [ 544/2600] -> Loss: 3112.1298
Epoch [ 545/2600] -> Loss: 3096.4716
Epoch [ 546/2600] -> Loss: 3091.1007
Epoch [ 547/2600] -> Loss: 3088.8438
Epoch [ 548/2600] -> Loss: 3089.1193
Epoch [ 549/2600] -> Loss: 3102.2097
Epoch [ 550/2600] -> Loss: 3082.7386
Epoch [ 551/2600] -> Loss: 3104.2047
Epoch [ 552/2600] -> Loss: 3102.0697
Epoch [ 553/2600] -> Loss: 3094.6515
Epoch [ 554/2600] -> Loss: 3097.9846
Epoch [ 555/2600] -> Loss: 3096.2726
Epoch [ 556/2600] -> Loss: 3084.9287
Epoch [ 557/2600] -> Loss: 3096.5237
Epoch [ 558/2600] -> Loss: 3097.0787
Epoch [ 559/2600] -> Loss: 3088.7374
Epoch [ 560/2600] -> Loss: 3093.4780
Epoch [ 561/2600] -> Loss: 3093.9503
Epoch [ 562/2600] -> Loss: 3096.8936
Epoch [ 563/2600] -> Loss: 3119.3232
Epoch [ 564/2600] -> Loss: 3081.7757
Epoch [ 565/2600] -> Loss: 3084.6118
Epoch [ 566/2600] -> Loss: 3089.5158
Epoch [ 567/2600] -> Loss: 3110.9984
Epoch [ 568/2600] -> Loss: 3087.0907
Epoch [ 569/2600] -> Loss: 3097.6635
Epoch [ 570/2600] -> Loss: 3100.9123
Epoch [ 571/2600] -> Loss: 3098.9395
Epoch [ 572/2600] -> Loss: 3095.5243
Epoch [ 573/2600] -> Loss: 3087.2424
Epoch [ 574/2600] -> Loss: 3086.5684
Epoch [ 575/2600] -> Loss: 3087.4137
Epoch [ 576/2600] -> Loss: 3108.2269
Epoch [ 577/2600] -> Loss: 3115.7773
Epoch [ 578/2600] -> Loss: 3093.8667
Epoch [ 579/2600] -> Loss: 3082.5255
Epoch [ 580/2600] -> Loss: 3082.4476
Epoch [ 581/2600] -> Loss: 3110.7584
Epoch [ 582/2600] -> Loss: 3108.0176
Epoch [ 583/2600] -> Loss: 3092.7593
Epoch [ 584/2600] -> Loss: 3105.7252
Epoch [ 585/2600] -> Loss: 3086.1187
Epoch [ 586/2600] -> Loss: 3084.2382
Epoch [ 587/2600] -> Loss: 3099.9142
Epoch [ 588/2600] -> Loss: 3088.5399
Epoch [ 589/2600] -> Loss: 3092.7829
Epoch [ 590/2600] -> Loss: 3099.0212
Epoch [ 591/2600] -> Loss: 3090.4119
Epoch [ 592/2600] -> Loss: 3094.1751
Epoch [ 593/2600] -> Loss: 3086.1044
Epoch [ 594/2600] -> Loss: 3105.3163
Epoch [ 595/2600] -> Loss: 3097.2809
Epoch [ 596/2600] -> Loss: 3089.8488
Epoch [ 597/2600] -> Loss: 3092.0452
Epoch [ 598/2600] -> Loss: 3092.1988
Epoch [ 599/2600] -> Loss: 3092.3656
--------------------------------------------------
Model checkpoint saved as FFNN_600.pth
--------------------------------------------------
Epoch [ 600/2600] -> Loss: 3099.2234
Epoch [ 601/2600] -> Loss: 3088.0658
Epoch [ 602/2600] -> Loss: 3085.5796
Epoch [ 603/2600] -> Loss: 3079.3363
Epoch [ 604/2600] -> Loss: 3087.1546
Epoch [ 605/2600] -> Loss: 3101.0098
Epoch [ 606/2600] -> Loss: 3076.6400
Epoch [ 607/2600] -> Loss: 3085.3691
Epoch [ 608/2600] -> Loss: 3083.0259
Epoch [ 609/2600] -> Loss: 3094.1169
Epoch [ 610/2600] -> Loss: 3076.8326
Epoch [ 611/2600] -> Loss: 3089.4837
Epoch [ 612/2600] -> Loss: 3110.8723
Epoch [ 613/2600] -> Loss: 3098.9794
Epoch [ 614/2600] -> Loss: 3101.8313
Epoch [ 615/2600] -> Loss: 3097.8020
Epoch [ 616/2600] -> Loss: 3094.2794
Epoch [ 617/2600] -> Loss: 3090.9587
Epoch [ 618/2600] -> Loss: 3085.5894
Epoch [ 619/2600] -> Loss: 3091.6723
Epoch [ 620/2600] -> Loss: 3080.5270
Epoch [ 621/2600] -> Loss: 3091.8248
Epoch [ 622/2600] -> Loss: 3106.9405
Epoch [ 623/2600] -> Loss: 3095.9306
Epoch [ 624/2600] -> Loss: 3100.0076
Epoch [ 625/2600] -> Loss: 3088.2583
Epoch [ 626/2600] -> Loss: 3088.2101
Epoch [ 627/2600] -> Loss: 3087.9614
Epoch [ 628/2600] -> Loss: 3100.3629
Epoch [ 629/2600] -> Loss: 3091.9081
Epoch [ 630/2600] -> Loss: 3101.4323
Epoch [ 631/2600] -> Loss: 3120.9509
Epoch [ 632/2600] -> Loss: 3089.4578
Epoch [ 633/2600] -> Loss: 3106.5505
Epoch [ 634/2600] -> Loss: 3089.5718
Epoch [ 635/2600] -> Loss: 3085.6963
Epoch [ 636/2600] -> Loss: 3097.5900
Epoch [ 637/2600] -> Loss: 3090.8827
Epoch [ 638/2600] -> Loss: 3106.3645
Epoch [ 639/2600] -> Loss: 3115.7435
Epoch [ 640/2600] -> Loss: 3095.3093
Epoch [ 641/2600] -> Loss: 3084.7149
Epoch [ 642/2600] -> Loss: 3108.5222
Epoch [ 643/2600] -> Loss: 3084.4965
Epoch [ 644/2600] -> Loss: 3106.1575
Epoch [ 645/2600] -> Loss: 3101.5704
Epoch [ 646/2600] -> Loss: 3106.1969
Epoch [ 647/2600] -> Loss: 3084.3207
Epoch [ 648/2600] -> Loss: 3093.7396
Epoch [ 649/2600] -> Loss: 3086.8470
Epoch [ 650/2600] -> Loss: 3100.4908
Epoch [ 651/2600] -> Loss: 3091.7209
Epoch [ 652/2600] -> Loss: 3106.0968
Epoch [ 653/2600] -> Loss: 3083.0027
Epoch [ 654/2600] -> Loss: 3090.8513
Epoch [ 655/2600] -> Loss: 3099.1058
Epoch [ 656/2600] -> Loss: 3099.0890
Epoch [ 657/2600] -> Loss: 3075.0944
Epoch [ 658/2600] -> Loss: 3094.0952
Epoch [ 659/2600] -> Loss: 3090.4536
Epoch [ 660/2600] -> Loss: 3086.1793
Epoch [ 661/2600] -> Loss: 3090.2701
Epoch [ 662/2600] -> Loss: 3084.2856
Epoch [ 663/2600] -> Loss: 3089.6360
Epoch [ 664/2600] -> Loss: 3081.4773
Epoch [ 665/2600] -> Loss: 3097.5168
Epoch [ 666/2600] -> Loss: 3078.5595
Epoch [ 667/2600] -> Loss: 3081.4786
Epoch [ 668/2600] -> Loss: 3095.0906
Epoch [ 669/2600] -> Loss: 3101.4964
Epoch [ 670/2600] -> Loss: 3090.4148
Epoch [ 671/2600] -> Loss: 3097.7006
Epoch [ 672/2600] -> Loss: 3102.4858
Epoch [ 673/2600] -> Loss: 3085.5287
Epoch [ 674/2600] -> Loss: 3083.7605
