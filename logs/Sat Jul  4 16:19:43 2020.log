Usage: python3 SSN_predictor.py <path_to_ssn_datafile> <path_to_aa_datafile>
----------------------------------------------------------------
Code running on device: cuda
----------------------------------------------------------------
Data loaded from file locations :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
----------------------------------------------------------------
Solar cycle data loaded/saved as: cycle_data.pickle
----------------------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=3, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=3, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
----------------------------------------------------------------
No pre-trained models available, initializing model weights
----------------------------------------------------------------
Training model with: num_epochs=1200, start_lr=0.0002
Epoch [   1/1200] -> Loss: 89.6298
Epoch [   2/1200] -> Loss: 52.7554
Epoch [   3/1200] -> Loss: 41.3090
Epoch [   4/1200] -> Loss: 40.9165
Epoch [   5/1200] -> Loss: 40.7390
Epoch [   6/1200] -> Loss: 40.5850
Epoch [   7/1200] -> Loss: 40.4374
Epoch [   8/1200] -> Loss: 40.3084
Epoch [   9/1200] -> Loss: 40.1912
Epoch [  10/1200] -> Loss: 40.0857
Epoch [  11/1200] -> Loss: 39.9860
Epoch [  12/1200] -> Loss: 39.9018
Epoch [  13/1200] -> Loss: 39.8027
Epoch [  14/1200] -> Loss: 39.7318
Epoch [  15/1200] -> Loss: 39.6536
Epoch [  16/1200] -> Loss: 39.5851
Epoch [  17/1200] -> Loss: 39.5276
Epoch [  18/1200] -> Loss: 39.4651
Epoch [  19/1200] -> Loss: 39.3964
Epoch [  20/1200] -> Loss: 39.3424
Epoch [  21/1200] -> Loss: 39.2659
Epoch [  22/1200] -> Loss: 39.2327
Epoch [  23/1200] -> Loss: 39.1460
Epoch [  24/1200] -> Loss: 39.0891
Epoch [  25/1200] -> Loss: 39.0401
Epoch [  26/1200] -> Loss: 38.9914
Epoch [  27/1200] -> Loss: 38.9247
Epoch [  28/1200] -> Loss: 38.8745
Epoch [  29/1200] -> Loss: 38.8096
Epoch [  30/1200] -> Loss: 38.7551
Epoch [  31/1200] -> Loss: 38.7034
Epoch [  32/1200] -> Loss: 38.6468
Epoch [  33/1200] -> Loss: 38.5733
Epoch [  34/1200] -> Loss: 38.5212
Epoch [  35/1200] -> Loss: 38.4756
Epoch [  36/1200] -> Loss: 38.4184
Epoch [  37/1200] -> Loss: 38.3563
Epoch [  38/1200] -> Loss: 38.2983
Epoch [  39/1200] -> Loss: 38.2343
Epoch [  40/1200] -> Loss: 38.1544
Epoch [  41/1200] -> Loss: 38.0617
Epoch [  42/1200] -> Loss: 38.0696
Epoch [  43/1200] -> Loss: 37.9966
Epoch [  44/1200] -> Loss: 37.9231
Epoch [  45/1200] -> Loss: 37.8394
Epoch [  46/1200] -> Loss: 37.7996
Epoch [  47/1200] -> Loss: 37.7286
Epoch [  48/1200] -> Loss: 37.6400
Epoch [  49/1200] -> Loss: 37.6000
Epoch [  50/1200] -> Loss: 37.5286
Epoch [  51/1200] -> Loss: 37.4413
Epoch [  52/1200] -> Loss: 37.3914
Epoch [  53/1200] -> Loss: 37.2968
Epoch [  54/1200] -> Loss: 37.2372
Epoch [  55/1200] -> Loss: 37.1640
Epoch [  56/1200] -> Loss: 37.0574
Epoch [  57/1200] -> Loss: 37.0067
Epoch [  58/1200] -> Loss: 36.9233
Epoch [  59/1200] -> Loss: 36.8473
Epoch [  60/1200] -> Loss: 36.7569
Epoch [  61/1200] -> Loss: 36.6711
Epoch [  62/1200] -> Loss: 36.5937
Epoch [  63/1200] -> Loss: 36.5007
Epoch [  64/1200] -> Loss: 36.3965
Epoch [  65/1200] -> Loss: 36.3284
Epoch [  66/1200] -> Loss: 36.2200
Epoch [  67/1200] -> Loss: 36.1388
Epoch [  68/1200] -> Loss: 36.0298
Epoch [  69/1200] -> Loss: 35.9380
Epoch [  70/1200] -> Loss: 35.8331
Epoch [  71/1200] -> Loss: 35.7329
Epoch [  72/1200] -> Loss: 35.6169
Epoch [  73/1200] -> Loss: 35.5151
Epoch [  74/1200] -> Loss: 35.3112
Epoch [  75/1200] -> Loss: 35.3126
Epoch [  76/1200] -> Loss: 35.1767
Epoch [  77/1200] -> Loss: 35.0450
Epoch [  78/1200] -> Loss: 34.9349
Epoch [  79/1200] -> Loss: 34.8319
Epoch [  80/1200] -> Loss: 34.6556
Epoch [  81/1200] -> Loss: 34.5644
Epoch [  82/1200] -> Loss: 34.4386
Epoch [  83/1200] -> Loss: 34.3283
Epoch [  84/1200] -> Loss: 34.1933
Epoch [  85/1200] -> Loss: 34.0479
Epoch [  86/1200] -> Loss: 33.9205
Epoch [  87/1200] -> Loss: 33.8216
Epoch [  88/1200] -> Loss: 33.7082
Epoch [  89/1200] -> Loss: 33.5701
Epoch [  90/1200] -> Loss: 33.4643
Epoch [  91/1200] -> Loss: 33.3496
Epoch [  92/1200] -> Loss: 33.2303
Epoch [  93/1200] -> Loss: 33.0999
Epoch [  94/1200] -> Loss: 32.9711
Epoch [  95/1200] -> Loss: 32.8663
Epoch [  96/1200] -> Loss: 32.7425
Epoch [  97/1200] -> Loss: 32.6191
Epoch [  98/1200] -> Loss: 32.5009
Epoch [  99/1200] -> Loss: 32.3880
----------------------------------------------------------------
Model checkpoint saved as FFNN_100.pth
----------------------------------------------------------------
Epoch [ 100/1200] -> Loss: 32.2494
Epoch [ 101/1200] -> Loss: 32.1700
Epoch [ 102/1200] -> Loss: 32.0554
Epoch [ 103/1200] -> Loss: 31.9466
Epoch [ 104/1200] -> Loss: 31.8533
Epoch [ 105/1200] -> Loss: 31.7439
Epoch [ 106/1200] -> Loss: 31.6418
Epoch [ 107/1200] -> Loss: 31.5369
Epoch [ 108/1200] -> Loss: 31.3822
Epoch [ 109/1200] -> Loss: 31.3878
Epoch [ 110/1200] -> Loss: 31.2161
Epoch [ 111/1200] -> Loss: 31.1715
Epoch [ 112/1200] -> Loss: 31.0580
Epoch [ 113/1200] -> Loss: 30.9733
Epoch [ 114/1200] -> Loss: 30.8714
Epoch [ 115/1200] -> Loss: 30.7852
Epoch [ 116/1200] -> Loss: 30.6866
Epoch [ 117/1200] -> Loss: 30.5756
Epoch [ 118/1200] -> Loss: 30.5619
Epoch [ 119/1200] -> Loss: 30.4738
Epoch [ 120/1200] -> Loss: 30.3754
Epoch [ 121/1200] -> Loss: 30.3059
Epoch [ 122/1200] -> Loss: 30.0853
Epoch [ 123/1200] -> Loss: 30.1091
Epoch [ 124/1200] -> Loss: 30.1030
Epoch [ 125/1200] -> Loss: 30.0171
Epoch [ 126/1200] -> Loss: 29.9789
Epoch [ 127/1200] -> Loss: 29.8387
Epoch [ 128/1200] -> Loss: 29.8446
Epoch [ 129/1200] -> Loss: 29.7843
Epoch [ 130/1200] -> Loss: 29.7068
Epoch [ 131/1200] -> Loss: 29.6759
Epoch [ 132/1200] -> Loss: 29.6276
Epoch [ 133/1200] -> Loss: 29.5617
Epoch [ 134/1200] -> Loss: 29.5905
Epoch [ 135/1200] -> Loss: 29.5372
Epoch [ 136/1200] -> Loss: 29.5141
Epoch [ 137/1200] -> Loss: 29.3816
Epoch [ 138/1200] -> Loss: 29.4599
Epoch [ 139/1200] -> Loss: 29.3825
Epoch [ 140/1200] -> Loss: 29.3950
Epoch [ 141/1200] -> Loss: 29.3481
Epoch [ 142/1200] -> Loss: 29.3321
Epoch [ 143/1200] -> Loss: 29.3346
Epoch [ 144/1200] -> Loss: 29.3121
Epoch [ 145/1200] -> Loss: 29.2675
Epoch [ 146/1200] -> Loss: 29.2541
Epoch [ 147/1200] -> Loss: 29.2266
Epoch [ 148/1200] -> Loss: 29.2498
Epoch [ 149/1200] -> Loss: 29.2128
Epoch [ 150/1200] -> Loss: 29.1937
Epoch [ 151/1200] -> Loss: 29.1816
Epoch [ 152/1200] -> Loss: 29.1684
Epoch [ 153/1200] -> Loss: 29.0547
Epoch [ 154/1200] -> Loss: 29.1109
Epoch [ 155/1200] -> Loss: 29.0924
Epoch [ 156/1200] -> Loss: 29.0940
Epoch [ 157/1200] -> Loss: 29.0579
Epoch [ 158/1200] -> Loss: 28.9972
Epoch [ 159/1200] -> Loss: 29.0336
Epoch [ 160/1200] -> Loss: 28.9982
Epoch [ 161/1200] -> Loss: 29.0064
Epoch [ 162/1200] -> Loss: 28.9990
Epoch [ 163/1200] -> Loss: 28.9752
Epoch [ 164/1200] -> Loss: 28.9595
Epoch [ 165/1200] -> Loss: 28.9026
Epoch [ 166/1200] -> Loss: 28.9317
Epoch [ 167/1200] -> Loss: 28.8378
Epoch [ 168/1200] -> Loss: 28.8864
Epoch [ 169/1200] -> Loss: 28.8740
Epoch [ 170/1200] -> Loss: 28.8386
Epoch [ 171/1200] -> Loss: 28.8181
Epoch [ 172/1200] -> Loss: 28.8406
Epoch [ 173/1200] -> Loss: 28.8075
Epoch [ 174/1200] -> Loss: 28.7686
Epoch [ 175/1200] -> Loss: 28.7770
Epoch [ 176/1200] -> Loss: 28.7747
Epoch [ 177/1200] -> Loss: 28.7647
Epoch [ 178/1200] -> Loss: 28.7065
Epoch [ 179/1200] -> Loss: 28.6983
Epoch [ 180/1200] -> Loss: 28.6175
Epoch [ 181/1200] -> Loss: 28.7496
Epoch [ 182/1200] -> Loss: 28.7352
Epoch [ 183/1200] -> Loss: 28.6906
Epoch [ 184/1200] -> Loss: 28.6098
Epoch [ 185/1200] -> Loss: 28.6807
Epoch [ 186/1200] -> Loss: 28.6286
Epoch [ 187/1200] -> Loss: 28.6398
Epoch [ 188/1200] -> Loss: 28.5900
Epoch [ 189/1200] -> Loss: 28.5881
Epoch [ 190/1200] -> Loss: 28.6106
Epoch [ 191/1200] -> Loss: 28.5384
Epoch [ 192/1200] -> Loss: 28.5949
Epoch [ 193/1200] -> Loss: 28.5411
Epoch [ 194/1200] -> Loss: 28.5721
Epoch [ 195/1200] -> Loss: 28.5430
Epoch [ 196/1200] -> Loss: 28.5657
Epoch [ 197/1200] -> Loss: 28.5674
Epoch [ 198/1200] -> Loss: 28.5498
Epoch [ 199/1200] -> Loss: 28.5261
----------------------------------------------------------------
Model checkpoint saved as FFNN_200.pth
----------------------------------------------------------------
Epoch [ 200/1200] -> Loss: 28.4766
Epoch [ 201/1200] -> Loss: 28.5270
Epoch [ 202/1200] -> Loss: 28.4144
Epoch [ 203/1200] -> Loss: 28.5417
Epoch [ 204/1200] -> Loss: 28.4700
Epoch [ 205/1200] -> Loss: 28.4743
Epoch [ 206/1200] -> Loss: 28.4519
Epoch [ 207/1200] -> Loss: 28.4668
Epoch [ 208/1200] -> Loss: 28.4104
Epoch [ 209/1200] -> Loss: 28.4382
Epoch [ 210/1200] -> Loss: 28.4285
Epoch [ 211/1200] -> Loss: 28.4409
Epoch [ 212/1200] -> Loss: 28.4083
Epoch [ 213/1200] -> Loss: 28.4194
Epoch [ 214/1200] -> Loss: 28.3359
Epoch [ 215/1200] -> Loss: 28.4278
Epoch [ 216/1200] -> Loss: 28.3302
Epoch [ 217/1200] -> Loss: 28.3184
Epoch [ 218/1200] -> Loss: 28.3197
Epoch [ 219/1200] -> Loss: 28.3916
Epoch [ 220/1200] -> Loss: 28.3048
Epoch [ 221/1200] -> Loss: 28.3827
Epoch [ 222/1200] -> Loss: 28.3716
Epoch [ 223/1200] -> Loss: 28.3067
Epoch [ 224/1200] -> Loss: 28.3621
Epoch [ 225/1200] -> Loss: 28.3194
Epoch [ 226/1200] -> Loss: 28.2534
Epoch [ 227/1200] -> Loss: 28.3298
Epoch [ 228/1200] -> Loss: 28.3206
Epoch [ 229/1200] -> Loss: 28.2498
Epoch [ 230/1200] -> Loss: 28.2906
Epoch [ 231/1200] -> Loss: 28.2979
Epoch [ 232/1200] -> Loss: 28.2443
Epoch [ 233/1200] -> Loss: 28.2865
Epoch [ 234/1200] -> Loss: 28.2851
Epoch [ 235/1200] -> Loss: 28.2313
Epoch [ 236/1200] -> Loss: 28.2397
Epoch [ 237/1200] -> Loss: 28.2530
Epoch [ 238/1200] -> Loss: 28.2598
Epoch [ 239/1200] -> Loss: 28.2722
Epoch [ 240/1200] -> Loss: 28.2494
Epoch [ 241/1200] -> Loss: 28.1945
Epoch [ 242/1200] -> Loss: 28.2258
Epoch [ 243/1200] -> Loss: 28.1961
Epoch [ 244/1200] -> Loss: 28.2361
Epoch [ 245/1200] -> Loss: 28.2331
Epoch [ 246/1200] -> Loss: 28.2205
Epoch [ 247/1200] -> Loss: 28.1766
Epoch [ 248/1200] -> Loss: 28.2382
Epoch [ 249/1200] -> Loss: 28.1311
Epoch [ 250/1200] -> Loss: 28.2328
Epoch [ 251/1200] -> Loss: 28.2098
Epoch [ 252/1200] -> Loss: 28.2086
Epoch [ 253/1200] -> Loss: 28.2116
Epoch [ 254/1200] -> Loss: 28.2216
Epoch [ 255/1200] -> Loss: 28.1533
Epoch [ 256/1200] -> Loss: 28.1976
Epoch [ 257/1200] -> Loss: 28.1741
Epoch [ 258/1200] -> Loss: 28.2123
Epoch [ 259/1200] -> Loss: 28.1573
Epoch [ 260/1200] -> Loss: 28.0997
Epoch [ 261/1200] -> Loss: 28.1327
Epoch [ 262/1200] -> Loss: 28.1297
Epoch [ 263/1200] -> Loss: 28.1027
Epoch [ 264/1200] -> Loss: 28.1551
Epoch [ 265/1200] -> Loss: 28.1473
Epoch [ 266/1200] -> Loss: 28.1159
Epoch [ 267/1200] -> Loss: 28.1544
Epoch [ 268/1200] -> Loss: 28.1442
Epoch [ 269/1200] -> Loss: 28.1176
Epoch [ 270/1200] -> Loss: 28.1193
Epoch   271: reducing learning rate of group 0 to 1.0000e-04.
Epoch [ 271/1200] -> Loss: 28.1207
Epoch [ 272/1200] -> Loss: 28.0777
Epoch [ 273/1200] -> Loss: 28.0725
Epoch [ 274/1200] -> Loss: 28.0686
Epoch [ 275/1200] -> Loss: 28.0536
Epoch [ 276/1200] -> Loss: 28.0657
Epoch [ 277/1200] -> Loss: 28.0709
Epoch [ 278/1200] -> Loss: 28.0534
Epoch [ 279/1200] -> Loss: 28.0099
Epoch [ 280/1200] -> Loss: 28.0651
Epoch [ 281/1200] -> Loss: 28.0554
Epoch [ 282/1200] -> Loss: 28.0092
Epoch [ 283/1200] -> Loss: 28.0279
Epoch [ 284/1200] -> Loss: 28.0366
Epoch [ 285/1200] -> Loss: 27.9819
Epoch [ 286/1200] -> Loss: 28.0335
Epoch [ 287/1200] -> Loss: 28.0331
Epoch [ 288/1200] -> Loss: 28.0266
Epoch [ 289/1200] -> Loss: 28.0426
Epoch [ 290/1200] -> Loss: 28.0204
Epoch [ 291/1200] -> Loss: 28.0121
Epoch [ 292/1200] -> Loss: 28.0231
Epoch [ 293/1200] -> Loss: 28.0176
Epoch [ 294/1200] -> Loss: 28.0400
Epoch [ 295/1200] -> Loss: 28.0057
Epoch   296: reducing learning rate of group 0 to 5.0000e-05.
Epoch [ 296/1200] -> Loss: 28.0130
Epoch [ 297/1200] -> Loss: 27.9830
Epoch [ 298/1200] -> Loss: 27.9877
Epoch [ 299/1200] -> Loss: 27.9824
----------------------------------------------------------------
Model checkpoint saved as FFNN_300.pth
----------------------------------------------------------------
Epoch [ 300/1200] -> Loss: 27.9831
Epoch [ 301/1200] -> Loss: 27.9790
Epoch [ 302/1200] -> Loss: 27.9825
Epoch [ 303/1200] -> Loss: 27.9817
Epoch [ 304/1200] -> Loss: 27.9817
Epoch [ 305/1200] -> Loss: 27.9785
Epoch [ 306/1200] -> Loss: 27.9759
Epoch [ 307/1200] -> Loss: 27.9736
Epoch [ 308/1200] -> Loss: 27.9694
Epoch [ 309/1200] -> Loss: 27.9698
Epoch [ 310/1200] -> Loss: 27.9637
Epoch [ 311/1200] -> Loss: 27.9681
Epoch [ 312/1200] -> Loss: 27.9651
Epoch [ 313/1200] -> Loss: 27.9844
Epoch [ 314/1200] -> Loss: 27.9661
Epoch [ 315/1200] -> Loss: 27.9676
Epoch [ 316/1200] -> Loss: 27.9592
Epoch [ 317/1200] -> Loss: 27.9549
Epoch [ 318/1200] -> Loss: 27.9720
Epoch [ 319/1200] -> Loss: 27.9457
Epoch [ 320/1200] -> Loss: 27.9474
Epoch [ 321/1200] -> Loss: 27.9428
Epoch [ 322/1200] -> Loss: 27.9571
Epoch [ 323/1200] -> Loss: 27.9552
Epoch [ 324/1200] -> Loss: 27.9568
Epoch [ 325/1200] -> Loss: 27.9579
Epoch [ 326/1200] -> Loss: 27.9366
Epoch [ 327/1200] -> Loss: 27.9625
Epoch [ 328/1200] -> Loss: 27.9571
Epoch [ 329/1200] -> Loss: 27.9551
Epoch [ 330/1200] -> Loss: 27.9484
Epoch [ 331/1200] -> Loss: 27.9487
Epoch [ 332/1200] -> Loss: 27.9599
Epoch [ 333/1200] -> Loss: 27.9570
Epoch [ 334/1200] -> Loss: 27.9536
Epoch [ 335/1200] -> Loss: 27.9428
Epoch [ 336/1200] -> Loss: 27.9519
Epoch   337: reducing learning rate of group 0 to 2.5000e-05.
Epoch [ 337/1200] -> Loss: 27.9439
Epoch [ 338/1200] -> Loss: 27.9265
Epoch [ 339/1200] -> Loss: 27.9256
Epoch [ 340/1200] -> Loss: 27.9240
Epoch [ 341/1200] -> Loss: 27.9306
Epoch [ 342/1200] -> Loss: 27.9270
Epoch [ 343/1200] -> Loss: 27.9313
Epoch [ 344/1200] -> Loss: 27.9228
Epoch [ 345/1200] -> Loss: 27.9281
Epoch [ 346/1200] -> Loss: 27.9243
Epoch [ 347/1200] -> Loss: 27.9189
Epoch [ 348/1200] -> Loss: 27.9252
Epoch [ 349/1200] -> Loss: 27.9264
Epoch [ 350/1200] -> Loss: 27.9258
Epoch [ 351/1200] -> Loss: 27.9229
Epoch [ 352/1200] -> Loss: 27.9112
Epoch [ 353/1200] -> Loss: 27.9216
Epoch [ 354/1200] -> Loss: 27.9237
Epoch [ 355/1200] -> Loss: 27.9218
Epoch [ 356/1200] -> Loss: 27.9176
Epoch [ 357/1200] -> Loss: 27.9221
Epoch [ 358/1200] -> Loss: 27.9186
Epoch [ 359/1200] -> Loss: 27.9229
Epoch [ 360/1200] -> Loss: 27.9200
Epoch [ 361/1200] -> Loss: 27.9179
Epoch [ 362/1200] -> Loss: 27.9190
Epoch   363: reducing learning rate of group 0 to 1.2500e-05.
Epoch [ 363/1200] -> Loss: 27.9195
Epoch [ 364/1200] -> Loss: 27.9097
Epoch [ 365/1200] -> Loss: 27.9097
Epoch [ 366/1200] -> Loss: 27.9090
Epoch [ 367/1200] -> Loss: 27.9083
Epoch [ 368/1200] -> Loss: 27.9055
Epoch [ 369/1200] -> Loss: 27.9088
Epoch [ 370/1200] -> Loss: 27.9082
Epoch [ 371/1200] -> Loss: 27.9087
Epoch [ 372/1200] -> Loss: 27.9061
Epoch [ 373/1200] -> Loss: 27.9070
Epoch [ 374/1200] -> Loss: 27.9064
Epoch [ 375/1200] -> Loss: 27.9063
Epoch [ 376/1200] -> Loss: 27.9079
Epoch [ 377/1200] -> Loss: 27.9071
Epoch   378: reducing learning rate of group 0 to 6.2500e-06.
Epoch [ 378/1200] -> Loss: 27.9060
Epoch [ 379/1200] -> Loss: 27.9022
Epoch [ 380/1200] -> Loss: 27.9010
Epoch [ 381/1200] -> Loss: 27.9016
Epoch [ 382/1200] -> Loss: 27.9012
Epoch [ 383/1200] -> Loss: 27.9021
Epoch [ 384/1200] -> Loss: 27.9004
Epoch [ 385/1200] -> Loss: 27.9012
Epoch [ 386/1200] -> Loss: 27.9010
Epoch [ 387/1200] -> Loss: 27.9002
Epoch [ 388/1200] -> Loss: 27.9009
Epoch [ 389/1200] -> Loss: 27.9012
Epoch   390: reducing learning rate of group 0 to 3.1250e-06.
Epoch [ 390/1200] -> Loss: 27.8999
Epoch [ 391/1200] -> Loss: 27.8983
Epoch [ 392/1200] -> Loss: 27.8980
Epoch [ 393/1200] -> Loss: 27.8975
Epoch [ 394/1200] -> Loss: 27.8976
Epoch [ 395/1200] -> Loss: 27.8978
Epoch [ 396/1200] -> Loss: 27.8978
Epoch [ 397/1200] -> Loss: 27.8972
Epoch [ 398/1200] -> Loss: 27.8973
Epoch [ 399/1200] -> Loss: 27.8973
----------------------------------------------------------------
Model checkpoint saved as FFNN_400.pth
----------------------------------------------------------------
Epoch [ 400/1200] -> Loss: 27.8976
Epoch [ 401/1200] -> Loss: 27.8971
Epoch   402: reducing learning rate of group 0 to 1.5625e-06.
Epoch [ 402/1200] -> Loss: 27.8974
Epoch [ 403/1200] -> Loss: 27.8962
Epoch [ 404/1200] -> Loss: 27.8961
Epoch [ 405/1200] -> Loss: 27.8959
Epoch [ 406/1200] -> Loss: 27.8960
Epoch [ 407/1200] -> Loss: 27.8958
Epoch [ 408/1200] -> Loss: 27.8957
Epoch [ 409/1200] -> Loss: 27.8959
Epoch [ 410/1200] -> Loss: 27.8959
Epoch [ 411/1200] -> Loss: 27.8958
Epoch [ 412/1200] -> Loss: 27.8958
Epoch   413: reducing learning rate of group 0 to 7.8125e-07.
Epoch [ 413/1200] -> Loss: 27.8956
Epoch [ 414/1200] -> Loss: 27.8951
Epoch [ 415/1200] -> Loss: 27.8951
Epoch [ 416/1200] -> Loss: 27.8951
Epoch [ 417/1200] -> Loss: 27.8951
Epoch [ 418/1200] -> Loss: 27.8950
Epoch [ 419/1200] -> Loss: 27.8950
Epoch [ 420/1200] -> Loss: 27.8950
Epoch [ 421/1200] -> Loss: 27.8949
Epoch [ 422/1200] -> Loss: 27.8950
Epoch [ 423/1200] -> Loss: 27.8949
Epoch [ 424/1200] -> Loss: 27.8950
Epoch   425: reducing learning rate of group 0 to 3.9063e-07.
Epoch [ 425/1200] -> Loss: 27.8949
Epoch [ 426/1200] -> Loss: 27.8946
Epoch [ 427/1200] -> Loss: 27.8946
Epoch [ 428/1200] -> Loss: 27.8946
Epoch [ 429/1200] -> Loss: 27.8946
Epoch [ 430/1200] -> Loss: 27.8946
Epoch [ 431/1200] -> Loss: 27.8945
Epoch [ 432/1200] -> Loss: 27.8946
Epoch [ 433/1200] -> Loss: 27.8945
Epoch [ 434/1200] -> Loss: 27.8945
Epoch [ 435/1200] -> Loss: 27.8946
Epoch   436: reducing learning rate of group 0 to 1.9531e-07.
Epoch [ 436/1200] -> Loss: 27.8945
Epoch [ 437/1200] -> Loss: 27.8944
Epoch [ 438/1200] -> Loss: 27.8944
Epoch [ 439/1200] -> Loss: 27.8944
Epoch [ 440/1200] -> Loss: 27.8944
Epoch [ 441/1200] -> Loss: 27.8944
Epoch [ 442/1200] -> Loss: 27.8944
Epoch [ 443/1200] -> Loss: 27.8943
Epoch [ 444/1200] -> Loss: 27.8944
Epoch [ 445/1200] -> Loss: 27.8943
Epoch [ 446/1200] -> Loss: 27.8944
Epoch   447: reducing learning rate of group 0 to 9.7656e-08.
Epoch [ 447/1200] -> Loss: 27.8943
Epoch [ 448/1200] -> Loss: 27.8943
Epoch [ 449/1200] -> Loss: 27.8943
Epoch [ 450/1200] -> Loss: 27.8943
Epoch [ 451/1200] -> Loss: 27.8943
Epoch [ 452/1200] -> Loss: 27.8943
Epoch [ 453/1200] -> Loss: 27.8943
Epoch [ 454/1200] -> Loss: 27.8943
Epoch [ 455/1200] -> Loss: 27.8943
Epoch [ 456/1200] -> Loss: 27.8943
Epoch [ 457/1200] -> Loss: 27.8943
Epoch   458: reducing learning rate of group 0 to 4.8828e-08.
Epoch [ 458/1200] -> Loss: 27.8943
Epoch [ 459/1200] -> Loss: 27.8942
Epoch [ 460/1200] -> Loss: 27.8942
Epoch [ 461/1200] -> Loss: 27.8942
Epoch [ 462/1200] -> Loss: 27.8942
Epoch [ 463/1200] -> Loss: 27.8942
Epoch [ 464/1200] -> Loss: 27.8942
Epoch [ 465/1200] -> Loss: 27.8942
Epoch [ 466/1200] -> Loss: 27.8942
Epoch [ 467/1200] -> Loss: 27.8942
Epoch [ 468/1200] -> Loss: 27.8942
Epoch   469: reducing learning rate of group 0 to 2.4414e-08.
Epoch [ 469/1200] -> Loss: 27.8942
Epoch [ 470/1200] -> Loss: 27.8942
Epoch [ 471/1200] -> Loss: 27.8942
Epoch [ 472/1200] -> Loss: 27.8942
Epoch [ 473/1200] -> Loss: 27.8942
Epoch [ 474/1200] -> Loss: 27.8942
Epoch [ 475/1200] -> Loss: 27.8942
Epoch [ 476/1200] -> Loss: 27.8942
Epoch [ 477/1200] -> Loss: 27.8942
Epoch [ 478/1200] -> Loss: 27.8942
Epoch [ 479/1200] -> Loss: 27.8942
Epoch   480: reducing learning rate of group 0 to 1.2207e-08.
Epoch [ 480/1200] -> Loss: 27.8942
Epoch [ 481/1200] -> Loss: 27.8942
Epoch [ 482/1200] -> Loss: 27.8942
Epoch [ 483/1200] -> Loss: 27.8942
Epoch [ 484/1200] -> Loss: 27.8942
Epoch [ 485/1200] -> Loss: 27.8942
Epoch [ 486/1200] -> Loss: 27.8942
Epoch [ 487/1200] -> Loss: 27.8942
Epoch [ 488/1200] -> Loss: 27.8942
Epoch [ 489/1200] -> Loss: 27.8942
Epoch [ 490/1200] -> Loss: 27.8942
Epoch [ 491/1200] -> Loss: 27.8942
Epoch [ 492/1200] -> Loss: 27.8942
Epoch [ 493/1200] -> Loss: 27.8942
Epoch [ 494/1200] -> Loss: 27.8942
Epoch [ 495/1200] -> Loss: 27.8942
Epoch [ 496/1200] -> Loss: 27.8942
Epoch [ 497/1200] -> Loss: 27.8942
Epoch [ 498/1200] -> Loss: 27.8942
Epoch [ 499/1200] -> Loss: 27.8942
----------------------------------------------------------------
Model checkpoint saved as FFNN_500.pth
----------------------------------------------------------------
Epoch [ 500/1200] -> Loss: 27.8942
Epoch [ 501/1200] -> Loss: 27.8942
Epoch [ 502/1200] -> Loss: 27.8942
Epoch [ 503/1200] -> Loss: 27.8942
Epoch [ 504/1200] -> Loss: 27.8942
Epoch [ 505/1200] -> Loss: 27.8942
Epoch [ 506/1200] -> Loss: 27.8942
Epoch [ 507/1200] -> Loss: 27.8942
Epoch [ 508/1200] -> Loss: 27.8942
Epoch [ 509/1200] -> Loss: 27.8942
Epoch [ 510/1200] -> Loss: 27.8942
Epoch [ 511/1200] -> Loss: 27.8942
Epoch [ 512/1200] -> Loss: 27.8942
Epoch [ 513/1200] -> Loss: 27.8942
Epoch [ 514/1200] -> Loss: 27.8942
Epoch [ 515/1200] -> Loss: 27.8942
Epoch [ 516/1200] -> Loss: 27.8942
Epoch [ 517/1200] -> Loss: 27.8942
Epoch [ 518/1200] -> Loss: 27.8942
