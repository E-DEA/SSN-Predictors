--------------------------------------------------
Code running on device: cuda
{'start_date': [[1749, 1], [1755, 2], [1766, 6], [1775, 6], [1784, 9], [1798, 4], [1810, 12], [1823, 5], [1833, 11], [1843, 7], [1855, 12], [1867, 3], [1878, 12], [1890, 3], [1902, 1], [1913, 7], [1923, 8], [1933, 9], [1944, 2], [1954, 4], [1964, 10], [1976, 3], [1986, 9], [1996, 5], [2008, 12]], 'end_date': [[1755, 1], [1766, 5], [1775, 5], [1784, 8], [1798, 3], [1810, 11], [1823, 4], [1833, 10], [1843, 6], [1855, 11], [1867, 2], [1878, 11], [1890, 2], [1902, 0], [1913, 6], [1923, 7], [1933, 8], [1944, 1], [1954, 3], [1964, 9], [1976, 2], [1986, 8], [1996, 4], [2008, 11], [2019, 11]], 'max_date': [[1750, 4], [1761, 6], [1769, 9], [1778, 5], [1788, 2], [1794, 6], [1805, 2], [1816, 5], [1829, 11], [1837, 3], [1848, 2], [1860, 2], [1870, 8], [1883, 12], [1894, 1], [1906, 2], [1917, 8], [1928, 4], [1937, 4], [1947, 5], [1958, 3], [1968, 11], [1979, 12], [1989, 11], [2001, 11]], 'solar_max': [154.27083333333334, 144.12499999999997, 192.97916666666666, 264.25, 235.28333333333333, 67.92916666666666, 81.99166666666667, 81.16250000000001, 119.24166666666667, 244.87083333333328, 219.94166666666663, 186.15416666666667, 234.0208333333333, 124.41250000000002, 146.54583333333332, 107.07916666666667, 175.66666666666663, 130.22916666666666, 198.64166666666665, 218.73333333333332, 285.00416666666666, 156.62916666666666, 232.9166666666666, 212.48333333333332, 180.275], 'length': [72, 135, 107, 110, 162, 151, 148, 125, 115, 148, 134, 140, 134, 141, 137, 120, 120, 124, 121, 125, 136, 125, 115, 150, 131]}
--------------------------------------------------
File location :
    SSN - /home/extern/Documents/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Documents/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
No pre-trained models available, initializing model weights
--------------------------------------------------
Training model with: num_epochs=1800, start_lr=0.0005
Epoch [   1/1800] -> Loss: 86.5580
Epoch [   2/1800] -> Loss: 61.0003
Epoch [   3/1800] -> Loss: 41.8430
Epoch [   4/1800] -> Loss: 40.5333
Epoch [   5/1800] -> Loss: 40.5303
Epoch [   6/1800] -> Loss: 40.4631
Epoch [   7/1800] -> Loss: 40.3344
Epoch [   8/1800] -> Loss: 40.2290
Epoch [   9/1800] -> Loss: 40.2610
Epoch [  10/1800] -> Loss: 40.2179
Epoch [  11/1800] -> Loss: 40.1570
Epoch [  12/1800] -> Loss: 40.0532
Epoch [  13/1800] -> Loss: 39.9162
Epoch [  14/1800] -> Loss: 39.8651
Epoch [  15/1800] -> Loss: 39.8055
Epoch [  16/1800] -> Loss: 39.7041
Epoch [  17/1800] -> Loss: 39.6783
Epoch [  18/1800] -> Loss: 39.5326
Epoch [  19/1800] -> Loss: 39.5517
Epoch [  20/1800] -> Loss: 39.4658
Epoch [  21/1800] -> Loss: 39.4525
Epoch [  22/1800] -> Loss: 39.1995
Epoch [  23/1800] -> Loss: 39.1376
Epoch [  24/1800] -> Loss: 39.0286
Epoch [  25/1800] -> Loss: 38.8419
Epoch [  26/1800] -> Loss: 38.7300
Epoch [  27/1800] -> Loss: 38.6914
Epoch [  28/1800] -> Loss: 38.5097
Epoch [  29/1800] -> Loss: 38.4430
Epoch [  30/1800] -> Loss: 38.2451
Epoch [  31/1800] -> Loss: 38.1664
Epoch [  32/1800] -> Loss: 38.1217
Epoch [  33/1800] -> Loss: 37.8692
Epoch [  34/1800] -> Loss: 37.7110
Epoch [  35/1800] -> Loss: 37.6251
Epoch [  36/1800] -> Loss: 37.4319
Epoch [  37/1800] -> Loss: 37.3336
Epoch [  38/1800] -> Loss: 37.2305
Epoch [  39/1800] -> Loss: 36.9403
Epoch [  40/1800] -> Loss: 36.9470
Epoch [  41/1800] -> Loss: 36.6948
Epoch [  42/1800] -> Loss: 36.5838
Epoch [  43/1800] -> Loss: 36.3300
Epoch [  44/1800] -> Loss: 36.2438
Epoch [  45/1800] -> Loss: 35.9923
Epoch [  46/1800] -> Loss: 35.9075
Epoch [  47/1800] -> Loss: 35.8260
Epoch [  48/1800] -> Loss: 35.6430
Epoch [  49/1800] -> Loss: 35.4560
Epoch [  50/1800] -> Loss: 35.3072
Epoch [  51/1800] -> Loss: 35.1946
Epoch [  52/1800] -> Loss: 35.0867
Epoch [  53/1800] -> Loss: 34.7656
Epoch [  54/1800] -> Loss: 34.7625
Epoch [  55/1800] -> Loss: 34.6364
Epoch [  56/1800] -> Loss: 34.4838
Epoch [  57/1800] -> Loss: 34.2744
Epoch [  58/1800] -> Loss: 34.2497
Epoch [  59/1800] -> Loss: 34.1212
Epoch [  60/1800] -> Loss: 33.8136
Epoch [  61/1800] -> Loss: 33.7656
Epoch [  62/1800] -> Loss: 33.8000
Epoch [  63/1800] -> Loss: 33.6656
Epoch [  64/1800] -> Loss: 33.4477
Epoch [  65/1800] -> Loss: 33.3302
Epoch [  66/1800] -> Loss: 33.4305
Epoch [  67/1800] -> Loss: 33.1610
Epoch [  68/1800] -> Loss: 33.1037
Epoch [  69/1800] -> Loss: 33.0338
Epoch [  70/1800] -> Loss: 32.7666
Epoch [  71/1800] -> Loss: 32.8623
Epoch [  72/1800] -> Loss: 32.6819
Epoch [  73/1800] -> Loss: 32.6073
Epoch [  74/1800] -> Loss: 32.5025
Epoch [  75/1800] -> Loss: 32.4800
Epoch [  76/1800] -> Loss: 32.2644
Epoch [  77/1800] -> Loss: 32.3781
Epoch [  78/1800] -> Loss: 32.1988
Epoch [  79/1800] -> Loss: 32.1745
Epoch [  80/1800] -> Loss: 32.0191
Epoch [  81/1800] -> Loss: 31.9481
Epoch [  82/1800] -> Loss: 31.9582
Epoch [  83/1800] -> Loss: 31.9472
Epoch [  84/1800] -> Loss: 31.7667
Epoch [  85/1800] -> Loss: 31.7758
Epoch [  86/1800] -> Loss: 31.6173
Epoch [  87/1800] -> Loss: 31.6450
Epoch [  88/1800] -> Loss: 31.5787
Epoch [  89/1800] -> Loss: 31.5225
Epoch [  90/1800] -> Loss: 31.4479
Epoch [  91/1800] -> Loss: 31.4799
Epoch [  92/1800] -> Loss: 31.4625
Epoch [  93/1800] -> Loss: 31.2873
Epoch [  94/1800] -> Loss: 31.3395
Epoch [  95/1800] -> Loss: 31.1924
Epoch [  96/1800] -> Loss: 31.1791
Epoch [  97/1800] -> Loss: 31.0802
Epoch [  98/1800] -> Loss: 31.1306
Epoch [  99/1800] -> Loss: 31.1053
--------------------------------------------------
Model checkpoint saved as FFNN_100.pth
--------------------------------------------------
Epoch [ 100/1800] -> Loss: 31.1959
Epoch [ 101/1800] -> Loss: 31.0183
Epoch [ 102/1800] -> Loss: 30.8938
Epoch [ 103/1800] -> Loss: 30.9668
Epoch [ 104/1800] -> Loss: 30.9464
Epoch [ 105/1800] -> Loss: 30.7918
Epoch [ 106/1800] -> Loss: 30.8755
Epoch [ 107/1800] -> Loss: 30.8151
Epoch [ 108/1800] -> Loss: 30.7227
Epoch [ 109/1800] -> Loss: 30.7060
Epoch [ 110/1800] -> Loss: 30.6556
Epoch [ 111/1800] -> Loss: 30.6948
Epoch [ 112/1800] -> Loss: 30.5875
Epoch [ 113/1800] -> Loss: 30.6269
Epoch [ 114/1800] -> Loss: 30.5529
Epoch [ 115/1800] -> Loss: 30.5623
Epoch [ 116/1800] -> Loss: 30.4739
Epoch [ 117/1800] -> Loss: 30.5924
Epoch [ 118/1800] -> Loss: 30.4389
Epoch [ 119/1800] -> Loss: 30.4659
Epoch [ 120/1800] -> Loss: 30.6335
Epoch [ 121/1800] -> Loss: 30.4643
Epoch [ 122/1800] -> Loss: 30.3854
Epoch [ 123/1800] -> Loss: 30.3304
Epoch [ 124/1800] -> Loss: 30.3636
Epoch [ 125/1800] -> Loss: 30.3675
Epoch [ 126/1800] -> Loss: 30.3496
Epoch [ 127/1800] -> Loss: 30.2972
Epoch [ 128/1800] -> Loss: 30.2666
Epoch [ 129/1800] -> Loss: 30.2452
Epoch [ 130/1800] -> Loss: 30.3143
Epoch [ 131/1800] -> Loss: 30.2331
Epoch [ 132/1800] -> Loss: 30.2292
Epoch [ 133/1800] -> Loss: 30.2117
Epoch [ 134/1800] -> Loss: 30.2366
Epoch [ 135/1800] -> Loss: 30.2089
Epoch [ 136/1800] -> Loss: 30.2426
Epoch [ 137/1800] -> Loss: 30.1821
Epoch [ 138/1800] -> Loss: 30.0472
Epoch [ 139/1800] -> Loss: 30.1410
Epoch [ 140/1800] -> Loss: 30.0664
Epoch [ 141/1800] -> Loss: 29.9860
Epoch [ 142/1800] -> Loss: 30.1024
Epoch [ 143/1800] -> Loss: 29.9395
Epoch [ 144/1800] -> Loss: 29.9964
Epoch [ 145/1800] -> Loss: 30.1294
Epoch [ 146/1800] -> Loss: 29.9688
Epoch [ 147/1800] -> Loss: 30.1588
Epoch [ 148/1800] -> Loss: 29.9303
Epoch [ 149/1800] -> Loss: 29.8615
Epoch [ 150/1800] -> Loss: 29.9210
Epoch [ 151/1800] -> Loss: 29.9070
Epoch [ 152/1800] -> Loss: 29.8700
Epoch [ 153/1800] -> Loss: 29.8743
Epoch [ 154/1800] -> Loss: 29.9685
Epoch [ 155/1800] -> Loss: 29.8497
Epoch [ 156/1800] -> Loss: 29.8885
Epoch [ 157/1800] -> Loss: 29.8923
Epoch [ 158/1800] -> Loss: 29.8754
Epoch [ 159/1800] -> Loss: 29.8871
Epoch [ 160/1800] -> Loss: 29.7938
Epoch [ 161/1800] -> Loss: 29.8182
Epoch [ 162/1800] -> Loss: 29.7433
Epoch [ 163/1800] -> Loss: 29.8810
Epoch [ 164/1800] -> Loss: 29.8269
Epoch [ 165/1800] -> Loss: 29.6787
Epoch [ 166/1800] -> Loss: 29.7287
Epoch [ 167/1800] -> Loss: 29.6759
Epoch [ 168/1800] -> Loss: 29.6607
Epoch [ 169/1800] -> Loss: 29.6693
Epoch [ 170/1800] -> Loss: 29.7345
Epoch [ 171/1800] -> Loss: 29.6577
Epoch [ 172/1800] -> Loss: 29.6511
Epoch [ 173/1800] -> Loss: 29.6108
Epoch [ 174/1800] -> Loss: 29.7895
Epoch [ 175/1800] -> Loss: 29.8132
Epoch [ 176/1800] -> Loss: 29.6128
Epoch [ 177/1800] -> Loss: 29.7128
Epoch [ 178/1800] -> Loss: 29.6448
Epoch [ 179/1800] -> Loss: 29.6358
Epoch [ 180/1800] -> Loss: 29.5890
Epoch [ 181/1800] -> Loss: 29.4711
Epoch [ 182/1800] -> Loss: 29.4981
Epoch [ 183/1800] -> Loss: 29.5229
Epoch [ 184/1800] -> Loss: 29.4521
Epoch [ 185/1800] -> Loss: 29.4207
Epoch [ 186/1800] -> Loss: 29.3600
Epoch [ 187/1800] -> Loss: 29.3445
Epoch [ 188/1800] -> Loss: 29.4836
Epoch [ 189/1800] -> Loss: 29.4282
Epoch [ 190/1800] -> Loss: 29.3572
Epoch [ 191/1800] -> Loss: 29.4630
Epoch [ 192/1800] -> Loss: 29.4442
Epoch [ 193/1800] -> Loss: 29.3797
Epoch [ 194/1800] -> Loss: 29.3666
Epoch [ 195/1800] -> Loss: 29.3172
Epoch [ 196/1800] -> Loss: 29.3433
Epoch [ 197/1800] -> Loss: 29.2926
Epoch [ 198/1800] -> Loss: 29.2692
Epoch [ 199/1800] -> Loss: 29.3240
--------------------------------------------------
Model checkpoint saved as FFNN_200.pth
--------------------------------------------------
Epoch [ 200/1800] -> Loss: 29.3019
Epoch [ 201/1800] -> Loss: 29.3765
Epoch [ 202/1800] -> Loss: 29.3778
Epoch [ 203/1800] -> Loss: 29.2416
Epoch [ 204/1800] -> Loss: 29.2973
Epoch [ 205/1800] -> Loss: 29.2702
Epoch [ 206/1800] -> Loss: 29.2598
Epoch [ 207/1800] -> Loss: 29.3141
Epoch [ 208/1800] -> Loss: 29.1825
Epoch [ 209/1800] -> Loss: 29.2882
Epoch [ 210/1800] -> Loss: 29.1883
Epoch [ 211/1800] -> Loss: 29.2253
Epoch [ 212/1800] -> Loss: 29.2894
Epoch [ 213/1800] -> Loss: 29.1905
Epoch [ 214/1800] -> Loss: 29.2099
Epoch [ 215/1800] -> Loss: 29.1684
Epoch [ 216/1800] -> Loss: 29.0777
Epoch [ 217/1800] -> Loss: 29.2538
Epoch [ 218/1800] -> Loss: 29.1662
Epoch [ 219/1800] -> Loss: 29.1927
Epoch [ 220/1800] -> Loss: 29.1495
Epoch [ 221/1800] -> Loss: 29.1296
Epoch [ 222/1800] -> Loss: 29.1911
Epoch [ 223/1800] -> Loss: 29.1324
Epoch [ 224/1800] -> Loss: 29.1510
Epoch [ 225/1800] -> Loss: 29.2217
Epoch [ 226/1800] -> Loss: 29.1913
Epoch   227: reducing learning rate of group 0 to 2.5000e-04.
Epoch [ 227/1800] -> Loss: 29.1425
Epoch [ 228/1800] -> Loss: 29.1562
Epoch [ 229/1800] -> Loss: 29.1857
Epoch [ 230/1800] -> Loss: 29.0525
Epoch [ 231/1800] -> Loss: 29.1218
Epoch [ 232/1800] -> Loss: 29.0514
Epoch [ 233/1800] -> Loss: 29.1136
Epoch [ 234/1800] -> Loss: 29.1070
Epoch [ 235/1800] -> Loss: 29.1844
Epoch [ 236/1800] -> Loss: 29.0390
Epoch [ 237/1800] -> Loss: 28.9898
Epoch [ 238/1800] -> Loss: 29.0528
Epoch [ 239/1800] -> Loss: 29.1121
Epoch [ 240/1800] -> Loss: 29.1276
Epoch [ 241/1800] -> Loss: 29.1275
Epoch [ 242/1800] -> Loss: 29.1059
Epoch [ 243/1800] -> Loss: 29.0780
Epoch [ 244/1800] -> Loss: 29.0416
Epoch [ 245/1800] -> Loss: 29.0408
Epoch [ 246/1800] -> Loss: 29.0343
Epoch [ 247/1800] -> Loss: 28.9776
Epoch [ 248/1800] -> Loss: 29.0539
Epoch [ 249/1800] -> Loss: 29.0135
Epoch [ 250/1800] -> Loss: 29.0314
Epoch [ 251/1800] -> Loss: 28.9371
Epoch [ 252/1800] -> Loss: 29.0070
Epoch [ 253/1800] -> Loss: 29.0371
Epoch [ 254/1800] -> Loss: 28.9928
Epoch [ 255/1800] -> Loss: 29.0237
Epoch [ 256/1800] -> Loss: 29.0510
Epoch [ 257/1800] -> Loss: 29.0463
Epoch [ 258/1800] -> Loss: 29.1048
Epoch [ 259/1800] -> Loss: 29.0321
Epoch [ 260/1800] -> Loss: 29.0037
Epoch [ 261/1800] -> Loss: 29.0327
Epoch   262: reducing learning rate of group 0 to 1.2500e-04.
Epoch [ 262/1800] -> Loss: 28.9869
Epoch [ 263/1800] -> Loss: 28.9013
Epoch [ 264/1800] -> Loss: 29.0288
Epoch [ 265/1800] -> Loss: 29.0029
Epoch [ 266/1800] -> Loss: 28.9579
Epoch [ 267/1800] -> Loss: 28.9706
Epoch [ 268/1800] -> Loss: 28.9639
Epoch [ 269/1800] -> Loss: 28.9504
Epoch [ 270/1800] -> Loss: 28.9546
Epoch [ 271/1800] -> Loss: 28.9382
Epoch [ 272/1800] -> Loss: 29.0373
Epoch [ 273/1800] -> Loss: 29.1145
Epoch   274: reducing learning rate of group 0 to 6.2500e-05.
Epoch [ 274/1800] -> Loss: 28.9341
Epoch [ 275/1800] -> Loss: 28.9637
Epoch [ 276/1800] -> Loss: 28.8982
Epoch [ 277/1800] -> Loss: 28.9097
Epoch [ 278/1800] -> Loss: 28.9456
Epoch [ 279/1800] -> Loss: 29.0085
Epoch [ 280/1800] -> Loss: 28.8898
Epoch [ 281/1800] -> Loss: 28.9014
Epoch [ 282/1800] -> Loss: 28.9485
Epoch [ 283/1800] -> Loss: 28.9555
Epoch [ 284/1800] -> Loss: 28.9469
Epoch [ 285/1800] -> Loss: 28.9531
Epoch [ 286/1800] -> Loss: 28.9177
Epoch [ 287/1800] -> Loss: 28.9263
Epoch [ 288/1800] -> Loss: 28.9290
Epoch [ 289/1800] -> Loss: 28.9669
Epoch [ 290/1800] -> Loss: 28.9346
Epoch   291: reducing learning rate of group 0 to 3.1250e-05.
Epoch [ 291/1800] -> Loss: 28.9399
Epoch [ 292/1800] -> Loss: 28.8711
Epoch [ 293/1800] -> Loss: 28.9558
Epoch [ 294/1800] -> Loss: 28.9509
Epoch [ 295/1800] -> Loss: 28.9020
Epoch [ 296/1800] -> Loss: 28.8832
Epoch [ 297/1800] -> Loss: 28.8984
Epoch [ 298/1800] -> Loss: 28.8967
Epoch [ 299/1800] -> Loss: 28.8745
--------------------------------------------------
Model checkpoint saved as FFNN_300.pth
--------------------------------------------------
Epoch [ 300/1800] -> Loss: 28.9180
Epoch [ 301/1800] -> Loss: 28.9081
Epoch [ 302/1800] -> Loss: 28.9169
Epoch   303: reducing learning rate of group 0 to 1.5625e-05.
Epoch [ 303/1800] -> Loss: 28.9187
Epoch [ 304/1800] -> Loss: 28.9283
Epoch [ 305/1800] -> Loss: 28.9514
Epoch [ 306/1800] -> Loss: 28.8787
Epoch [ 307/1800] -> Loss: 28.9736
Epoch [ 308/1800] -> Loss: 28.8669
Epoch [ 309/1800] -> Loss: 28.8611
Epoch [ 310/1800] -> Loss: 28.8668
Epoch [ 311/1800] -> Loss: 28.8577
Epoch [ 312/1800] -> Loss: 28.8978
Epoch [ 313/1800] -> Loss: 28.8399
Epoch [ 314/1800] -> Loss: 28.8611
Epoch [ 315/1800] -> Loss: 28.8883
Epoch [ 316/1800] -> Loss: 28.8592
Epoch [ 317/1800] -> Loss: 28.8912
