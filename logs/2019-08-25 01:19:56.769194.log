--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
Dataset source : NOAA, ISGI
File location :
    SSN - /home/extern/Documents/Research/data/NOAA/table_international-sunspot-numbers_monthly.txt
    AA - /home/extern/Documents/Research/data/ISGI/aa_1869-08-01_2017-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
Pre-trained model available, loading model weights
--------------------------------------------------
Training model with: num_epochs=1800, start_lr=0.0005
Epoch [   1/1800] -> Loss: 1174.2663
Epoch [   2/1800] -> Loss: 1172.3364
Epoch [   3/1800] -> Loss: 1176.4967
Epoch [   4/1800] -> Loss: 1171.3479
Epoch [   5/1800] -> Loss: 1171.3737
Epoch [   6/1800] -> Loss: 1170.2855
Epoch [   7/1800] -> Loss: 1170.5645
Epoch [   8/1800] -> Loss: 1168.1574
Epoch [   9/1800] -> Loss: 1172.6069
Epoch [  10/1800] -> Loss: 1169.3368
Epoch [  11/1800] -> Loss: 1169.5459
Epoch [  12/1800] -> Loss: 1168.2793
Epoch [  13/1800] -> Loss: 1169.1496
Epoch [  14/1800] -> Loss: 1168.7444
Epoch [  15/1800] -> Loss: 1167.5124
Epoch [  16/1800] -> Loss: 1166.4312
Epoch [  17/1800] -> Loss: 1167.6722
Epoch [  18/1800] -> Loss: 1168.7924
Epoch [  19/1800] -> Loss: 1165.5366
Epoch [  20/1800] -> Loss: 1174.7243
Epoch [  21/1800] -> Loss: 1168.0837
Epoch [  22/1800] -> Loss: 1167.1795
Epoch [  23/1800] -> Loss: 1169.9724
Epoch [  24/1800] -> Loss: 1166.6688
Epoch [  25/1800] -> Loss: 1168.3272
Epoch [  26/1800] -> Loss: 1166.9291
Epoch [  27/1800] -> Loss: 1165.7389
Epoch [  28/1800] -> Loss: 1167.4540
Epoch [  29/1800] -> Loss: 1169.2708
Epoch    29: reducing learning rate of group 0 to 2.5000e-04.
Epoch [  30/1800] -> Loss: 1166.2693
Epoch [  31/1800] -> Loss: 1165.4278
Epoch [  32/1800] -> Loss: 1165.2193
Epoch [  33/1800] -> Loss: 1164.6204
Epoch [  34/1800] -> Loss: 1164.6933
Epoch [  35/1800] -> Loss: 1164.6559
Epoch [  36/1800] -> Loss: 1165.6261
Epoch [  37/1800] -> Loss: 1164.9849
Epoch [  38/1800] -> Loss: 1164.5594
Epoch [  39/1800] -> Loss: 1164.3035
Epoch [  40/1800] -> Loss: 1164.3321
Epoch [  41/1800] -> Loss: 1164.1155
Epoch [  42/1800] -> Loss: 1165.0507
Epoch [  43/1800] -> Loss: 1165.2710
Epoch [  44/1800] -> Loss: 1165.1886
Epoch [  45/1800] -> Loss: 1164.0577
Epoch [  46/1800] -> Loss: 1164.0173
Epoch [  47/1800] -> Loss: 1164.6618
Epoch [  48/1800] -> Loss: 1164.3539
Epoch [  49/1800] -> Loss: 1164.1150
--------------------------------------------------
Model checkpoint saved as FFNN_50.pth
--------------------------------------------------
Epoch [  50/1800] -> Loss: 1163.6623
Epoch [  51/1800] -> Loss: 1164.2183
Epoch [  52/1800] -> Loss: 1163.5876
Epoch [  53/1800] -> Loss: 1164.0006
Epoch [  54/1800] -> Loss: 1164.2693
Epoch [  55/1800] -> Loss: 1164.4018
Epoch [  56/1800] -> Loss: 1165.3890
Epoch [  57/1800] -> Loss: 1163.6593
Epoch [  58/1800] -> Loss: 1164.4966
Epoch [  59/1800] -> Loss: 1164.5275
Epoch [  60/1800] -> Loss: 1164.4073
Epoch    60: reducing learning rate of group 0 to 1.2500e-04.
Epoch [  61/1800] -> Loss: 1164.2670
Epoch [  62/1800] -> Loss: 1162.8632
Epoch [  63/1800] -> Loss: 1162.5878
Epoch [  64/1800] -> Loss: 1162.7186
Epoch [  65/1800] -> Loss: 1162.7984
Epoch [  66/1800] -> Loss: 1162.2803
Epoch [  67/1800] -> Loss: 1162.6846
Epoch [  68/1800] -> Loss: 1162.8168
Epoch [  69/1800] -> Loss: 1162.5843
Epoch [  70/1800] -> Loss: 1162.4330
Epoch [  71/1800] -> Loss: 1162.4878
Epoch [  72/1800] -> Loss: 1162.8660
Epoch [  73/1800] -> Loss: 1162.6211
Epoch [  74/1800] -> Loss: 1161.9942
Epoch [  75/1800] -> Loss: 1161.9836
Epoch [  76/1800] -> Loss: 1162.7286
Epoch [  77/1800] -> Loss: 1162.5104
Epoch [  78/1800] -> Loss: 1162.6280
Epoch [  79/1800] -> Loss: 1162.2540
Epoch [  80/1800] -> Loss: 1162.1413
Epoch [  81/1800] -> Loss: 1162.5985
Epoch [  82/1800] -> Loss: 1162.5642
Epoch [  83/1800] -> Loss: 1162.5313
Epoch [  84/1800] -> Loss: 1162.7306
Epoch    84: reducing learning rate of group 0 to 6.2500e-05.
Epoch [  85/1800] -> Loss: 1162.3075
Epoch [  86/1800] -> Loss: 1161.9255
Epoch [  87/1800] -> Loss: 1161.9213
Epoch [  88/1800] -> Loss: 1161.9004
Epoch [  89/1800] -> Loss: 1161.8940
Epoch [  90/1800] -> Loss: 1161.8115
Epoch [  91/1800] -> Loss: 1162.2776
Epoch [  92/1800] -> Loss: 1161.6884
Epoch [  93/1800] -> Loss: 1161.9508
Epoch [  94/1800] -> Loss: 1161.7963
Epoch [  95/1800] -> Loss: 1161.7911
Epoch [  96/1800] -> Loss: 1161.8217
