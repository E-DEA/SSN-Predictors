--------------------------------------------------
Code running on device: cuda
--------------------------------------------------
Dataset source : NOAA, ISGI
File location :
    SSN - /home/extern/Documents/Research/data/NOAA/table_international-sunspot-numbers_monthly.txt
    AA - /home/extern/Documents/Research/data/ISGI/aa_1869-08-01_2017-12-31_D.dat
--------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace)
    (2): Linear(in_features=6, out_features=6, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace)
    (4): Linear(in_features=6, out_features=1, bias=True)
  )
)    Training mode: True    Testing mode: False
--------------------------------------------------
Pre-trained model available, loading model weights
--------------------------------------------------
Training model with: num_epochs=450, start_lr=0.0005
Epoch [   1/450] -> Loss: 302.0457
Epoch [   2/450] -> Loss: 302.5787
Epoch [   3/450] -> Loss: 299.9466
Epoch [   4/450] -> Loss: 300.2812
Epoch [   5/450] -> Loss: 301.8969
Epoch [   6/450] -> Loss: 301.4449
Epoch [   7/450] -> Loss: 300.5105
Epoch [   8/450] -> Loss: 302.3337
Epoch [   9/450] -> Loss: 301.5511
Epoch [  10/450] -> Loss: 301.0357
Epoch [  11/450] -> Loss: 301.1672
Epoch [  12/450] -> Loss: 301.9527
Epoch [  13/450] -> Loss: 301.3605
Epoch    13: reducing learning rate of group 0 to 2.5000e-04.
Epoch [  14/450] -> Loss: 302.7103
Epoch [  15/450] -> Loss: 300.4522
Epoch [  16/450] -> Loss: 300.7449
Epoch [  17/450] -> Loss: 300.1788
Epoch [  18/450] -> Loss: 299.8374
Epoch [  19/450] -> Loss: 300.2993
Epoch [  20/450] -> Loss: 300.4448
Epoch [  21/450] -> Loss: 299.8442
Epoch [  22/450] -> Loss: 300.0381
Epoch [  23/450] -> Loss: 299.5901
Epoch [  24/450] -> Loss: 299.4606
Epoch [  25/450] -> Loss: 300.0307
Epoch [  26/450] -> Loss: 299.9288
Epoch [  27/450] -> Loss: 299.7584
Epoch [  28/450] -> Loss: 300.0598
Epoch [  29/450] -> Loss: 299.6755
Epoch [  30/450] -> Loss: 300.4855
Epoch [  31/450] -> Loss: 299.1750
Epoch [  32/450] -> Loss: 300.3185
Epoch [  33/450] -> Loss: 299.9466
Epoch [  34/450] -> Loss: 299.7469
Epoch [  35/450] -> Loss: 299.7895
Epoch [  36/450] -> Loss: 299.6973
Epoch [  37/450] -> Loss: 300.0416
Epoch [  38/450] -> Loss: 299.5701
Epoch [  39/450] -> Loss: 299.8159
Epoch [  40/450] -> Loss: 299.3903
Epoch [  41/450] -> Loss: 300.0428
Epoch [  42/450] -> Loss: 298.6409
Epoch [  43/450] -> Loss: 299.7431
Epoch [  44/450] -> Loss: 299.9707
Epoch [  45/450] -> Loss: 300.3830
Epoch [  46/450] -> Loss: 299.7708
Epoch [  47/450] -> Loss: 300.1587
Epoch [  48/450] -> Loss: 299.5873
Epoch [  49/450] -> Loss: 299.5369
--------------------------------------------------
Model checkpoint saved as FFNN_50.pth
--------------------------------------------------
Epoch [  50/450] -> Loss: 300.0829
Epoch [  51/450] -> Loss: 300.2252
Epoch [  52/450] -> Loss: 298.9847
Epoch    52: reducing learning rate of group 0 to 1.2500e-04.
Epoch [  53/450] -> Loss: 299.4500
Epoch [  54/450] -> Loss: 298.9821
Epoch [  55/450] -> Loss: 299.1752
Epoch [  56/450] -> Loss: 299.2290
Epoch [  57/450] -> Loss: 298.8595
Epoch [  58/450] -> Loss: 298.9959
Epoch [  59/450] -> Loss: 299.1552
Epoch [  60/450] -> Loss: 298.9074
Epoch [  61/450] -> Loss: 298.8694
Epoch [  62/450] -> Loss: 298.9544
Epoch [  63/450] -> Loss: 299.0006
Epoch    63: reducing learning rate of group 0 to 6.2500e-05.
Epoch [  64/450] -> Loss: 299.0477
Epoch [  65/450] -> Loss: 298.6857
Epoch [  66/450] -> Loss: 298.6198
Epoch [  67/450] -> Loss: 298.5774
Epoch [  68/450] -> Loss: 298.6575
Epoch [  69/450] -> Loss: 298.5464
Epoch [  70/450] -> Loss: 298.6767
Epoch [  71/450] -> Loss: 298.5930
Epoch [  72/450] -> Loss: 298.5418
Epoch [  73/450] -> Loss: 298.5804
Epoch [  74/450] -> Loss: 298.6418
Epoch [  75/450] -> Loss: 298.7784
Epoch [  76/450] -> Loss: 298.5043
Epoch [  77/450] -> Loss: 298.5527
Epoch [  78/450] -> Loss: 298.5274
Epoch [  79/450] -> Loss: 298.4311
Epoch [  80/450] -> Loss: 298.7768
Epoch [  81/450] -> Loss: 298.5015
Epoch [  82/450] -> Loss: 298.5871
Epoch [  83/450] -> Loss: 298.5047
Epoch [  84/450] -> Loss: 298.4675
Epoch [  85/450] -> Loss: 298.5816
Epoch [  86/450] -> Loss: 298.5903
Epoch [  87/450] -> Loss: 298.5760
Epoch [  88/450] -> Loss: 298.6069
Epoch [  89/450] -> Loss: 298.5717
Epoch    89: reducing learning rate of group 0 to 3.1250e-05.
Epoch [  90/450] -> Loss: 298.4795
Epoch [  91/450] -> Loss: 298.3749
Epoch [  92/450] -> Loss: 298.3831
Epoch [  93/450] -> Loss: 298.3714
Epoch [  94/450] -> Loss: 298.3693
Epoch [  95/450] -> Loss: 298.3467
Epoch [  96/450] -> Loss: 298.3245
Epoch [  97/450] -> Loss: 298.3976
Epoch [  98/450] -> Loss: 298.3166
Epoch [  99/450] -> Loss: 298.3513
--------------------------------------------------
Model checkpoint saved as FFNN_100.pth
--------------------------------------------------
Epoch [ 100/450] -> Loss: 298.3435
Epoch [ 101/450] -> Loss: 298.3266
Epoch [ 102/450] -> Loss: 298.2792
Epoch [ 103/450] -> Loss: 298.3207
Epoch [ 104/450] -> Loss: 298.2827
Epoch [ 105/450] -> Loss: 298.3329
Epoch [ 106/450] -> Loss: 298.3598
Epoch [ 107/450] -> Loss: 298.3122
Epoch [ 108/450] -> Loss: 298.3243
Epoch [ 109/450] -> Loss: 298.3214
Epoch [ 110/450] -> Loss: 298.3515
Epoch [ 111/450] -> Loss: 298.2827
Epoch [ 112/450] -> Loss: 298.3292
Epoch   112: reducing learning rate of group 0 to 1.5625e-05.
Epoch [ 113/450] -> Loss: 298.3946
Epoch [ 114/450] -> Loss: 298.1956
Epoch [ 115/450] -> Loss: 298.1819
Epoch [ 116/450] -> Loss: 298.2265
Epoch [ 117/450] -> Loss: 298.1901
Epoch [ 118/450] -> Loss: 298.2067
Epoch [ 119/450] -> Loss: 298.2326
Epoch [ 120/450] -> Loss: 298.1865
Epoch [ 121/450] -> Loss: 298.2248
Epoch [ 122/450] -> Loss: 298.2027
Epoch [ 123/450] -> Loss: 298.2189
Epoch [ 124/450] -> Loss: 298.1999
Epoch   124: reducing learning rate of group 0 to 7.8125e-06.
Epoch [ 125/450] -> Loss: 298.1952
Epoch [ 126/450] -> Loss: 298.1314
Epoch [ 127/450] -> Loss: 298.1403
Epoch [ 128/450] -> Loss: 298.1232
Epoch [ 129/450] -> Loss: 298.1393
Epoch [ 130/450] -> Loss: 298.1172
Epoch [ 131/450] -> Loss: 298.1315
Epoch [ 132/450] -> Loss: 298.1490
Epoch [ 133/450] -> Loss: 298.1218
Epoch [ 134/450] -> Loss: 298.1187
Epoch [ 135/450] -> Loss: 298.1586
Epoch [ 136/450] -> Loss: 298.1472
Epoch   136: reducing learning rate of group 0 to 3.9063e-06.
Epoch [ 137/450] -> Loss: 298.1346
Epoch [ 138/450] -> Loss: 298.1128
Epoch [ 139/450] -> Loss: 298.0973
Epoch [ 140/450] -> Loss: 298.0985
Epoch [ 141/450] -> Loss: 298.0979
Epoch [ 142/450] -> Loss: 298.1154
Epoch [ 143/450] -> Loss: 298.1004
Epoch [ 144/450] -> Loss: 298.1105
Epoch [ 145/450] -> Loss: 298.0981
Epoch [ 146/450] -> Loss: 298.0934
Epoch [ 147/450] -> Loss: 298.0986
Epoch [ 148/450] -> Loss: 298.0967
Epoch [ 149/450] -> Loss: 298.0940
Epoch   149: reducing learning rate of group 0 to 1.9531e-06.
--------------------------------------------------
Model checkpoint saved as FFNN_150.pth
--------------------------------------------------
Epoch [ 150/450] -> Loss: 298.0989
Epoch [ 151/450] -> Loss: 298.0822
Epoch [ 152/450] -> Loss: 298.0802
Epoch [ 153/450] -> Loss: 298.0850
Epoch [ 154/450] -> Loss: 298.0849
Epoch [ 155/450] -> Loss: 298.0848
Epoch [ 156/450] -> Loss: 298.0853
Epoch [ 157/450] -> Loss: 298.0838
Epoch [ 158/450] -> Loss: 298.0809
Epoch [ 159/450] -> Loss: 298.0795
Epoch [ 160/450] -> Loss: 298.0869
Epoch   160: reducing learning rate of group 0 to 9.7656e-07.
Epoch [ 161/450] -> Loss: 298.0885
Epoch [ 162/450] -> Loss: 298.0751
Epoch [ 163/450] -> Loss: 298.0740
Epoch [ 164/450] -> Loss: 298.0755
Epoch [ 165/450] -> Loss: 298.0758
Epoch [ 166/450] -> Loss: 298.0736
Epoch [ 167/450] -> Loss: 298.0761
Epoch [ 168/450] -> Loss: 298.0753
Epoch [ 169/450] -> Loss: 298.0754
Epoch [ 170/450] -> Loss: 298.0729
Epoch [ 171/450] -> Loss: 298.0734
Epoch   171: reducing learning rate of group 0 to 4.8828e-07.
Epoch [ 172/450] -> Loss: 298.0723
Epoch [ 173/450] -> Loss: 298.0682
Epoch [ 174/450] -> Loss: 298.0703
Epoch [ 175/450] -> Loss: 298.0698
Epoch [ 176/450] -> Loss: 298.0699
Epoch [ 177/450] -> Loss: 298.0705
Epoch [ 178/450] -> Loss: 298.0697
Epoch [ 179/450] -> Loss: 298.0699
Epoch [ 180/450] -> Loss: 298.0700
Epoch [ 181/450] -> Loss: 298.0713
Epoch [ 182/450] -> Loss: 298.0696
Epoch   182: reducing learning rate of group 0 to 2.4414e-07.
Epoch [ 183/450] -> Loss: 298.0697
Epoch [ 184/450] -> Loss: 298.0678
Epoch [ 185/450] -> Loss: 298.0678
Epoch [ 186/450] -> Loss: 298.0677
Epoch [ 187/450] -> Loss: 298.0679
Epoch [ 188/450] -> Loss: 298.0679
Epoch [ 189/450] -> Loss: 298.0680
Epoch [ 190/450] -> Loss: 298.0673
Epoch [ 191/450] -> Loss: 298.0683
Epoch [ 192/450] -> Loss: 298.0675
Epoch [ 193/450] -> Loss: 298.0676
Epoch [ 194/450] -> Loss: 298.0673
Epoch [ 195/450] -> Loss: 298.0676
Epoch [ 196/450] -> Loss: 298.0680
Epoch [ 197/450] -> Loss: 298.0676
Epoch [ 198/450] -> Loss: 298.0679
Epoch [ 199/450] -> Loss: 298.0675
--------------------------------------------------
Model checkpoint saved as FFNN_200.pth
--------------------------------------------------
Epoch [ 200/450] -> Loss: 298.0673
Epoch   200: reducing learning rate of group 0 to 1.2207e-07.
Epoch [ 201/450] -> Loss: 298.0676
Epoch [ 202/450] -> Loss: 298.0667
Epoch [ 203/450] -> Loss: 298.0666
Epoch [ 204/450] -> Loss: 298.0666
Epoch [ 205/450] -> Loss: 298.0667
Epoch [ 206/450] -> Loss: 298.0666
Epoch [ 207/450] -> Loss: 298.0665
Epoch [ 208/450] -> Loss: 298.0668
Epoch [ 209/450] -> Loss: 298.0666
Epoch [ 210/450] -> Loss: 298.0666
Epoch [ 211/450] -> Loss: 298.0667
Epoch   211: reducing learning rate of group 0 to 6.1035e-08.
Epoch [ 212/450] -> Loss: 298.0665
Epoch [ 213/450] -> Loss: 298.0660
Epoch [ 214/450] -> Loss: 298.0658
Epoch [ 215/450] -> Loss: 298.0659
Epoch [ 216/450] -> Loss: 298.0664
Epoch [ 217/450] -> Loss: 298.0660
Epoch [ 218/450] -> Loss: 298.0659
Epoch [ 219/450] -> Loss: 298.0660
Epoch [ 220/450] -> Loss: 298.0663
Epoch [ 221/450] -> Loss: 298.0660
Epoch [ 222/450] -> Loss: 298.0660
Epoch   222: reducing learning rate of group 0 to 3.0518e-08.
Epoch [ 223/450] -> Loss: 298.0662
Epoch [ 224/450] -> Loss: 298.0658
Epoch [ 225/450] -> Loss: 298.0657
Epoch [ 226/450] -> Loss: 298.0658
Epoch [ 227/450] -> Loss: 298.0657
Epoch [ 228/450] -> Loss: 298.0657
Epoch [ 229/450] -> Loss: 298.0658
Epoch [ 230/450] -> Loss: 298.0657
Epoch [ 231/450] -> Loss: 298.0657
Epoch [ 232/450] -> Loss: 298.0657
Epoch [ 233/450] -> Loss: 298.0657
Epoch   233: reducing learning rate of group 0 to 1.5259e-08.
Epoch [ 234/450] -> Loss: 298.0657
Epoch [ 235/450] -> Loss: 298.0657
Epoch [ 236/450] -> Loss: 298.0657
Epoch [ 237/450] -> Loss: 298.0657
Epoch [ 238/450] -> Loss: 298.0657
Epoch [ 239/450] -> Loss: 298.0657
Epoch [ 240/450] -> Loss: 298.0657
Epoch [ 241/450] -> Loss: 298.0657
Epoch [ 242/450] -> Loss: 298.0657
Epoch [ 243/450] -> Loss: 298.0657
Epoch [ 244/450] -> Loss: 298.0657
Epoch [ 245/450] -> Loss: 298.0657
Epoch [ 246/450] -> Loss: 298.0657
Epoch [ 247/450] -> Loss: 298.0657
Epoch [ 248/450] -> Loss: 298.0657
Epoch [ 249/450] -> Loss: 298.0657
--------------------------------------------------
Model checkpoint saved as FFNN_250.pth
--------------------------------------------------
Epoch [ 250/450] -> Loss: 298.0656
Epoch [ 251/450] -> Loss: 298.0657
Epoch [ 252/450] -> Loss: 298.0657
Epoch [ 253/450] -> Loss: 298.0657
Epoch [ 254/450] -> Loss: 298.0657
Epoch [ 255/450] -> Loss: 298.0657
Epoch [ 256/450] -> Loss: 298.0657
Epoch [ 257/450] -> Loss: 298.0657
Epoch [ 258/450] -> Loss: 298.0657
Epoch [ 259/450] -> Loss: 298.0657
Epoch [ 260/450] -> Loss: 298.0657
Epoch [ 261/450] -> Loss: 298.0657
Epoch [ 262/450] -> Loss: 298.0657
Epoch [ 263/450] -> Loss: 298.0657
Epoch [ 264/450] -> Loss: 298.0657
Epoch [ 265/450] -> Loss: 298.0657
Epoch [ 266/450] -> Loss: 298.0657
Epoch [ 267/450] -> Loss: 298.0657
Epoch [ 268/450] -> Loss: 298.0657
Epoch [ 269/450] -> Loss: 298.0657
Epoch [ 270/450] -> Loss: 298.0657
Epoch [ 271/450] -> Loss: 298.0657
Epoch [ 272/450] -> Loss: 298.0656
Epoch [ 273/450] -> Loss: 298.0657
Epoch [ 274/450] -> Loss: 298.0657
Epoch [ 275/450] -> Loss: 298.0657
Epoch [ 276/450] -> Loss: 298.0657
Epoch [ 277/450] -> Loss: 298.0657
Epoch [ 278/450] -> Loss: 298.0657
Epoch [ 279/450] -> Loss: 298.0656
Epoch [ 280/450] -> Loss: 298.0657
Epoch [ 281/450] -> Loss: 298.0657
Epoch [ 282/450] -> Loss: 298.0657
Epoch [ 283/450] -> Loss: 298.0657
Epoch [ 284/450] -> Loss: 298.0657
Epoch [ 285/450] -> Loss: 298.0657
Epoch [ 286/450] -> Loss: 298.0657
Epoch [ 287/450] -> Loss: 298.0657
Epoch [ 288/450] -> Loss: 298.0657
Epoch [ 289/450] -> Loss: 298.0657
Epoch [ 290/450] -> Loss: 298.0657
Epoch [ 291/450] -> Loss: 298.0657
Epoch [ 292/450] -> Loss: 298.0657
Epoch [ 293/450] -> Loss: 298.0656
Epoch [ 294/450] -> Loss: 298.0657
Epoch [ 295/450] -> Loss: 298.0657
Epoch [ 296/450] -> Loss: 298.0656
Epoch [ 297/450] -> Loss: 298.0657
Epoch [ 298/450] -> Loss: 298.0657
Epoch [ 299/450] -> Loss: 298.0657
--------------------------------------------------
Model checkpoint saved as FFNN_300.pth
--------------------------------------------------
Epoch [ 300/450] -> Loss: 298.0657
Epoch [ 301/450] -> Loss: 298.0657
Epoch [ 302/450] -> Loss: 298.0657
Epoch [ 303/450] -> Loss: 298.0657
Epoch [ 304/450] -> Loss: 298.0657
Epoch [ 305/450] -> Loss: 298.0657
Epoch [ 306/450] -> Loss: 298.0657
Epoch [ 307/450] -> Loss: 298.0656
Epoch [ 308/450] -> Loss: 298.0656
Epoch [ 309/450] -> Loss: 298.0657
Epoch [ 310/450] -> Loss: 298.0657
Epoch [ 311/450] -> Loss: 298.0657
Epoch [ 312/450] -> Loss: 298.0657
Epoch [ 313/450] -> Loss: 298.0657
Epoch [ 314/450] -> Loss: 298.0657
Epoch [ 315/450] -> Loss: 298.0657
Epoch [ 316/450] -> Loss: 298.0657
Epoch [ 317/450] -> Loss: 298.0657
Epoch [ 318/450] -> Loss: 298.0656
Epoch [ 319/450] -> Loss: 298.0657
Epoch [ 320/450] -> Loss: 298.0657
Epoch [ 321/450] -> Loss: 298.0657
Epoch [ 322/450] -> Loss: 298.0657
Epoch [ 323/450] -> Loss: 298.0657
Epoch [ 324/450] -> Loss: 298.0656
Epoch [ 325/450] -> Loss: 298.0657
Epoch [ 326/450] -> Loss: 298.0657
Epoch [ 327/450] -> Loss: 298.0657
Epoch [ 328/450] -> Loss: 298.0657
Epoch [ 329/450] -> Loss: 298.0657
Epoch [ 330/450] -> Loss: 298.0657
Epoch [ 331/450] -> Loss: 298.0657
Epoch [ 332/450] -> Loss: 298.0656
Epoch [ 333/450] -> Loss: 298.0657
Epoch [ 334/450] -> Loss: 298.0657
Epoch [ 335/450] -> Loss: 298.0657
Epoch [ 336/450] -> Loss: 298.0657
Epoch [ 337/450] -> Loss: 298.0656
Epoch [ 338/450] -> Loss: 298.0656
Epoch [ 339/450] -> Loss: 298.0657
Epoch [ 340/450] -> Loss: 298.0657
Epoch [ 341/450] -> Loss: 298.0657
Epoch [ 342/450] -> Loss: 298.0657
Epoch [ 343/450] -> Loss: 298.0657
Epoch [ 344/450] -> Loss: 298.0657
Epoch [ 345/450] -> Loss: 298.0657
Epoch [ 346/450] -> Loss: 298.0657
Epoch [ 347/450] -> Loss: 298.0656
Epoch [ 348/450] -> Loss: 298.0657
Epoch [ 349/450] -> Loss: 298.0657
--------------------------------------------------
Model checkpoint saved as FFNN_350.pth
--------------------------------------------------
Epoch [ 350/450] -> Loss: 298.0657
Epoch [ 351/450] -> Loss: 298.0657
Epoch [ 352/450] -> Loss: 298.0657
Epoch [ 353/450] -> Loss: 298.0656
Epoch [ 354/450] -> Loss: 298.0657
Epoch [ 355/450] -> Loss: 298.0656
Epoch [ 356/450] -> Loss: 298.0656
Epoch [ 357/450] -> Loss: 298.0657
Epoch [ 358/450] -> Loss: 298.0657
Epoch [ 359/450] -> Loss: 298.0656
Epoch [ 360/450] -> Loss: 298.0656
Epoch [ 361/450] -> Loss: 298.0657
