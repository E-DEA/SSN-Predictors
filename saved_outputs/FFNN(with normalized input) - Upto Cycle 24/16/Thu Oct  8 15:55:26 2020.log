----------------------------------------------------------------------------------------------------
Usage: python3 SSN_predictor.py <path_to_ssn_datafile> <path_to_aa_datafile>
----------------------------------------------------------------------------------------------------
Code running on device: cuda
----------------------------------------------------------------------------------------------------
Data loaded from file locations :
    SSN - /home/extern/Dropbox/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Dropbox/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
----------------------------------------------------------------------------------------------------
Solar cycle data loaded/saved as: cycle_data.pickle
----------------------------------------------------------------------------------------------------
SC Number     Start Date       End Date      Solar Max   Length(in months)
         0      [1749, 1]      [1755, 1]         154.27                  73
         1      [1755, 2]      [1766, 5]         144.12                 136
         2      [1766, 6]      [1775, 5]         192.98                 108
         3      [1775, 6]      [1784, 8]         264.25                 111
         4      [1784, 9]      [1798, 3]         235.28                 163
         5      [1798, 4]     [1810, 11]          67.93                 152
         6     [1810, 12]      [1823, 4]          81.99                 149
         7      [1823, 5]     [1833, 10]          81.16                 126
         8     [1833, 11]      [1843, 6]         119.24                 116
         9      [1843, 7]     [1855, 11]         244.87                 149
        10     [1855, 12]      [1867, 2]         219.94                 135
        11      [1867, 3]     [1878, 11]         186.15                 141
        12     [1878, 12]      [1890, 2]         234.02                 135
        13      [1890, 3]      [1902, 0]         124.41                 142
        14      [1902, 1]      [1913, 6]         146.55                 138
        15      [1913, 7]      [1923, 7]         107.08                 121
        16      [1923, 8]      [1933, 8]         175.67                 121
        17      [1933, 9]      [1944, 1]         130.23                 125
        18      [1944, 2]      [1954, 3]         198.64                 122
        19      [1954, 4]      [1964, 9]         218.73                 126
        20     [1964, 10]      [1976, 2]          285.0                 137
        21      [1976, 3]      [1986, 8]         156.63                 126
        22      [1986, 9]      [1996, 4]         232.92                 116
        23      [1996, 5]     [2008, 11]         212.48                 151
        24     [2008, 12]     [2019, 11]         180.28                 132
----------------------------------------------------------------------------------------------------
Selected data:
    Training: SC 13 to 22
    Validation: SC 23
    Prediction: SC 24
----------------------------------------------------------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=3, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=3, out_features=1, bias=True)
  )
)    Training mode: True    Prediction mode: True
----------------------------------------------------------------------------------------------------
Selected optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
----------------------------------------------------------------------------------------------------
Selected scheduler: ReduceLROnPlateau(
    {'factor': 0.9, 'min_lrs': [0], 'patience': 10, 'verbose': True, 'cooldown': 0, 'cooldown_counter': 0, 'mode': 'min', 'threshold': 0.0001, 'threshold_mode': 'rel', 'best': inf, 'num_bad_epochs': 0, 'mode_worse': inf, 'eps': 1e-08, 'last_epoch': 0})
----------------------------------------------------------------------------------------------------
No pre-trained models available, initializing model weights
----------------------------------------------------------------------------------------------------
Training model with solar cycle 12 to 22 data with: num_epochs=1200
Epoch [   5/1200] -> Loss: 59.7698
Epoch [  10/1200] -> Loss: 37.1612
Epoch [  15/1200] -> Loss: 34.5566
Epoch [  20/1200] -> Loss: 34.0327
Epoch [  25/1200] -> Loss: 33.8332
Epoch [  30/1200] -> Loss: 33.6696
Epoch [  35/1200] -> Loss: 33.6400
Epoch [  40/1200] -> Loss: 33.6475
Epoch [  45/1200] -> Loss: 33.5647
Epoch [  50/1200] -> Loss: 33.5097
Epoch    55: reducing learning rate of group 0 to 9.0000e-04.
Epoch [  55/1200] -> Loss: 33.4193
Epoch [  60/1200] -> Loss: 33.2644
Epoch [  65/1200] -> Loss: 33.2951
Epoch    66: reducing learning rate of group 0 to 8.1000e-04.
Epoch [  70/1200] -> Loss: 33.3258
Epoch [  75/1200] -> Loss: 33.4315
Epoch [  80/1200] -> Loss: 33.2630
Epoch [  85/1200] -> Loss: 33.2849
Epoch [  90/1200] -> Loss: 33.2003
Epoch [  95/1200] -> Loss: 33.1636
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_100.pth
----------------------------------------------------------------------------------------------------
Epoch [ 100/1200] -> Loss: 33.0915
Epoch [ 105/1200] -> Loss: 33.0662
Epoch [ 110/1200] -> Loss: 33.1155
Epoch [ 115/1200] -> Loss: 33.3697
Epoch   120: reducing learning rate of group 0 to 7.2900e-04.
Epoch [ 120/1200] -> Loss: 32.9907
Epoch [ 125/1200] -> Loss: 33.4330
Epoch [ 130/1200] -> Loss: 33.0768
Epoch   131: reducing learning rate of group 0 to 6.5610e-04.
Epoch [ 135/1200] -> Loss: 33.0746
Epoch [ 140/1200] -> Loss: 33.0379
Epoch   142: reducing learning rate of group 0 to 5.9049e-04.
Epoch [ 145/1200] -> Loss: 32.9563
Epoch [ 150/1200] -> Loss: 33.1688
Epoch   153: reducing learning rate of group 0 to 5.3144e-04.
Epoch [ 155/1200] -> Loss: 33.1835
Epoch [ 160/1200] -> Loss: 33.0226
Epoch   164: reducing learning rate of group 0 to 4.7830e-04.
Epoch [ 165/1200] -> Loss: 33.2709
Epoch [ 170/1200] -> Loss: 33.3338
Epoch [ 175/1200] -> Loss: 32.8192
Epoch [ 180/1200] -> Loss: 33.2162
Epoch [ 185/1200] -> Loss: 33.0219
Epoch   186: reducing learning rate of group 0 to 4.3047e-04.
Epoch [ 190/1200] -> Loss: 33.1437
Epoch [ 195/1200] -> Loss: 33.0466
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_200.pth
----------------------------------------------------------------------------------------------------
Epoch [ 200/1200] -> Loss: 33.1263
Epoch [ 205/1200] -> Loss: 32.9519
Epoch   208: reducing learning rate of group 0 to 3.8742e-04.
Epoch [ 210/1200] -> Loss: 33.1837
Epoch [ 215/1200] -> Loss: 33.0802
Epoch   219: reducing learning rate of group 0 to 3.4868e-04.
Epoch [ 220/1200] -> Loss: 32.8874
Epoch [ 225/1200] -> Loss: 32.9025
Epoch   230: reducing learning rate of group 0 to 3.1381e-04.
Epoch [ 230/1200] -> Loss: 32.9661
Epoch [ 235/1200] -> Loss: 33.2620
Epoch [ 240/1200] -> Loss: 33.1422
Epoch   241: reducing learning rate of group 0 to 2.8243e-04.
Epoch [ 245/1200] -> Loss: 33.0354
Epoch [ 250/1200] -> Loss: 33.0735
Epoch   252: reducing learning rate of group 0 to 2.5419e-04.
Epoch [ 255/1200] -> Loss: 33.0946
Epoch [ 260/1200] -> Loss: 32.9185
Epoch   263: reducing learning rate of group 0 to 2.2877e-04.
Epoch [ 265/1200] -> Loss: 33.1587
Epoch [ 270/1200] -> Loss: 33.3740
Epoch [ 275/1200] -> Loss: 33.0201
Epoch   280: reducing learning rate of group 0 to 2.0589e-04.
Epoch [ 280/1200] -> Loss: 33.1673
Epoch [ 285/1200] -> Loss: 32.9458
Epoch [ 290/1200] -> Loss: 33.0481
Epoch   291: reducing learning rate of group 0 to 1.8530e-04.
Epoch [ 295/1200] -> Loss: 33.1005
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_300.pth
----------------------------------------------------------------------------------------------------
Epoch [ 300/1200] -> Loss: 32.9842
Epoch   302: reducing learning rate of group 0 to 1.6677e-04.
Epoch [ 305/1200] -> Loss: 33.1834
Epoch [ 310/1200] -> Loss: 32.9166
Epoch   313: reducing learning rate of group 0 to 1.5009e-04.
Epoch [ 315/1200] -> Loss: 32.9177
Epoch [ 320/1200] -> Loss: 32.9143
Epoch   324: reducing learning rate of group 0 to 1.3509e-04.
Epoch [ 325/1200] -> Loss: 32.8866
Epoch [ 330/1200] -> Loss: 33.1789
Epoch   335: reducing learning rate of group 0 to 1.2158e-04.
Epoch [ 335/1200] -> Loss: 33.0495
Epoch [ 340/1200] -> Loss: 33.0113
Epoch [ 345/1200] -> Loss: 33.0563
Epoch   346: reducing learning rate of group 0 to 1.0942e-04.
Epoch [ 350/1200] -> Loss: 32.8207
Epoch [ 355/1200] -> Loss: 32.8777
Epoch   357: reducing learning rate of group 0 to 9.8477e-05.
Epoch [ 360/1200] -> Loss: 33.0472
Epoch [ 365/1200] -> Loss: 33.2792
Epoch   368: reducing learning rate of group 0 to 8.8629e-05.
Epoch [ 370/1200] -> Loss: 32.9629
Epoch [ 375/1200] -> Loss: 33.0782
Epoch   379: reducing learning rate of group 0 to 7.9766e-05.
Epoch [ 380/1200] -> Loss: 33.0817
Epoch [ 385/1200] -> Loss: 33.0642
Epoch   390: reducing learning rate of group 0 to 7.1790e-05.
Epoch [ 390/1200] -> Loss: 32.9468
Epoch [ 395/1200] -> Loss: 33.0061
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_400.pth
----------------------------------------------------------------------------------------------------
Epoch [ 400/1200] -> Loss: 33.1108
Epoch   401: reducing learning rate of group 0 to 6.4611e-05.
Epoch [ 405/1200] -> Loss: 32.9689
Epoch [ 410/1200] -> Loss: 33.1861
Epoch   412: reducing learning rate of group 0 to 5.8150e-05.
Epoch [ 415/1200] -> Loss: 32.9467
Epoch [ 420/1200] -> Loss: 33.1158
Epoch   423: reducing learning rate of group 0 to 5.2335e-05.
Epoch [ 425/1200] -> Loss: 33.1661
Epoch [ 430/1200] -> Loss: 33.0270
Epoch   434: reducing learning rate of group 0 to 4.7101e-05.
Epoch [ 435/1200] -> Loss: 32.9004
Epoch [ 440/1200] -> Loss: 32.7299
Epoch   445: reducing learning rate of group 0 to 4.2391e-05.
Epoch [ 445/1200] -> Loss: 33.0260
Epoch [ 450/1200] -> Loss: 33.1062
Epoch [ 455/1200] -> Loss: 32.8043
Epoch   456: reducing learning rate of group 0 to 3.8152e-05.
Epoch [ 460/1200] -> Loss: 33.1199
Epoch [ 465/1200] -> Loss: 33.1253
Epoch   467: reducing learning rate of group 0 to 3.4337e-05.
Epoch [ 470/1200] -> Loss: 33.2538
Epoch [ 475/1200] -> Loss: 33.2337
Epoch   478: reducing learning rate of group 0 to 3.0903e-05.
Epoch [ 480/1200] -> Loss: 33.1576
Epoch [ 485/1200] -> Loss: 32.8976
Epoch   489: reducing learning rate of group 0 to 2.7813e-05.
Epoch [ 490/1200] -> Loss: 33.0952
Epoch [ 495/1200] -> Loss: 33.0053
Epoch   500: reducing learning rate of group 0 to 2.5032e-05.
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_500.pth
----------------------------------------------------------------------------------------------------
Epoch [ 500/1200] -> Loss: 33.0309
Epoch [ 505/1200] -> Loss: 33.1605
Epoch [ 510/1200] -> Loss: 33.0271
Epoch   511: reducing learning rate of group 0 to 2.2528e-05.
Epoch [ 515/1200] -> Loss: 32.9625
Epoch [ 520/1200] -> Loss: 33.0482
Epoch   522: reducing learning rate of group 0 to 2.0276e-05.
Epoch [ 525/1200] -> Loss: 32.8915
Epoch [ 530/1200] -> Loss: 33.0498
Epoch   533: reducing learning rate of group 0 to 1.8248e-05.
Epoch [ 535/1200] -> Loss: 33.2340
Epoch [ 540/1200] -> Loss: 33.2437
Epoch   544: reducing learning rate of group 0 to 1.6423e-05.
Epoch [ 545/1200] -> Loss: 33.2003
Epoch [ 550/1200] -> Loss: 33.0424
Epoch   555: reducing learning rate of group 0 to 1.4781e-05.
Epoch [ 555/1200] -> Loss: 33.2268
Epoch [ 560/1200] -> Loss: 32.9786
Epoch [ 565/1200] -> Loss: 33.1272
Epoch   566: reducing learning rate of group 0 to 1.3303e-05.
Epoch [ 570/1200] -> Loss: 32.9263
Epoch [ 575/1200] -> Loss: 33.0799
Epoch   577: reducing learning rate of group 0 to 1.1973e-05.
Epoch [ 580/1200] -> Loss: 33.0167
Epoch [ 585/1200] -> Loss: 33.1134
Epoch   588: reducing learning rate of group 0 to 1.0775e-05.
Epoch [ 590/1200] -> Loss: 32.9754
Epoch [ 595/1200] -> Loss: 32.8661
Epoch   599: reducing learning rate of group 0 to 9.6977e-06.
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_600.pth
----------------------------------------------------------------------------------------------------
Epoch [ 600/1200] -> Loss: 33.1812
Epoch [ 605/1200] -> Loss: 33.2539
Epoch   610: reducing learning rate of group 0 to 8.7280e-06.
Epoch [ 610/1200] -> Loss: 33.1140
Epoch [ 615/1200] -> Loss: 32.9710
Epoch [ 620/1200] -> Loss: 33.0793
Epoch   621: reducing learning rate of group 0 to 7.8552e-06.
Epoch [ 625/1200] -> Loss: 32.8165
Epoch [ 630/1200] -> Loss: 33.1151
Epoch   632: reducing learning rate of group 0 to 7.0697e-06.
Epoch [ 635/1200] -> Loss: 32.7262
Epoch [ 640/1200] -> Loss: 33.2419
Epoch   643: reducing learning rate of group 0 to 6.3627e-06.
Epoch [ 645/1200] -> Loss: 33.1124
Epoch [ 650/1200] -> Loss: 33.0575
Epoch   654: reducing learning rate of group 0 to 5.7264e-06.
Epoch [ 655/1200] -> Loss: 33.0811
Epoch [ 660/1200] -> Loss: 33.0779
Epoch   665: reducing learning rate of group 0 to 5.1538e-06.
Epoch [ 665/1200] -> Loss: 33.0108
Epoch [ 670/1200] -> Loss: 32.8489
Epoch [ 675/1200] -> Loss: 32.9253
Epoch   676: reducing learning rate of group 0 to 4.6384e-06.
Epoch [ 680/1200] -> Loss: 33.0057
Epoch [ 685/1200] -> Loss: 33.0538
Epoch   687: reducing learning rate of group 0 to 4.1746e-06.
Epoch [ 690/1200] -> Loss: 33.1272
Epoch [ 695/1200] -> Loss: 33.1511
Epoch   698: reducing learning rate of group 0 to 3.7571e-06.
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_700.pth
----------------------------------------------------------------------------------------------------
Epoch [ 700/1200] -> Loss: 33.0713
Epoch [ 705/1200] -> Loss: 33.0473
Epoch   709: reducing learning rate of group 0 to 3.3814e-06.
Epoch [ 710/1200] -> Loss: 33.0446
Epoch [ 715/1200] -> Loss: 33.2362
Epoch   720: reducing learning rate of group 0 to 3.0433e-06.
Epoch [ 720/1200] -> Loss: 32.9226
Epoch [ 725/1200] -> Loss: 33.3797
Epoch [ 730/1200] -> Loss: 32.9308
Epoch   731: reducing learning rate of group 0 to 2.7389e-06.
Epoch [ 735/1200] -> Loss: 32.8714
Epoch [ 740/1200] -> Loss: 32.9965
Epoch   742: reducing learning rate of group 0 to 2.4650e-06.
Epoch [ 745/1200] -> Loss: 33.1596
Epoch [ 750/1200] -> Loss: 33.1417
Epoch   753: reducing learning rate of group 0 to 2.2185e-06.
Epoch [ 755/1200] -> Loss: 33.1653
Epoch [ 760/1200] -> Loss: 33.0616
Epoch   764: reducing learning rate of group 0 to 1.9967e-06.
Epoch [ 765/1200] -> Loss: 33.0882
Epoch [ 770/1200] -> Loss: 32.9174
Epoch   775: reducing learning rate of group 0 to 1.7970e-06.
Epoch [ 775/1200] -> Loss: 33.0629
Epoch [ 780/1200] -> Loss: 32.8701
Epoch [ 785/1200] -> Loss: 32.9855
Epoch   786: reducing learning rate of group 0 to 1.6173e-06.
Epoch [ 790/1200] -> Loss: 33.1005
Epoch [ 795/1200] -> Loss: 33.0450
Epoch   797: reducing learning rate of group 0 to 1.4556e-06.
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_800.pth
----------------------------------------------------------------------------------------------------
Epoch [ 800/1200] -> Loss: 33.0152
Epoch [ 805/1200] -> Loss: 32.9721
Epoch   808: reducing learning rate of group 0 to 1.3100e-06.
Epoch [ 810/1200] -> Loss: 33.0241
Epoch [ 815/1200] -> Loss: 33.0025
Epoch   819: reducing learning rate of group 0 to 1.1790e-06.
Epoch [ 820/1200] -> Loss: 33.2511
Epoch [ 825/1200] -> Loss: 32.7637
Epoch   830: reducing learning rate of group 0 to 1.0611e-06.
Epoch [ 830/1200] -> Loss: 33.0778
Epoch [ 835/1200] -> Loss: 33.3520
Epoch [ 840/1200] -> Loss: 32.9197
Epoch   841: reducing learning rate of group 0 to 9.5500e-07.
Epoch [ 845/1200] -> Loss: 32.8773
Epoch [ 850/1200] -> Loss: 33.2051
Epoch   852: reducing learning rate of group 0 to 8.5950e-07.
Epoch [ 855/1200] -> Loss: 32.9569
Epoch [ 860/1200] -> Loss: 33.1843
Epoch   863: reducing learning rate of group 0 to 7.7355e-07.
Epoch [ 865/1200] -> Loss: 32.9636
Epoch [ 870/1200] -> Loss: 32.8579
Epoch   874: reducing learning rate of group 0 to 6.9620e-07.
Epoch [ 875/1200] -> Loss: 32.9894
Epoch [ 880/1200] -> Loss: 32.9117
Epoch   885: reducing learning rate of group 0 to 6.2658e-07.
Epoch [ 885/1200] -> Loss: 33.1975
Epoch [ 890/1200] -> Loss: 33.0666
Epoch [ 895/1200] -> Loss: 33.0946
Epoch   896: reducing learning rate of group 0 to 5.6392e-07.
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_900.pth
----------------------------------------------------------------------------------------------------
Epoch [ 900/1200] -> Loss: 33.0655
Epoch [ 905/1200] -> Loss: 32.9414
Epoch   907: reducing learning rate of group 0 to 5.0753e-07.
Epoch [ 910/1200] -> Loss: 33.1702
Epoch [ 915/1200] -> Loss: 33.0480
Epoch   918: reducing learning rate of group 0 to 4.5678e-07.
Epoch [ 920/1200] -> Loss: 32.8414
Epoch [ 925/1200] -> Loss: 32.7277
Epoch   929: reducing learning rate of group 0 to 4.1110e-07.
Epoch [ 930/1200] -> Loss: 33.0779
Epoch [ 935/1200] -> Loss: 32.9387
Epoch [ 940/1200] -> Loss: 32.8876
Epoch   943: reducing learning rate of group 0 to 3.6999e-07.
Epoch [ 945/1200] -> Loss: 32.9391
Epoch [ 950/1200] -> Loss: 33.3053
Epoch   954: reducing learning rate of group 0 to 3.3299e-07.
Epoch [ 955/1200] -> Loss: 33.0669
Epoch [ 960/1200] -> Loss: 33.2111
Epoch   965: reducing learning rate of group 0 to 2.9969e-07.
Epoch [ 965/1200] -> Loss: 32.9632
Epoch [ 970/1200] -> Loss: 33.1197
Epoch [ 975/1200] -> Loss: 32.9531
Epoch   976: reducing learning rate of group 0 to 2.6972e-07.
Epoch [ 980/1200] -> Loss: 32.8497
Epoch [ 985/1200] -> Loss: 33.1621
Epoch   987: reducing learning rate of group 0 to 2.4275e-07.
Epoch [ 990/1200] -> Loss: 32.9733
Epoch [ 995/1200] -> Loss: 33.0759
Epoch   998: reducing learning rate of group 0 to 2.1847e-07.
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_1000.pth
----------------------------------------------------------------------------------------------------
Epoch [1000/1200] -> Loss: 32.8791
Epoch [1005/1200] -> Loss: 33.0882
Epoch  1009: reducing learning rate of group 0 to 1.9663e-07.
Epoch [1010/1200] -> Loss: 32.9213
Epoch [1015/1200] -> Loss: 32.8981
Epoch  1020: reducing learning rate of group 0 to 1.7696e-07.
Epoch [1020/1200] -> Loss: 33.2438
Epoch [1025/1200] -> Loss: 33.2148
Epoch [1030/1200] -> Loss: 33.3197
Epoch  1031: reducing learning rate of group 0 to 1.5927e-07.
Epoch [1035/1200] -> Loss: 33.0357
Epoch [1040/1200] -> Loss: 32.9441
Epoch  1042: reducing learning rate of group 0 to 1.4334e-07.
Epoch [1045/1200] -> Loss: 32.9439
Epoch [1050/1200] -> Loss: 33.1122
Epoch [1055/1200] -> Loss: 32.8897
Epoch [1060/1200] -> Loss: 32.8789
Epoch  1064: reducing learning rate of group 0 to 1.2901e-07.
Epoch [1065/1200] -> Loss: 32.9156
Epoch [1070/1200] -> Loss: 33.0752
Epoch  1075: reducing learning rate of group 0 to 1.1611e-07.
Epoch [1075/1200] -> Loss: 32.8188
Epoch [1080/1200] -> Loss: 33.2063
Epoch [1085/1200] -> Loss: 33.1441
Epoch  1086: reducing learning rate of group 0 to 1.0450e-07.
Epoch [1090/1200] -> Loss: 33.1438
Epoch [1095/1200] -> Loss: 33.0508
Epoch  1097: reducing learning rate of group 0 to 9.4046e-08.
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_1100.pth
----------------------------------------------------------------------------------------------------
Epoch [1100/1200] -> Loss: 33.1787
Epoch [1105/1200] -> Loss: 33.1333
Epoch [1110/1200] -> Loss: 33.0330
Epoch [1115/1200] -> Loss: 32.8514
Epoch [1120/1200] -> Loss: 33.0234
Epoch [1125/1200] -> Loss: 33.1735
Epoch [1130/1200] -> Loss: 32.9982
Epoch [1135/1200] -> Loss: 33.0087
Epoch [1140/1200] -> Loss: 33.0784
Epoch [1145/1200] -> Loss: 32.9699
Epoch [1150/1200] -> Loss: 33.2469
Epoch [1155/1200] -> Loss: 32.7849
Epoch [1160/1200] -> Loss: 33.1786
Epoch [1165/1200] -> Loss: 33.0318
Epoch [1170/1200] -> Loss: 32.8298
Epoch [1175/1200] -> Loss: 33.1443
Epoch [1180/1200] -> Loss: 33.1089
Epoch [1185/1200] -> Loss: 33.0511
Epoch [1190/1200] -> Loss: 33.0457
Epoch [1195/1200] -> Loss: 33.0201
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_1200.pth
----------------------------------------------------------------------------------------------------
Epoch [1200/1200] -> Loss: 32.9864
----------------------------------------------------------------------------------------------------
Plotting loss data...
----------------------------------------------------------------------------------------------------
Training finished successfully.
        Saved model checkpoints can be found in: /home/extern/Dropbox/Research/scripts/models/
        Saved data/loss graphs can be found in: /home/extern/Dropbox/Research/scripts/graphs/
----------------------------------------------------------------------------------------------------
Validating model for solar cycle 23 data
Step [   1/151] -> Date: 5/1996, Target: 7.6, Prediction: 29.39443588256836
Step [   2/151] -> Date: 6/1996, Target: 16.5, Prediction: 20.465999603271484
Step [   3/151] -> Date: 7/1996, Target: 11.8, Prediction: 15.51727294921875
Step [   4/151] -> Date: 8/1996, Target: 19.7, Prediction: 16.459829330444336
Step [   5/151] -> Date: 9/1996, Target: 3.0, Prediction: 14.830488204956055
Step [   6/151] -> Date: 10/1996, Target: 0.7, Prediction: 19.480182647705078
Step [   7/151] -> Date: 11/1996, Target: 24.9, Prediction: 29.29180908203125
Step [   8/151] -> Date: 12/1996, Target: 14.0, Prediction: 26.39113998413086
Step [   9/151] -> Date: 1/1997, Target: 7.4, Prediction: 17.17473602294922
Step [  10/151] -> Date: 2/1997, Target: 11.0, Prediction: 27.039859771728516
Step [  11/151] -> Date: 3/1997, Target: 12.1, Prediction: 32.939300537109375
Step [  12/151] -> Date: 4/1997, Target: 23.0, Prediction: 32.95390701293945
Step [  13/151] -> Date: 5/1997, Target: 25.4, Prediction: 70.57747650146484
Step [  14/151] -> Date: 6/1997, Target: 20.8, Prediction: 64.07780456542969
Step [  15/151] -> Date: 7/1997, Target: 12.9, Prediction: 54.29719924926758
Step [  16/151] -> Date: 8/1997, Target: 35.7, Prediction: 73.39846801757812
Step [  17/151] -> Date: 9/1997, Target: 59.7, Prediction: 62.820247650146484
Step [  18/151] -> Date: 10/1997, Target: 32.8, Prediction: 81.75480651855469
Step [  19/151] -> Date: 11/1997, Target: 50.4, Prediction: 88.47940826416016
Step [  20/151] -> Date: 12/1997, Target: 55.5, Prediction: 72.10350799560547
Step [  21/151] -> Date: 1/1998, Target: 44.5, Prediction: 92.54293823242188
Step [  22/151] -> Date: 2/1998, Target: 50.2, Prediction: 95.59158325195312
Step [  23/151] -> Date: 3/1998, Target: 82.0, Prediction: 99.43902587890625
Step [  24/151] -> Date: 4/1998, Target: 70.6, Prediction: 101.6529769897461
Step [  25/151] -> Date: 5/1998, Target: 74.0, Prediction: 141.22901916503906
Step [  26/151] -> Date: 6/1998, Target: 90.5, Prediction: 141.9495849609375
Step [  27/151] -> Date: 7/1998, Target: 96.7, Prediction: 173.94760131835938
Step [  28/151] -> Date: 8/1998, Target: 121.1, Prediction: 167.70416259765625
Step [  29/151] -> Date: 9/1998, Target: 132.0, Prediction: 164.7119903564453
Step [  30/151] -> Date: 10/1998, Target: 78.5, Prediction: 157.67489624023438
Step [  31/151] -> Date: 11/1998, Target: 97.3, Prediction: 147.30149841308594
Step [  32/151] -> Date: 12/1998, Target: 119.2, Prediction: 156.78298950195312
Step [  33/151] -> Date: 1/1999, Target: 86.0, Prediction: 193.35049438476562
Step [  34/151] -> Date: 2/1999, Target: 98.0, Prediction: 146.98023986816406
Step [  35/151] -> Date: 3/1999, Target: 103.5, Prediction: 172.89109802246094
Step [  36/151] -> Date: 4/1999, Target: 93.6, Prediction: 170.16696166992188
Step [  37/151] -> Date: 5/1999, Target: 149.6, Prediction: 192.26483154296875
Step [  38/151] -> Date: 6/1999, Target: 207.2, Prediction: 204.6645965576172
Step [  39/151] -> Date: 7/1999, Target: 173.5, Prediction: 192.8304901123047
Step [  40/151] -> Date: 8/1999, Target: 142.3, Prediction: 198.2814178466797
Step [  41/151] -> Date: 9/1999, Target: 106.3, Prediction: 177.76185607910156
Step [  42/151] -> Date: 10/1999, Target: 168.7, Prediction: 184.30816650390625
Step [  43/151] -> Date: 11/1999, Target: 188.3, Prediction: 183.81317138671875
Step [  44/151] -> Date: 12/1999, Target: 116.8, Prediction: 173.5689697265625
Step [  45/151] -> Date: 1/2000, Target: 133.1, Prediction: 164.01002502441406
Step [  46/151] -> Date: 2/2000, Target: 165.7, Prediction: 178.9665985107422
Step [  47/151] -> Date: 3/2000, Target: 217.7, Prediction: 206.6316680908203
Step [  48/151] -> Date: 4/2000, Target: 191.5, Prediction: 171.7823944091797
Step [  49/151] -> Date: 5/2000, Target: 165.9, Prediction: 195.28009033203125
Step [  50/151] -> Date: 6/2000, Target: 188.0, Prediction: 185.03208923339844
Step [  51/151] -> Date: 7/2000, Target: 244.3, Prediction: 192.28749084472656
Step [  52/151] -> Date: 8/2000, Target: 180.5, Prediction: 192.1470184326172
Step [  53/151] -> Date: 9/2000, Target: 156.0, Prediction: 211.30747985839844
Step [  54/151] -> Date: 10/2000, Target: 141.6, Prediction: 197.7140655517578
Step [  55/151] -> Date: 11/2000, Target: 158.1, Prediction: 189.6874237060547
Step [  56/151] -> Date: 12/2000, Target: 143.3, Prediction: 178.36215209960938
Step [  57/151] -> Date: 1/2001, Target: 142.6, Prediction: 213.19822692871094
Step [  58/151] -> Date: 2/2001, Target: 121.5, Prediction: 212.497314453125
Step [  59/151] -> Date: 3/2001, Target: 165.8, Prediction: 215.8544921875
Step [  60/151] -> Date: 4/2001, Target: 161.7, Prediction: 183.4877166748047
Step [  61/151] -> Date: 5/2001, Target: 142.1, Prediction: 191.56520080566406
Step [  62/151] -> Date: 6/2001, Target: 202.9, Prediction: 178.12374877929688
Step [  63/151] -> Date: 7/2001, Target: 123.0, Prediction: 194.19244384765625
Step [  64/151] -> Date: 8/2001, Target: 161.5, Prediction: 187.56564331054688
Step [  65/151] -> Date: 9/2001, Target: 238.2, Prediction: 207.11691284179688
Step [  66/151] -> Date: 10/2001, Target: 194.1, Prediction: 166.4591827392578
Step [  67/151] -> Date: 11/2001, Target: 176.6, Prediction: 157.52476501464844
Step [  68/151] -> Date: 12/2001, Target: 213.4, Prediction: 143.09326171875
Step [  69/151] -> Date: 1/2002, Target: 184.6, Prediction: 143.07785034179688
Step [  70/151] -> Date: 2/2002, Target: 170.2, Prediction: 147.65040588378906
Step [  71/151] -> Date: 3/2002, Target: 147.1, Prediction: 140.54605102539062
Step [  72/151] -> Date: 4/2002, Target: 186.9, Prediction: 145.49021911621094
Step [  73/151] -> Date: 5/2002, Target: 187.5, Prediction: 135.81910705566406
Step [  74/151] -> Date: 6/2002, Target: 128.8, Prediction: 134.32933044433594
Step [  75/151] -> Date: 7/2002, Target: 161.0, Prediction: 131.7215576171875
Step [  76/151] -> Date: 8/2002, Target: 175.6, Prediction: 116.24504089355469
Step [  77/151] -> Date: 9/2002, Target: 187.9, Prediction: 134.0218505859375
Step [  78/151] -> Date: 10/2002, Target: 151.2, Prediction: 125.96591186523438
Step [  79/151] -> Date: 11/2002, Target: 147.2, Prediction: 119.40158081054688
Step [  80/151] -> Date: 12/2002, Target: 135.3, Prediction: 110.23369598388672
Step [  81/151] -> Date: 1/2003, Target: 133.5, Prediction: 105.98696899414062
Step [  82/151] -> Date: 2/2003, Target: 75.7, Prediction: 109.53033447265625
Step [  83/151] -> Date: 3/2003, Target: 100.7, Prediction: 102.6888656616211
Step [  84/151] -> Date: 4/2003, Target: 97.9, Prediction: 91.66206359863281
Step [  85/151] -> Date: 5/2003, Target: 86.8, Prediction: 76.84088897705078
Step [  86/151] -> Date: 6/2003, Target: 118.7, Prediction: 68.15127563476562
Step [  87/151] -> Date: 7/2003, Target: 128.3, Prediction: 76.80400848388672
Step [  88/151] -> Date: 8/2003, Target: 115.4, Prediction: 84.00562286376953
Step [  89/151] -> Date: 9/2003, Target: 78.5, Prediction: 76.01126098632812
Step [  90/151] -> Date: 10/2003, Target: 97.8, Prediction: 70.75265502929688
Step [  91/151] -> Date: 11/2003, Target: 82.9, Prediction: 62.4018669128418
Step [  92/151] -> Date: 12/2003, Target: 72.2, Prediction: 62.796653747558594
Step [  93/151] -> Date: 1/2004, Target: 60.6, Prediction: 62.78537368774414
Step [  94/151] -> Date: 2/2004, Target: 74.6, Prediction: 65.46744537353516
Step [  95/151] -> Date: 3/2004, Target: 74.8, Prediction: 57.6652717590332
Step [  96/151] -> Date: 4/2004, Target: 59.2, Prediction: 62.08750534057617
Step [  97/151] -> Date: 5/2004, Target: 72.8, Prediction: 47.51053237915039
Step [  98/151] -> Date: 6/2004, Target: 66.5, Prediction: 29.564537048339844
Step [  99/151] -> Date: 7/2004, Target: 83.8, Prediction: 31.15052032470703
Step [ 100/151] -> Date: 8/2004, Target: 69.7, Prediction: 31.075252532958984
Step [ 101/151] -> Date: 9/2004, Target: 48.8, Prediction: 36.504249572753906
Step [ 102/151] -> Date: 10/2004, Target: 74.2, Prediction: 35.8427734375
Step [ 103/151] -> Date: 11/2004, Target: 70.1, Prediction: 25.01589584350586
Step [ 104/151] -> Date: 12/2004, Target: 28.9, Prediction: 25.500648498535156
Step [ 105/151] -> Date: 1/2005, Target: 48.1, Prediction: 23.034454345703125
Step [ 106/151] -> Date: 2/2005, Target: 43.5, Prediction: 20.980133056640625
Step [ 107/151] -> Date: 3/2005, Target: 39.6, Prediction: 21.03124237060547
Step [ 108/151] -> Date: 4/2005, Target: 38.7, Prediction: 22.132061004638672
Step [ 109/151] -> Date: 5/2005, Target: 61.9, Prediction: 15.082691192626953
Step [ 110/151] -> Date: 6/2005, Target: 56.8, Prediction: 14.580047607421875
Step [ 111/151] -> Date: 7/2005, Target: 62.4, Prediction: 14.61445140838623
Step [ 112/151] -> Date: 8/2005, Target: 60.5, Prediction: 14.356507301330566
Step [ 113/151] -> Date: 9/2005, Target: 37.2, Prediction: 13.910758972167969
Step [ 114/151] -> Date: 10/2005, Target: 13.2, Prediction: 13.555578231811523
Step [ 115/151] -> Date: 11/2005, Target: 27.5, Prediction: 12.964987754821777
Step [ 116/151] -> Date: 12/2005, Target: 59.3, Prediction: 12.455818176269531
Step [ 117/151] -> Date: 1/2006, Target: 20.9, Prediction: 12.482784271240234
Step [ 118/151] -> Date: 2/2006, Target: 5.7, Prediction: 12.83393383026123
Step [ 119/151] -> Date: 3/2006, Target: 17.3, Prediction: 13.58332347869873
Step [ 120/151] -> Date: 4/2006, Target: 50.3, Prediction: 14.161293983459473
Step [ 121/151] -> Date: 5/2006, Target: 37.2, Prediction: 14.950252532958984
Step [ 122/151] -> Date: 6/2006, Target: 24.5, Prediction: 15.232011795043945
Step [ 123/151] -> Date: 7/2006, Target: 22.2, Prediction: 15.025984764099121
Step [ 124/151] -> Date: 8/2006, Target: 20.8, Prediction: 14.71934700012207
Step [ 125/151] -> Date: 9/2006, Target: 23.7, Prediction: 14.49590015411377
Step [ 126/151] -> Date: 10/2006, Target: 14.9, Prediction: 13.80270767211914
Step [ 127/151] -> Date: 11/2006, Target: 35.7, Prediction: 13.645824432373047
Step [ 128/151] -> Date: 12/2006, Target: 22.3, Prediction: 13.299356460571289
Step [ 129/151] -> Date: 1/2007, Target: 29.3, Prediction: 13.184236526489258
Step [ 130/151] -> Date: 2/2007, Target: 18.4, Prediction: 13.269208908081055
Step [ 131/151] -> Date: 3/2007, Target: 7.2, Prediction: 14.130443572998047
Step [ 132/151] -> Date: 4/2007, Target: 5.4, Prediction: 19.84613037109375
Step [ 133/151] -> Date: 5/2007, Target: 19.5, Prediction: 26.31454849243164
Step [ 134/151] -> Date: 6/2007, Target: 21.3, Prediction: 33.995182037353516
Step [ 135/151] -> Date: 7/2007, Target: 15.1, Prediction: 34.317073822021484
Step [ 136/151] -> Date: 8/2007, Target: 9.8, Prediction: 31.506603240966797
Step [ 137/151] -> Date: 9/2007, Target: 4.0, Prediction: 33.332027435302734
Step [ 138/151] -> Date: 10/2007, Target: 1.5, Prediction: 47.989768981933594
Step [ 139/151] -> Date: 11/2007, Target: 2.8, Prediction: 41.039005279541016
Step [ 140/151] -> Date: 12/2007, Target: 17.3, Prediction: 44.813987731933594
Step [ 141/151] -> Date: 1/2008, Target: 4.1, Prediction: 48.65690612792969
Step [ 142/151] -> Date: 2/2008, Target: 2.9, Prediction: 51.41786575317383
Step [ 143/151] -> Date: 3/2008, Target: 15.5, Prediction: 64.22843170166016
Step [ 144/151] -> Date: 4/2008, Target: 3.6, Prediction: 67.69623565673828
Step [ 145/151] -> Date: 5/2008, Target: 4.6, Prediction: 75.85751342773438
Step [ 146/151] -> Date: 6/2008, Target: 5.2, Prediction: 85.48145294189453
Step [ 147/151] -> Date: 7/2008, Target: 0.6, Prediction: 91.85298156738281
Step [ 148/151] -> Date: 8/2008, Target: 0.3, Prediction: 79.37503814697266
Step [ 149/151] -> Date: 9/2008, Target: 1.2, Prediction: 84.10457611083984
Step [ 150/151] -> Date: 10/2008, Target: 4.2, Prediction: 86.45464324951172
Step [ 151/151] -> Date: 11/2008, Target: 6.6, Prediction: 81.05999755859375
----------------------------------------------------------------------------------------------------
Average Validation Loss: 32.573972859919465
----------------------------------------------------------------------------------------------------
Plotting data...
----------------------------------------------------------------------------------------------------
Plotting loss data...
----------------------------------------------------------------------------------------------------
Validation finished successfully.

        Saved prediction/loss graphs can be found in: /home/extern/Dropbox/Research/scripts/graphs/
----------------------------------------------------------------------------------------------------
Predicting SC 24 using the above trained model
Step [   1/151] -> Date: 12/2008, Prediction: 17.270736694335938
Step [   2/151] -> Date: 1/2009, Prediction: 16.926151275634766
Step [   3/151] -> Date: 2/2009, Prediction: 21.75457763671875
Step [   4/151] -> Date: 3/2009, Prediction: 16.989227294921875
Step [   5/151] -> Date: 4/2009, Prediction: 15.090950012207031
Step [   6/151] -> Date: 5/2009, Prediction: 22.154563903808594
Step [   7/151] -> Date: 6/2009, Prediction: 16.808090209960938
Step [   8/151] -> Date: 7/2009, Prediction: 13.900260925292969
Step [   9/151] -> Date: 8/2009, Prediction: 16.184284210205078
Step [  10/151] -> Date: 9/2009, Prediction: 15.283550262451172
Step [  11/151] -> Date: 10/2009, Prediction: 20.855159759521484
Step [  12/151] -> Date: 11/2009, Prediction: 21.464628219604492
Step [  13/151] -> Date: 12/2009, Prediction: 49.244895935058594
Step [  14/151] -> Date: 1/2010, Prediction: 46.12355041503906
Step [  15/151] -> Date: 2/2010, Prediction: 56.37575149536133
Step [  16/151] -> Date: 3/2010, Prediction: 67.72936248779297
Step [  17/151] -> Date: 4/2010, Prediction: 56.20936584472656
Step [  18/151] -> Date: 5/2010, Prediction: 62.699153900146484
Step [  19/151] -> Date: 6/2010, Prediction: 62.027183532714844
Step [  20/151] -> Date: 7/2010, Prediction: 58.6861686706543
Step [  21/151] -> Date: 8/2010, Prediction: 60.58652877807617
Step [  22/151] -> Date: 9/2010, Prediction: 75.90457916259766
Step [  23/151] -> Date: 10/2010, Prediction: 70.02259063720703
Step [  24/151] -> Date: 11/2010, Prediction: 74.9884262084961
Step [  25/151] -> Date: 12/2010, Prediction: 112.43720245361328
Step [  26/151] -> Date: 1/2011, Prediction: 115.72870635986328
Step [  27/151] -> Date: 2/2011, Prediction: 128.46434020996094
Step [  28/151] -> Date: 3/2011, Prediction: 131.2718505859375
Step [  29/151] -> Date: 4/2011, Prediction: 108.92512512207031
Step [  30/151] -> Date: 5/2011, Prediction: 117.35823822021484
Step [  31/151] -> Date: 6/2011, Prediction: 122.73590087890625
Step [  32/151] -> Date: 7/2011, Prediction: 109.72742462158203
Step [  33/151] -> Date: 8/2011, Prediction: 114.53189849853516
Step [  34/151] -> Date: 9/2011, Prediction: 117.54228973388672
Step [  35/151] -> Date: 10/2011, Prediction: 113.21641540527344
Step [  36/151] -> Date: 11/2011, Prediction: 135.5750274658203
Step [  37/151] -> Date: 12/2011, Prediction: 184.88572692871094
Step [  38/151] -> Date: 1/2012, Prediction: 172.49163818359375
Step [  39/151] -> Date: 2/2012, Prediction: 162.50242614746094
Step [  40/151] -> Date: 3/2012, Prediction: 148.96522521972656
Step [  41/151] -> Date: 4/2012, Prediction: 174.57427978515625
Step [  42/151] -> Date: 5/2012, Prediction: 180.30947875976562
Step [  43/151] -> Date: 6/2012, Prediction: 148.44387817382812
Step [  44/151] -> Date: 7/2012, Prediction: 155.78854370117188
Step [  45/151] -> Date: 8/2012, Prediction: 170.56942749023438
Step [  46/151] -> Date: 9/2012, Prediction: 188.6698760986328
Step [  47/151] -> Date: 10/2012, Prediction: 180.52914428710938
Step [  48/151] -> Date: 11/2012, Prediction: 170.41925048828125
Step [  49/151] -> Date: 12/2012, Prediction: 190.43331909179688
Step [  50/151] -> Date: 1/2013, Prediction: 216.3339385986328
Step [  51/151] -> Date: 2/2013, Prediction: 188.04771423339844
Step [  52/151] -> Date: 3/2013, Prediction: 179.5386505126953
Step [  53/151] -> Date: 4/2013, Prediction: 172.5482940673828
Step [  54/151] -> Date: 5/2013, Prediction: 179.05372619628906
Step [  55/151] -> Date: 6/2013, Prediction: 166.1986083984375
Step [  56/151] -> Date: 7/2013, Prediction: 163.63375854492188
Step [  57/151] -> Date: 8/2013, Prediction: 154.7670440673828
Step [  58/151] -> Date: 9/2013, Prediction: 181.0496826171875
Step [  59/151] -> Date: 10/2013, Prediction: 180.56448364257812
Step [  60/151] -> Date: 11/2013, Prediction: 168.56675720214844
Step [  61/151] -> Date: 12/2013, Prediction: 186.40574645996094
Step [  62/151] -> Date: 1/2014, Prediction: 153.85154724121094
Step [  63/151] -> Date: 2/2014, Prediction: 170.52322387695312
Step [  64/151] -> Date: 3/2014, Prediction: 203.24331665039062
Step [  65/151] -> Date: 4/2014, Prediction: 187.03323364257812
Step [  66/151] -> Date: 5/2014, Prediction: 177.05430603027344
Step [  67/151] -> Date: 6/2014, Prediction: 180.09437561035156
Step [  68/151] -> Date: 7/2014, Prediction: 165.03579711914062
Step [  69/151] -> Date: 8/2014, Prediction: 163.21519470214844
Step [  70/151] -> Date: 9/2014, Prediction: 160.34239196777344
Step [  71/151] -> Date: 10/2014, Prediction: 180.70199584960938
Step [  72/151] -> Date: 11/2014, Prediction: 179.79969787597656
Step [  73/151] -> Date: 12/2014, Prediction: 130.17608642578125
Step [  74/151] -> Date: 1/2015, Prediction: 145.86949157714844
Step [  75/151] -> Date: 2/2015, Prediction: 153.01681518554688
Step [  76/151] -> Date: 3/2015, Prediction: 157.77244567871094
Step [  77/151] -> Date: 4/2015, Prediction: 147.0039825439453
Step [  78/151] -> Date: 5/2015, Prediction: 142.1113739013672
Step [  79/151] -> Date: 6/2015, Prediction: 135.0333251953125
Step [  80/151] -> Date: 7/2015, Prediction: 133.68106079101562
Step [  81/151] -> Date: 8/2015, Prediction: 111.34131622314453
Step [  82/151] -> Date: 9/2015, Prediction: 122.94775390625
Step [  83/151] -> Date: 10/2015, Prediction: 122.09595489501953
Step [  84/151] -> Date: 11/2015, Prediction: 120.19743347167969
Step [  85/151] -> Date: 12/2015, Prediction: 101.25812530517578
Step [  86/151] -> Date: 1/2016, Prediction: 103.5218276977539
Step [  87/151] -> Date: 2/2016, Prediction: 99.67304992675781
Step [  88/151] -> Date: 3/2016, Prediction: 82.54659271240234
Step [  89/151] -> Date: 4/2016, Prediction: 96.54118347167969
Step [  90/151] -> Date: 5/2016, Prediction: 87.48832702636719
Step [  91/151] -> Date: 6/2016, Prediction: 78.17236328125
Step [  92/151] -> Date: 7/2016, Prediction: 75.0071029663086
Step [  93/151] -> Date: 8/2016, Prediction: 76.4000015258789
Step [  94/151] -> Date: 9/2016, Prediction: 76.87669372558594
Step [  95/151] -> Date: 10/2016, Prediction: 69.25331115722656
Step [  96/151] -> Date: 11/2016, Prediction: 74.54360961914062
Step [  97/151] -> Date: 12/2016, Prediction: 42.1409797668457
Step [  98/151] -> Date: 1/2017, Prediction: 54.04055404663086
Step [  99/151] -> Date: 2/2017, Prediction: 44.430904388427734
Step [ 100/151] -> Date: 3/2017, Prediction: 35.523704528808594
Step [ 101/151] -> Date: 4/2017, Prediction: 45.91004180908203
Step [ 102/151] -> Date: 5/2017, Prediction: 49.54164505004883
Step [ 103/151] -> Date: 6/2017, Prediction: 27.611011505126953
Step [ 104/151] -> Date: 7/2017, Prediction: 39.8448486328125
Step [ 105/151] -> Date: 8/2017, Prediction: 32.502079010009766
Step [ 106/151] -> Date: 9/2017, Prediction: 30.854583740234375
Step [ 107/151] -> Date: 10/2017, Prediction: 30.131580352783203
Step [ 108/151] -> Date: 11/2017, Prediction: 43.20838165283203
Step [ 109/151] -> Date: 12/2017, Prediction: 20.639190673828125
Step [ 110/151] -> Date: 1/2018, Prediction: 24.82711410522461
Step [ 111/151] -> Date: 2/2018, Prediction: 24.351119995117188
Step [ 112/151] -> Date: 3/2018, Prediction: 16.877071380615234
Step [ 113/151] -> Date: 4/2018, Prediction: 13.760123252868652
Step [ 114/151] -> Date: 5/2018, Prediction: 13.552428245544434
Step [ 115/151] -> Date: 6/2018, Prediction: 20.068740844726562
Step [ 116/151] -> Date: 7/2018, Prediction: 12.641063690185547
Step [ 117/151] -> Date: 8/2018, Prediction: 12.424997329711914
Step [ 118/151] -> Date: 9/2018, Prediction: 12.96851921081543
Step [ 119/151] -> Date: 10/2018, Prediction: 16.352153778076172
Step [ 120/151] -> Date: 11/2018, Prediction: 14.125301361083984
Step [ 121/151] -> Date: 12/2018, Prediction: 14.943799018859863
Step [ 122/151] -> Date: 1/2019, Prediction: 15.022159576416016
Step [ 123/151] -> Date: 2/2019, Prediction: 15.2232666015625
Step [ 124/151] -> Date: 3/2019, Prediction: 14.8905029296875
Step [ 125/151] -> Date: 4/2019, Prediction: 14.362434387207031
Step [ 126/151] -> Date: 5/2019, Prediction: 14.192123413085938
Step [ 127/151] -> Date: 6/2019, Prediction: 13.90865707397461
Step [ 128/151] -> Date: 7/2019, Prediction: 13.547143936157227
Step [ 129/151] -> Date: 8/2019, Prediction: 13.186877250671387
Step [ 130/151] -> Date: 9/2019, Prediction: 13.358970642089844
Step [ 131/151] -> Date: 10/2019, Prediction: 13.846305847167969
Step [ 132/151] -> Date: 11/2019, Prediction: 14.43974781036377
Step [ 133/151] -> Date: 12/2019, Prediction: 19.74029541015625
Step [ 134/151] -> Date: 1/2020, Prediction: 17.960678100585938
Step [ 135/151] -> Date: 2/2020, Prediction: 16.020532608032227
Step [ 136/151] -> Date: 3/2020, Prediction: 15.296234130859375
Step [ 137/151] -> Date: 4/2020, Prediction: 14.66690444946289
Step [ 138/151] -> Date: 5/2020, Prediction: 14.197425842285156
Step [ 139/151] -> Date: 6/2020, Prediction: 17.227603912353516
Step [ 140/151] -> Date: 7/2020, Prediction: 13.67244815826416
Step [ 141/151] -> Date: 8/2020, Prediction: 13.940166473388672
Step [ 142/151] -> Date: 9/2020, Prediction: 18.129135131835938
Step [ 143/151] -> Date: 10/2020, Prediction: 14.442943572998047
Step [ 144/151] -> Date: 11/2020, Prediction: 14.782204627990723
Step [ 145/151] -> Date: 12/2020, Prediction: 42.988529205322266
Step [ 146/151] -> Date: 1/2021, Prediction: 40.676212310791016
Step [ 147/151] -> Date: 2/2021, Prediction: 40.4222526550293
Step [ 148/151] -> Date: 3/2021, Prediction: 41.151451110839844
Step [ 149/151] -> Date: 4/2021, Prediction: 42.68268966674805
Step [ 150/151] -> Date: 5/2021, Prediction: 41.973243713378906
Step [ 151/151] -> Date: 6/2021, Prediction: 39.10786056518555
----------------------------------------------------------------------------------------------------
Plotting data...
----------------------------------------------------------------------------------------------------
All newly generated files moved to saved outputs and shared with the cloud(Dropbox) successfully!
