----------------------------------------------------------------------------------------------------
Usage: python3 SSN_predictor.py <path_to_ssn_datafile> <path_to_aa_datafile>
----------------------------------------------------------------------------------------------------
Code running on device: cuda
----------------------------------------------------------------------------------------------------
Data loaded from file locations :
    SSN - /home/extern/Dropbox/Research/scripts/data/SILSO/TSN/SN_m_tot_V2.0.txt
    AA - /home/extern/Dropbox/Research/scripts/data/ISGI/aa_1869-01-01_2018-12-31_D.dat
----------------------------------------------------------------------------------------------------
Solar cycle data loaded/saved as: cycle_data.pickle
----------------------------------------------------------------------------------------------------
SC Number     Start Date       End Date      Solar Max   Length(in months)
         0      [1749, 1]      [1755, 1]         154.27                  73
         1      [1755, 2]      [1766, 5]         144.12                 136
         2      [1766, 6]      [1775, 5]         192.98                 108
         3      [1775, 6]      [1784, 8]         264.25                 111
         4      [1784, 9]      [1798, 3]         235.28                 163
         5      [1798, 4]     [1810, 11]          67.93                 152
         6     [1810, 12]      [1823, 4]          81.99                 149
         7      [1823, 5]     [1833, 10]          81.16                 126
         8     [1833, 11]      [1843, 6]         119.24                 116
         9      [1843, 7]     [1855, 11]         244.87                 149
        10     [1855, 12]      [1867, 2]         219.94                 135
        11      [1867, 3]     [1878, 11]         186.15                 141
        12     [1878, 12]      [1890, 2]         234.02                 135
        13      [1890, 3]      [1902, 0]         124.41                 142
        14      [1902, 1]      [1913, 6]         146.55                 138
        15      [1913, 7]      [1923, 7]         107.08                 121
        16      [1923, 8]      [1933, 8]         175.67                 121
        17      [1933, 9]      [1944, 1]         130.23                 125
        18      [1944, 2]      [1954, 3]         198.64                 122
        19      [1954, 4]      [1964, 9]         218.73                 126
        20     [1964, 10]      [1976, 2]          285.0                 137
        21      [1976, 3]      [1986, 8]         156.63                 126
        22      [1986, 9]      [1996, 4]         232.92                 116
        23      [1996, 5]     [2008, 11]         212.48                 151
        24     [2008, 12]     [2019, 11]         180.28                 132
----------------------------------------------------------------------------------------------------
Selected data:
    Training: SC 13 to 22
    Validation: SC 23
    Prediction: SC 24
----------------------------------------------------------------------------------------------------
Selected model: FFNN(
  (model): Sequential(
    (0): Linear(in_features=6, out_features=6, bias=True)
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
    (2): Linear(in_features=6, out_features=3, bias=True)
    (3): LeakyReLU(negative_slope=0.01, inplace=True)
    (4): Linear(in_features=3, out_features=1, bias=True)
  )
)    Training mode: True    Prediction mode: True
----------------------------------------------------------------------------------------------------
Selected optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
----------------------------------------------------------------------------------------------------
Selected scheduler: ReduceLROnPlateau(
    {'factor': 0.9, 'min_lrs': [0], 'patience': 10, 'verbose': True, 'cooldown': 0, 'cooldown_counter': 0, 'mode': 'min', 'threshold': 0.0001, 'threshold_mode': 'rel', 'best': inf, 'num_bad_epochs': 0, 'mode_worse': inf, 'eps': 1e-08, 'last_epoch': 0})
----------------------------------------------------------------------------------------------------
No pre-trained models available, initializing model weights
----------------------------------------------------------------------------------------------------
Training model with solar cycle 12 to 22 data with: num_epochs=1400
Epoch [   5/1400] -> Loss: 109.0236
Epoch [  10/1400] -> Loss: 103.0175
Epoch [  15/1400] -> Loss: 92.3699
Epoch [  20/1400] -> Loss: 80.8038
Epoch [  25/1400] -> Loss: 73.3814
Epoch [  30/1400] -> Loss: 69.4615
Epoch [  35/1400] -> Loss: 67.8674
Epoch [  40/1400] -> Loss: 66.6498
Epoch [  45/1400] -> Loss: 65.4659
Epoch [  50/1400] -> Loss: 63.7507
Epoch [  55/1400] -> Loss: 62.0711
Epoch [  60/1400] -> Loss: 60.2650
Epoch [  65/1400] -> Loss: 59.0220
Epoch [  70/1400] -> Loss: 57.5724
Epoch [  75/1400] -> Loss: 55.5496
Epoch [  80/1400] -> Loss: 50.3177
Epoch [  85/1400] -> Loss: 45.2700
Epoch [  90/1400] -> Loss: 43.2174
Epoch [  95/1400] -> Loss: 40.7649
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_100.pth
----------------------------------------------------------------------------------------------------
Epoch [ 100/1400] -> Loss: 40.5005
Epoch [ 105/1400] -> Loss: 39.6103
Epoch [ 110/1400] -> Loss: 39.3463
Epoch [ 115/1400] -> Loss: 39.6570
Epoch [ 120/1400] -> Loss: 39.3753
Epoch [ 125/1400] -> Loss: 39.3675
Epoch [ 130/1400] -> Loss: 39.2512
Epoch [ 135/1400] -> Loss: 39.3468
Epoch   140: reducing learning rate of group 0 to 4.5000e-04.
Epoch [ 140/1400] -> Loss: 39.2864
Epoch [ 145/1400] -> Loss: 38.5943
Epoch [ 150/1400] -> Loss: 39.0037
Epoch [ 155/1400] -> Loss: 38.7948
Epoch [ 160/1400] -> Loss: 38.7492
Epoch   162: reducing learning rate of group 0 to 4.0500e-04.
Epoch [ 165/1400] -> Loss: 39.2486
Epoch [ 170/1400] -> Loss: 38.8159
Epoch   173: reducing learning rate of group 0 to 3.6450e-04.
Epoch [ 175/1400] -> Loss: 38.7776
Epoch [ 180/1400] -> Loss: 38.5936
Epoch [ 185/1400] -> Loss: 39.0030
Epoch [ 190/1400] -> Loss: 38.3998
Epoch [ 195/1400] -> Loss: 38.3806
Epoch   200: reducing learning rate of group 0 to 3.2805e-04.
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_200.pth
----------------------------------------------------------------------------------------------------
Epoch [ 200/1400] -> Loss: 38.7869
Epoch [ 205/1400] -> Loss: 38.5530
Epoch [ 210/1400] -> Loss: 38.5117
Epoch   214: reducing learning rate of group 0 to 2.9525e-04.
Epoch [ 215/1400] -> Loss: 39.0681
Epoch [ 220/1400] -> Loss: 38.4242
Epoch [ 225/1400] -> Loss: 38.1726
Epoch [ 230/1400] -> Loss: 38.0726
Epoch [ 235/1400] -> Loss: 38.4395
Epoch [ 240/1400] -> Loss: 38.6185
Epoch   241: reducing learning rate of group 0 to 2.6572e-04.
Epoch [ 245/1400] -> Loss: 38.4741
Epoch [ 250/1400] -> Loss: 38.7798
Epoch   252: reducing learning rate of group 0 to 2.3915e-04.
Epoch [ 255/1400] -> Loss: 38.7076
Epoch [ 260/1400] -> Loss: 38.2720
Epoch   263: reducing learning rate of group 0 to 2.1523e-04.
Epoch [ 265/1400] -> Loss: 38.5617
Epoch [ 270/1400] -> Loss: 38.6665
Epoch   274: reducing learning rate of group 0 to 1.9371e-04.
Epoch [ 275/1400] -> Loss: 38.2349
Epoch [ 280/1400] -> Loss: 38.4752
Epoch [ 285/1400] -> Loss: 38.3263
Epoch [ 290/1400] -> Loss: 38.5722
Epoch   295: reducing learning rate of group 0 to 1.7434e-04.
Epoch [ 295/1400] -> Loss: 38.3185
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_300.pth
----------------------------------------------------------------------------------------------------
Epoch [ 300/1400] -> Loss: 38.6952
Epoch [ 305/1400] -> Loss: 38.1535
Epoch   306: reducing learning rate of group 0 to 1.5691e-04.
Epoch [ 310/1400] -> Loss: 38.4478
Epoch [ 315/1400] -> Loss: 38.2705
Epoch [ 320/1400] -> Loss: 38.7083
Epoch   323: reducing learning rate of group 0 to 1.4121e-04.
Epoch [ 325/1400] -> Loss: 39.0089
Epoch [ 330/1400] -> Loss: 38.1643
Epoch   334: reducing learning rate of group 0 to 1.2709e-04.
Epoch [ 335/1400] -> Loss: 38.6734
Epoch [ 340/1400] -> Loss: 38.1275
Epoch   345: reducing learning rate of group 0 to 1.1438e-04.
Epoch [ 345/1400] -> Loss: 38.3246
Epoch [ 350/1400] -> Loss: 37.9985
Epoch [ 355/1400] -> Loss: 38.1407
Epoch [ 360/1400] -> Loss: 38.0964
Epoch [ 365/1400] -> Loss: 38.2773
Epoch   367: reducing learning rate of group 0 to 1.0295e-04.
Epoch [ 370/1400] -> Loss: 38.1364
Epoch [ 375/1400] -> Loss: 38.4820
Epoch   378: reducing learning rate of group 0 to 9.2651e-05.
Epoch [ 380/1400] -> Loss: 38.5725
Epoch [ 385/1400] -> Loss: 38.4239
Epoch [ 390/1400] -> Loss: 38.4469
Epoch   394: reducing learning rate of group 0 to 8.3386e-05.
Epoch [ 395/1400] -> Loss: 38.6411
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_400.pth
----------------------------------------------------------------------------------------------------
Epoch [ 400/1400] -> Loss: 38.3068
Epoch   405: reducing learning rate of group 0 to 7.5047e-05.
Epoch [ 405/1400] -> Loss: 38.6303
Epoch [ 410/1400] -> Loss: 38.4790
Epoch [ 415/1400] -> Loss: 38.3583
Epoch   416: reducing learning rate of group 0 to 6.7543e-05.
Epoch [ 420/1400] -> Loss: 38.6064
Epoch [ 425/1400] -> Loss: 38.2491
Epoch   427: reducing learning rate of group 0 to 6.0788e-05.
Epoch [ 430/1400] -> Loss: 38.3333
Epoch [ 435/1400] -> Loss: 38.2169
Epoch   438: reducing learning rate of group 0 to 5.4709e-05.
Epoch [ 440/1400] -> Loss: 37.9838
Epoch [ 445/1400] -> Loss: 38.2829
Epoch   449: reducing learning rate of group 0 to 4.9239e-05.
Epoch [ 450/1400] -> Loss: 38.4079
Epoch [ 455/1400] -> Loss: 38.4192
Epoch   460: reducing learning rate of group 0 to 4.4315e-05.
Epoch [ 460/1400] -> Loss: 38.8125
Epoch [ 465/1400] -> Loss: 38.7760
Epoch [ 470/1400] -> Loss: 38.5760
Epoch   471: reducing learning rate of group 0 to 3.9883e-05.
Epoch [ 475/1400] -> Loss: 38.6845
Epoch [ 480/1400] -> Loss: 38.1919
Epoch   482: reducing learning rate of group 0 to 3.5895e-05.
Epoch [ 485/1400] -> Loss: 38.1686
Epoch [ 490/1400] -> Loss: 38.4822
Epoch   493: reducing learning rate of group 0 to 3.2305e-05.
Epoch [ 495/1400] -> Loss: 38.2891
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_500.pth
----------------------------------------------------------------------------------------------------
Epoch [ 500/1400] -> Loss: 38.6549
Epoch   504: reducing learning rate of group 0 to 2.9075e-05.
Epoch [ 505/1400] -> Loss: 38.4365
Epoch [ 510/1400] -> Loss: 38.4435
Epoch   515: reducing learning rate of group 0 to 2.6167e-05.
Epoch [ 515/1400] -> Loss: 38.2830
Epoch [ 520/1400] -> Loss: 38.2231
Epoch [ 525/1400] -> Loss: 38.0844
Epoch   526: reducing learning rate of group 0 to 2.3551e-05.
Epoch [ 530/1400] -> Loss: 38.1267
Epoch [ 535/1400] -> Loss: 38.0715
Epoch   537: reducing learning rate of group 0 to 2.1196e-05.
Epoch [ 540/1400] -> Loss: 38.0038
Epoch [ 545/1400] -> Loss: 38.1707
Epoch   548: reducing learning rate of group 0 to 1.9076e-05.
Epoch [ 550/1400] -> Loss: 38.3698
Epoch [ 555/1400] -> Loss: 38.4151
Epoch   559: reducing learning rate of group 0 to 1.7168e-05.
Epoch [ 560/1400] -> Loss: 38.3709
Epoch [ 565/1400] -> Loss: 38.4052
Epoch   570: reducing learning rate of group 0 to 1.5452e-05.
Epoch [ 570/1400] -> Loss: 38.5206
Epoch [ 575/1400] -> Loss: 38.2661
Epoch [ 580/1400] -> Loss: 38.2483
Epoch   581: reducing learning rate of group 0 to 1.3906e-05.
Epoch [ 585/1400] -> Loss: 38.1706
Epoch [ 590/1400] -> Loss: 37.9632
Epoch   592: reducing learning rate of group 0 to 1.2516e-05.
Epoch [ 595/1400] -> Loss: 38.6892
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_600.pth
----------------------------------------------------------------------------------------------------
Epoch [ 600/1400] -> Loss: 38.4925
Epoch   603: reducing learning rate of group 0 to 1.1264e-05.
Epoch [ 605/1400] -> Loss: 37.9334
Epoch [ 610/1400] -> Loss: 38.2627
Epoch   614: reducing learning rate of group 0 to 1.0138e-05.
Epoch [ 615/1400] -> Loss: 37.9905
Epoch [ 620/1400] -> Loss: 38.3586
Epoch   625: reducing learning rate of group 0 to 9.1240e-06.
Epoch [ 625/1400] -> Loss: 38.3807
Epoch [ 630/1400] -> Loss: 38.5591
Epoch [ 635/1400] -> Loss: 37.9145
Epoch   636: reducing learning rate of group 0 to 8.2116e-06.
Epoch [ 640/1400] -> Loss: 38.1042
Epoch [ 645/1400] -> Loss: 38.5311
Epoch   647: reducing learning rate of group 0 to 7.3904e-06.
Epoch [ 650/1400] -> Loss: 38.1339
Epoch [ 655/1400] -> Loss: 38.6130
Epoch   658: reducing learning rate of group 0 to 6.6514e-06.
Epoch [ 660/1400] -> Loss: 38.0275
Epoch [ 665/1400] -> Loss: 38.2014
Epoch   669: reducing learning rate of group 0 to 5.9863e-06.
Epoch [ 670/1400] -> Loss: 38.1209
Epoch [ 675/1400] -> Loss: 38.0588
Epoch   680: reducing learning rate of group 0 to 5.3876e-06.
Epoch [ 680/1400] -> Loss: 38.2769
Epoch [ 685/1400] -> Loss: 38.2253
Epoch [ 690/1400] -> Loss: 38.4543
Epoch   691: reducing learning rate of group 0 to 4.8489e-06.
Epoch [ 695/1400] -> Loss: 38.1701
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_700.pth
----------------------------------------------------------------------------------------------------
Epoch [ 700/1400] -> Loss: 38.3901
Epoch   702: reducing learning rate of group 0 to 4.3640e-06.
Epoch [ 705/1400] -> Loss: 38.0440
Epoch [ 710/1400] -> Loss: 38.3182
Epoch   713: reducing learning rate of group 0 to 3.9276e-06.
Epoch [ 715/1400] -> Loss: 38.3435
Epoch [ 720/1400] -> Loss: 38.5010
Epoch   724: reducing learning rate of group 0 to 3.5348e-06.
Epoch [ 725/1400] -> Loss: 38.7378
Epoch [ 730/1400] -> Loss: 38.2578
Epoch   735: reducing learning rate of group 0 to 3.1813e-06.
Epoch [ 735/1400] -> Loss: 38.2848
Epoch [ 740/1400] -> Loss: 38.3050
Epoch [ 745/1400] -> Loss: 38.4827
Epoch   746: reducing learning rate of group 0 to 2.8632e-06.
Epoch [ 750/1400] -> Loss: 38.5395
Epoch [ 755/1400] -> Loss: 38.3158
Epoch   757: reducing learning rate of group 0 to 2.5769e-06.
Epoch [ 760/1400] -> Loss: 38.3138
Epoch [ 765/1400] -> Loss: 38.3563
Epoch   768: reducing learning rate of group 0 to 2.3192e-06.
Epoch [ 770/1400] -> Loss: 38.1476
Epoch [ 775/1400] -> Loss: 38.2957
Epoch   779: reducing learning rate of group 0 to 2.0873e-06.
Epoch [ 780/1400] -> Loss: 38.1769
Epoch [ 785/1400] -> Loss: 38.5713
Epoch   790: reducing learning rate of group 0 to 1.8786e-06.
Epoch [ 790/1400] -> Loss: 38.0915
Epoch [ 795/1400] -> Loss: 38.0235
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_800.pth
----------------------------------------------------------------------------------------------------
Epoch [ 800/1400] -> Loss: 38.7123
Epoch   801: reducing learning rate of group 0 to 1.6907e-06.
Epoch [ 805/1400] -> Loss: 38.0977
Epoch [ 810/1400] -> Loss: 37.9815
Epoch   812: reducing learning rate of group 0 to 1.5216e-06.
Epoch [ 815/1400] -> Loss: 38.5734
Epoch [ 820/1400] -> Loss: 38.1822
Epoch   823: reducing learning rate of group 0 to 1.3695e-06.
Epoch [ 825/1400] -> Loss: 38.4245
Epoch [ 830/1400] -> Loss: 38.1462
Epoch   834: reducing learning rate of group 0 to 1.2325e-06.
Epoch [ 835/1400] -> Loss: 38.5355
Epoch [ 840/1400] -> Loss: 38.9202
Epoch   845: reducing learning rate of group 0 to 1.1093e-06.
Epoch [ 845/1400] -> Loss: 38.2365
Epoch [ 850/1400] -> Loss: 38.0355
Epoch [ 855/1400] -> Loss: 38.2514
Epoch   856: reducing learning rate of group 0 to 9.9834e-07.
Epoch [ 860/1400] -> Loss: 38.2285
Epoch [ 865/1400] -> Loss: 38.2027
Epoch   867: reducing learning rate of group 0 to 8.9851e-07.
Epoch [ 870/1400] -> Loss: 38.5086
Epoch [ 875/1400] -> Loss: 38.4968
Epoch   878: reducing learning rate of group 0 to 8.0865e-07.
Epoch [ 880/1400] -> Loss: 38.4100
Epoch [ 885/1400] -> Loss: 38.3470
Epoch   889: reducing learning rate of group 0 to 7.2779e-07.
Epoch [ 890/1400] -> Loss: 38.2961
Epoch [ 895/1400] -> Loss: 38.5028
Epoch   900: reducing learning rate of group 0 to 6.5501e-07.
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_900.pth
----------------------------------------------------------------------------------------------------
Epoch [ 900/1400] -> Loss: 38.0848
Epoch [ 905/1400] -> Loss: 38.2400
Epoch [ 910/1400] -> Loss: 38.2103
Epoch   911: reducing learning rate of group 0 to 5.8951e-07.
Epoch [ 915/1400] -> Loss: 38.4897
Epoch [ 920/1400] -> Loss: 38.1323
Epoch   922: reducing learning rate of group 0 to 5.3056e-07.
Epoch [ 925/1400] -> Loss: 37.9645
Epoch [ 930/1400] -> Loss: 38.3000
Epoch   933: reducing learning rate of group 0 to 4.7750e-07.
Epoch [ 935/1400] -> Loss: 38.1142
Epoch [ 940/1400] -> Loss: 38.6683
Epoch   944: reducing learning rate of group 0 to 4.2975e-07.
Epoch [ 945/1400] -> Loss: 38.0849
Epoch [ 950/1400] -> Loss: 38.6010
Epoch   955: reducing learning rate of group 0 to 3.8678e-07.
Epoch [ 955/1400] -> Loss: 38.0036
Epoch [ 960/1400] -> Loss: 38.1573
Epoch [ 965/1400] -> Loss: 38.2071
Epoch   966: reducing learning rate of group 0 to 3.4810e-07.
Epoch [ 970/1400] -> Loss: 38.3108
Epoch [ 975/1400] -> Loss: 38.5084
Epoch   977: reducing learning rate of group 0 to 3.1329e-07.
Epoch [ 980/1400] -> Loss: 38.2610
Epoch [ 985/1400] -> Loss: 38.1772
Epoch   988: reducing learning rate of group 0 to 2.8196e-07.
Epoch [ 990/1400] -> Loss: 38.3644
Epoch [ 995/1400] -> Loss: 38.3965
Epoch   999: reducing learning rate of group 0 to 2.5376e-07.
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_1000.pth
----------------------------------------------------------------------------------------------------
Epoch [1000/1400] -> Loss: 38.2747
Epoch [1005/1400] -> Loss: 38.1259
Epoch  1010: reducing learning rate of group 0 to 2.2839e-07.
Epoch [1010/1400] -> Loss: 38.1381
Epoch [1015/1400] -> Loss: 38.1840
Epoch [1020/1400] -> Loss: 38.4151
Epoch  1021: reducing learning rate of group 0 to 2.0555e-07.
Epoch [1025/1400] -> Loss: 38.2171
Epoch [1030/1400] -> Loss: 38.2116
Epoch  1032: reducing learning rate of group 0 to 1.8499e-07.
Epoch [1035/1400] -> Loss: 38.1635
Epoch [1040/1400] -> Loss: 38.2564
Epoch  1043: reducing learning rate of group 0 to 1.6649e-07.
Epoch [1045/1400] -> Loss: 38.0847
Epoch [1050/1400] -> Loss: 38.2431
Epoch  1054: reducing learning rate of group 0 to 1.4985e-07.
Epoch [1055/1400] -> Loss: 38.1324
Epoch [1060/1400] -> Loss: 38.1718
Epoch  1065: reducing learning rate of group 0 to 1.3486e-07.
Epoch [1065/1400] -> Loss: 38.1702
Epoch [1070/1400] -> Loss: 38.2613
Epoch [1075/1400] -> Loss: 38.2410
Epoch  1076: reducing learning rate of group 0 to 1.2137e-07.
Epoch [1080/1400] -> Loss: 38.1153
Epoch [1085/1400] -> Loss: 38.1477
Epoch  1087: reducing learning rate of group 0 to 1.0924e-07.
Epoch [1090/1400] -> Loss: 38.1647
Epoch [1095/1400] -> Loss: 38.3191
Epoch  1098: reducing learning rate of group 0 to 9.8314e-08.
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_1100.pth
----------------------------------------------------------------------------------------------------
Epoch [1100/1400] -> Loss: 38.2948
Epoch [1105/1400] -> Loss: 38.2958
Epoch [1110/1400] -> Loss: 38.1675
Epoch [1115/1400] -> Loss: 38.0033
Epoch [1120/1400] -> Loss: 38.4272
Epoch [1125/1400] -> Loss: 38.5395
Epoch [1130/1400] -> Loss: 38.6721
Epoch [1135/1400] -> Loss: 38.0831
Epoch [1140/1400] -> Loss: 38.4483
Epoch [1145/1400] -> Loss: 38.5458
Epoch [1150/1400] -> Loss: 38.2827
Epoch [1155/1400] -> Loss: 38.4967
Epoch [1160/1400] -> Loss: 38.2799
Epoch [1165/1400] -> Loss: 38.3168
Epoch [1170/1400] -> Loss: 38.1303
Epoch [1175/1400] -> Loss: 38.5127
Epoch [1180/1400] -> Loss: 38.5050
Epoch [1185/1400] -> Loss: 38.5825
Epoch [1190/1400] -> Loss: 38.4428
Epoch [1195/1400] -> Loss: 38.3263
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_1200.pth
----------------------------------------------------------------------------------------------------
Epoch [1200/1400] -> Loss: 38.0871
Epoch [1205/1400] -> Loss: 38.8884
Epoch [1210/1400] -> Loss: 38.8480
Epoch [1215/1400] -> Loss: 38.6332
Epoch [1220/1400] -> Loss: 38.1717
Epoch [1225/1400] -> Loss: 38.3192
Epoch [1230/1400] -> Loss: 38.5233
Epoch [1235/1400] -> Loss: 38.3601
Epoch [1240/1400] -> Loss: 38.6546
Epoch [1245/1400] -> Loss: 38.8005
Epoch [1250/1400] -> Loss: 38.1358
Epoch [1255/1400] -> Loss: 38.3613
Epoch [1260/1400] -> Loss: 38.0153
Epoch [1265/1400] -> Loss: 38.3588
Epoch [1270/1400] -> Loss: 38.1249
Epoch [1275/1400] -> Loss: 38.6016
Epoch [1280/1400] -> Loss: 38.1979
Epoch [1285/1400] -> Loss: 38.6797
Epoch [1290/1400] -> Loss: 38.7106
Epoch [1295/1400] -> Loss: 37.9711
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_1300.pth
----------------------------------------------------------------------------------------------------
Epoch [1300/1400] -> Loss: 38.4488
Epoch [1305/1400] -> Loss: 38.0856
Epoch [1310/1400] -> Loss: 38.1548
Epoch [1315/1400] -> Loss: 38.1608
Epoch [1320/1400] -> Loss: 38.5375
Epoch [1325/1400] -> Loss: 38.6528
Epoch [1330/1400] -> Loss: 38.2376
Epoch [1335/1400] -> Loss: 38.3501
Epoch [1340/1400] -> Loss: 38.5470
Epoch [1345/1400] -> Loss: 38.3653
Epoch [1350/1400] -> Loss: 38.3508
Epoch [1355/1400] -> Loss: 38.1986
Epoch [1360/1400] -> Loss: 38.0605
Epoch [1365/1400] -> Loss: 38.3842
Epoch [1370/1400] -> Loss: 38.1992
Epoch [1375/1400] -> Loss: 38.6261
Epoch [1380/1400] -> Loss: 38.3502
Epoch [1385/1400] -> Loss: 38.4184
Epoch [1390/1400] -> Loss: 38.1773
Epoch [1395/1400] -> Loss: 38.3816
----------------------------------------------------------------------------------------------------
Model checkpoint saved as _FFNN_1400.pth
----------------------------------------------------------------------------------------------------
Epoch [1400/1400] -> Loss: 38.3698
----------------------------------------------------------------------------------------------------
Plotting loss data...
----------------------------------------------------------------------------------------------------
Training finished successfully.
        Saved model checkpoints can be found in: /home/extern/Dropbox/Research/scripts/models/
        Saved data/loss graphs can be found in: /home/extern/Dropbox/Research/scripts/graphs/
----------------------------------------------------------------------------------------------------
Validating model for solar cycle 23 data
Step [   1/151] -> Date: 5/1996, Target: 7.6, Prediction: 31.840408325195312
Step [   2/151] -> Date: 6/1996, Target: 16.5, Prediction: 22.861595153808594
Step [   3/151] -> Date: 7/1996, Target: 11.8, Prediction: 12.072078704833984
Step [   4/151] -> Date: 8/1996, Target: 19.7, Prediction: 13.369094848632812
Step [   5/151] -> Date: 9/1996, Target: 3.0, Prediction: 12.334972381591797
Step [   6/151] -> Date: 10/1996, Target: 0.7, Prediction: 20.058795928955078
Step [   7/151] -> Date: 11/1996, Target: 24.9, Prediction: 28.686376571655273
Step [   8/151] -> Date: 12/1996, Target: 14.0, Prediction: 27.622026443481445
Step [   9/151] -> Date: 1/1997, Target: 7.4, Prediction: 16.853374481201172
Step [  10/151] -> Date: 2/1997, Target: 11.0, Prediction: 31.875694274902344
Step [  11/151] -> Date: 3/1997, Target: 12.1, Prediction: 41.37288284301758
Step [  12/151] -> Date: 4/1997, Target: 23.0, Prediction: 44.637786865234375
Step [  13/151] -> Date: 5/1997, Target: 25.4, Prediction: 79.59217834472656
Step [  14/151] -> Date: 6/1997, Target: 20.8, Prediction: 69.18156433105469
Step [  15/151] -> Date: 7/1997, Target: 12.9, Prediction: 53.4245719909668
Step [  16/151] -> Date: 8/1997, Target: 35.7, Prediction: 79.0334701538086
Step [  17/151] -> Date: 9/1997, Target: 59.7, Prediction: 68.3704605102539
Step [  18/151] -> Date: 10/1997, Target: 32.8, Prediction: 91.37026977539062
Step [  19/151] -> Date: 11/1997, Target: 50.4, Prediction: 100.04033660888672
Step [  20/151] -> Date: 12/1997, Target: 55.5, Prediction: 79.50906372070312
Step [  21/151] -> Date: 1/1998, Target: 44.5, Prediction: 102.45231628417969
Step [  22/151] -> Date: 2/1998, Target: 50.2, Prediction: 105.97105407714844
Step [  23/151] -> Date: 3/1998, Target: 82.0, Prediction: 110.06788635253906
Step [  24/151] -> Date: 4/1998, Target: 70.6, Prediction: 112.17747497558594
Step [  25/151] -> Date: 5/1998, Target: 74.0, Prediction: 152.2248992919922
Step [  26/151] -> Date: 6/1998, Target: 90.5, Prediction: 152.18905639648438
Step [  27/151] -> Date: 7/1998, Target: 96.7, Prediction: 188.83717346191406
Step [  28/151] -> Date: 8/1998, Target: 121.1, Prediction: 187.8928680419922
Step [  29/151] -> Date: 9/1998, Target: 132.0, Prediction: 180.34783935546875
Step [  30/151] -> Date: 10/1998, Target: 78.5, Prediction: 195.536376953125
Step [  31/151] -> Date: 11/1998, Target: 97.3, Prediction: 166.4981231689453
Step [  32/151] -> Date: 12/1998, Target: 119.2, Prediction: 173.119384765625
Step [  33/151] -> Date: 1/1999, Target: 86.0, Prediction: 209.8056182861328
Step [  34/151] -> Date: 2/1999, Target: 98.0, Prediction: 155.0346221923828
Step [  35/151] -> Date: 3/1999, Target: 103.5, Prediction: 192.79876708984375
Step [  36/151] -> Date: 4/1999, Target: 93.6, Prediction: 187.82208251953125
Step [  37/151] -> Date: 5/1999, Target: 149.6, Prediction: 210.21791076660156
Step [  38/151] -> Date: 6/1999, Target: 207.2, Prediction: 225.4129180908203
Step [  39/151] -> Date: 7/1999, Target: 173.5, Prediction: 209.5415802001953
Step [  40/151] -> Date: 8/1999, Target: 142.3, Prediction: 212.78956604003906
Step [  41/151] -> Date: 9/1999, Target: 106.3, Prediction: 197.56825256347656
Step [  42/151] -> Date: 10/1999, Target: 168.7, Prediction: 196.65548706054688
Step [  43/151] -> Date: 11/1999, Target: 188.3, Prediction: 194.55503845214844
Step [  44/151] -> Date: 12/1999, Target: 116.8, Prediction: 175.09603881835938
Step [  45/151] -> Date: 1/2000, Target: 133.1, Prediction: 166.63401794433594
Step [  46/151] -> Date: 2/2000, Target: 165.7, Prediction: 179.6903076171875
Step [  47/151] -> Date: 3/2000, Target: 217.7, Prediction: 211.5137939453125
Step [  48/151] -> Date: 4/2000, Target: 191.5, Prediction: 184.26272583007812
Step [  49/151] -> Date: 5/2000, Target: 165.9, Prediction: 194.974365234375
Step [  50/151] -> Date: 6/2000, Target: 188.0, Prediction: 183.45945739746094
Step [  51/151] -> Date: 7/2000, Target: 244.3, Prediction: 188.10687255859375
Step [  52/151] -> Date: 8/2000, Target: 180.5, Prediction: 187.2114715576172
Step [  53/151] -> Date: 9/2000, Target: 156.0, Prediction: 201.31350708007812
Step [  54/151] -> Date: 10/2000, Target: 141.6, Prediction: 196.82591247558594
Step [  55/151] -> Date: 11/2000, Target: 158.1, Prediction: 180.20498657226562
Step [  56/151] -> Date: 12/2000, Target: 143.3, Prediction: 169.2838134765625
Step [  57/151] -> Date: 1/2001, Target: 142.6, Prediction: 216.25473022460938
Step [  58/151] -> Date: 2/2001, Target: 121.5, Prediction: 204.644775390625
Step [  59/151] -> Date: 3/2001, Target: 165.8, Prediction: 214.4513397216797
Step [  60/151] -> Date: 4/2001, Target: 161.7, Prediction: 184.6123809814453
Step [  61/151] -> Date: 5/2001, Target: 142.1, Prediction: 199.8154296875
Step [  62/151] -> Date: 6/2001, Target: 202.9, Prediction: 194.5816192626953
Step [  63/151] -> Date: 7/2001, Target: 123.0, Prediction: 193.5673370361328
Step [  64/151] -> Date: 8/2001, Target: 161.5, Prediction: 184.20693969726562
Step [  65/151] -> Date: 9/2001, Target: 238.2, Prediction: 211.6832733154297
Step [  66/151] -> Date: 10/2001, Target: 194.1, Prediction: 156.6957550048828
Step [  67/151] -> Date: 11/2001, Target: 176.6, Prediction: 144.57090759277344
Step [  68/151] -> Date: 12/2001, Target: 213.4, Prediction: 133.9437713623047
Step [  69/151] -> Date: 1/2002, Target: 184.6, Prediction: 130.5101776123047
Step [  70/151] -> Date: 2/2002, Target: 170.2, Prediction: 132.02700805664062
Step [  71/151] -> Date: 3/2002, Target: 147.1, Prediction: 134.17300415039062
Step [  72/151] -> Date: 4/2002, Target: 186.9, Prediction: 151.0066375732422
Step [  73/151] -> Date: 5/2002, Target: 187.5, Prediction: 142.4125518798828
Step [  74/151] -> Date: 6/2002, Target: 128.8, Prediction: 138.7544403076172
Step [  75/151] -> Date: 7/2002, Target: 161.0, Prediction: 135.91775512695312
Step [  76/151] -> Date: 8/2002, Target: 175.6, Prediction: 122.13824462890625
Step [  77/151] -> Date: 9/2002, Target: 187.9, Prediction: 139.6572723388672
Step [  78/151] -> Date: 10/2002, Target: 151.2, Prediction: 136.77806091308594
Step [  79/151] -> Date: 11/2002, Target: 147.2, Prediction: 121.26893615722656
Step [  80/151] -> Date: 12/2002, Target: 135.3, Prediction: 106.27264404296875
Step [  81/151] -> Date: 1/2003, Target: 133.5, Prediction: 102.37074279785156
Step [  82/151] -> Date: 2/2003, Target: 75.7, Prediction: 105.35431671142578
Step [  83/151] -> Date: 3/2003, Target: 100.7, Prediction: 102.1640396118164
Step [  84/151] -> Date: 4/2003, Target: 97.9, Prediction: 92.28614044189453
Step [  85/151] -> Date: 5/2003, Target: 86.8, Prediction: 78.86436462402344
Step [  86/151] -> Date: 6/2003, Target: 118.7, Prediction: 68.96431732177734
Step [  87/151] -> Date: 7/2003, Target: 128.3, Prediction: 77.89461517333984
Step [  88/151] -> Date: 8/2003, Target: 115.4, Prediction: 86.73716735839844
Step [  89/151] -> Date: 9/2003, Target: 78.5, Prediction: 89.68468475341797
Step [  90/151] -> Date: 10/2003, Target: 97.8, Prediction: 81.08853149414062
Step [  91/151] -> Date: 11/2003, Target: 82.9, Prediction: 74.47137451171875
Step [  92/151] -> Date: 12/2003, Target: 72.2, Prediction: 75.7245864868164
Step [  93/151] -> Date: 1/2004, Target: 60.6, Prediction: 67.34477996826172
Step [  94/151] -> Date: 2/2004, Target: 74.6, Prediction: 65.92922973632812
Step [  95/151] -> Date: 3/2004, Target: 74.8, Prediction: 53.537715911865234
Step [  96/151] -> Date: 4/2004, Target: 59.2, Prediction: 60.423336029052734
Step [  97/151] -> Date: 5/2004, Target: 72.8, Prediction: 56.630401611328125
Step [  98/151] -> Date: 6/2004, Target: 66.5, Prediction: 29.17190170288086
Step [  99/151] -> Date: 7/2004, Target: 83.8, Prediction: 30.769506454467773
Step [ 100/151] -> Date: 8/2004, Target: 69.7, Prediction: 30.203712463378906
Step [ 101/151] -> Date: 9/2004, Target: 48.8, Prediction: 37.44651794433594
Step [ 102/151] -> Date: 10/2004, Target: 74.2, Prediction: 36.990966796875
Step [ 103/151] -> Date: 11/2004, Target: 70.1, Prediction: 26.322790145874023
Step [ 104/151] -> Date: 12/2004, Target: 28.9, Prediction: 32.33456039428711
Step [ 105/151] -> Date: 1/2005, Target: 48.1, Prediction: 22.10466194152832
Step [ 106/151] -> Date: 2/2005, Target: 43.5, Prediction: 17.00379753112793
Step [ 107/151] -> Date: 3/2005, Target: 39.6, Prediction: 17.934070587158203
Step [ 108/151] -> Date: 4/2005, Target: 38.7, Prediction: 21.83267593383789
Step [ 109/151] -> Date: 5/2005, Target: 61.9, Prediction: 17.525964736938477
Step [ 110/151] -> Date: 6/2005, Target: 56.8, Prediction: 5.520987033843994
Step [ 111/151] -> Date: 7/2005, Target: 62.4, Prediction: 5.560868263244629
Step [ 112/151] -> Date: 8/2005, Target: 60.5, Prediction: 5.539102077484131
Step [ 113/151] -> Date: 9/2005, Target: 37.2, Prediction: 5.479049205780029
Step [ 114/151] -> Date: 10/2005, Target: 13.2, Prediction: 5.5175275802612305
Step [ 115/151] -> Date: 11/2005, Target: 27.5, Prediction: 5.352627754211426
Step [ 116/151] -> Date: 12/2005, Target: 59.3, Prediction: 5.154388904571533
Step [ 117/151] -> Date: 1/2006, Target: 20.9, Prediction: 5.170420169830322
Step [ 118/151] -> Date: 2/2006, Target: 5.7, Prediction: 5.235915660858154
Step [ 119/151] -> Date: 3/2006, Target: 17.3, Prediction: 5.482802867889404
Step [ 120/151] -> Date: 4/2006, Target: 50.3, Prediction: 5.555187225341797
Step [ 121/151] -> Date: 5/2006, Target: 37.2, Prediction: 5.637298583984375
Step [ 122/151] -> Date: 6/2006, Target: 24.5, Prediction: 5.747561931610107
Step [ 123/151] -> Date: 7/2006, Target: 22.2, Prediction: 5.620101451873779
Step [ 124/151] -> Date: 8/2006, Target: 20.8, Prediction: 5.559370040893555
Step [ 125/151] -> Date: 9/2006, Target: 23.7, Prediction: 5.650715351104736
Step [ 126/151] -> Date: 10/2006, Target: 14.9, Prediction: 5.478799819946289
Step [ 127/151] -> Date: 11/2006, Target: 35.7, Prediction: 5.606846809387207
Step [ 128/151] -> Date: 12/2006, Target: 22.3, Prediction: 5.509435653686523
Step [ 129/151] -> Date: 1/2007, Target: 29.3, Prediction: 5.413885116577148
Step [ 130/151] -> Date: 2/2007, Target: 18.4, Prediction: 5.313296318054199
Step [ 131/151] -> Date: 3/2007, Target: 7.2, Prediction: 6.733726501464844
Step [ 132/151] -> Date: 4/2007, Target: 5.4, Prediction: 22.464008331298828
Step [ 133/151] -> Date: 5/2007, Target: 19.5, Prediction: 28.328296661376953
Step [ 134/151] -> Date: 6/2007, Target: 21.3, Prediction: 35.452354431152344
Step [ 135/151] -> Date: 7/2007, Target: 15.1, Prediction: 30.38857078552246
Step [ 136/151] -> Date: 8/2007, Target: 9.8, Prediction: 31.460033416748047
Step [ 137/151] -> Date: 9/2007, Target: 4.0, Prediction: 33.5864372253418
Step [ 138/151] -> Date: 10/2007, Target: 1.5, Prediction: 55.44602584838867
Step [ 139/151] -> Date: 11/2007, Target: 2.8, Prediction: 45.21952819824219
Step [ 140/151] -> Date: 12/2007, Target: 17.3, Prediction: 57.109947204589844
Step [ 141/151] -> Date: 1/2008, Target: 4.1, Prediction: 55.577781677246094
Step [ 142/151] -> Date: 2/2008, Target: 2.9, Prediction: 58.97948455810547
Step [ 143/151] -> Date: 3/2008, Target: 15.5, Prediction: 77.98348236083984
Step [ 144/151] -> Date: 4/2008, Target: 3.6, Prediction: 77.0460205078125
Step [ 145/151] -> Date: 5/2008, Target: 4.6, Prediction: 83.11244201660156
Step [ 146/151] -> Date: 6/2008, Target: 5.2, Prediction: 95.74734497070312
Step [ 147/151] -> Date: 7/2008, Target: 0.6, Prediction: 95.20150756835938
Step [ 148/151] -> Date: 8/2008, Target: 0.3, Prediction: 84.55814361572266
Step [ 149/151] -> Date: 9/2008, Target: 1.2, Prediction: 90.56051635742188
Step [ 150/151] -> Date: 10/2008, Target: 4.2, Prediction: 95.60725402832031
Step [ 151/151] -> Date: 11/2008, Target: 6.6, Prediction: 89.2720947265625
----------------------------------------------------------------------------------------------------
Average Validation Loss: 36.82223130851392
----------------------------------------------------------------------------------------------------
Plotting data...
----------------------------------------------------------------------------------------------------
Plotting loss data...
----------------------------------------------------------------------------------------------------
Validation finished successfully.

        Saved prediction/loss graphs can be found in: /home/extern/Dropbox/Research/scripts/graphs/
----------------------------------------------------------------------------------------------------
Predicting SC 24 using the above trained model
Step [   1/151] -> Date: 12/2008, Prediction: 13.047334671020508
Step [   2/151] -> Date: 1/2009, Prediction: 14.191268920898438
Step [   3/151] -> Date: 2/2009, Prediction: 21.890432357788086
Step [   4/151] -> Date: 3/2009, Prediction: 21.764371871948242
Step [   5/151] -> Date: 4/2009, Prediction: 17.98798370361328
Step [   6/151] -> Date: 5/2009, Prediction: 21.98524284362793
Step [   7/151] -> Date: 6/2009, Prediction: 16.7828311920166
Step [   8/151] -> Date: 7/2009, Prediction: 15.40379524230957
Step [   9/151] -> Date: 8/2009, Prediction: 20.998197555541992
Step [  10/151] -> Date: 9/2009, Prediction: 16.762271881103516
Step [  11/151] -> Date: 10/2009, Prediction: 23.973085403442383
Step [  12/151] -> Date: 11/2009, Prediction: 21.444950103759766
Step [  13/151] -> Date: 12/2009, Prediction: 47.5632438659668
Step [  14/151] -> Date: 1/2010, Prediction: 42.26973342895508
Step [  15/151] -> Date: 2/2010, Prediction: 54.176082611083984
Step [  16/151] -> Date: 3/2010, Prediction: 70.01797485351562
Step [  17/151] -> Date: 4/2010, Prediction: 57.86290740966797
Step [  18/151] -> Date: 5/2010, Prediction: 65.41706848144531
Step [  19/151] -> Date: 6/2010, Prediction: 60.78550720214844
Step [  20/151] -> Date: 7/2010, Prediction: 62.17549133300781
Step [  21/151] -> Date: 8/2010, Prediction: 64.42135620117188
Step [  22/151] -> Date: 9/2010, Prediction: 86.16938018798828
Step [  23/151] -> Date: 10/2010, Prediction: 75.630126953125
Step [  24/151] -> Date: 11/2010, Prediction: 87.0904312133789
Step [  25/151] -> Date: 12/2010, Prediction: 117.22310638427734
Step [  26/151] -> Date: 1/2011, Prediction: 120.30628967285156
Step [  27/151] -> Date: 2/2011, Prediction: 139.19082641601562
Step [  28/151] -> Date: 3/2011, Prediction: 138.36492919921875
Step [  29/151] -> Date: 4/2011, Prediction: 114.7715072631836
Step [  30/151] -> Date: 5/2011, Prediction: 127.83649444580078
Step [  31/151] -> Date: 6/2011, Prediction: 127.72286987304688
Step [  32/151] -> Date: 7/2011, Prediction: 117.3980484008789
Step [  33/151] -> Date: 8/2011, Prediction: 123.51988220214844
Step [  34/151] -> Date: 9/2011, Prediction: 128.45501708984375
Step [  35/151] -> Date: 10/2011, Prediction: 121.80713653564453
Step [  36/151] -> Date: 11/2011, Prediction: 142.09747314453125
Step [  37/151] -> Date: 12/2011, Prediction: 189.78407287597656
Step [  38/151] -> Date: 1/2012, Prediction: 177.61322021484375
Step [  39/151] -> Date: 2/2012, Prediction: 172.26431274414062
Step [  40/151] -> Date: 3/2012, Prediction: 160.6929168701172
Step [  41/151] -> Date: 4/2012, Prediction: 189.6236572265625
Step [  42/151] -> Date: 5/2012, Prediction: 187.2886505126953
Step [  43/151] -> Date: 6/2012, Prediction: 152.96165466308594
Step [  44/151] -> Date: 7/2012, Prediction: 159.43980407714844
Step [  45/151] -> Date: 8/2012, Prediction: 175.42633056640625
Step [  46/151] -> Date: 9/2012, Prediction: 184.7422332763672
Step [  47/151] -> Date: 10/2012, Prediction: 187.36886596679688
Step [  48/151] -> Date: 11/2012, Prediction: 182.67835998535156
Step [  49/151] -> Date: 12/2012, Prediction: 190.63328552246094
Step [  50/151] -> Date: 1/2013, Prediction: 219.3222198486328
Step [  51/151] -> Date: 2/2013, Prediction: 190.69448852539062
Step [  52/151] -> Date: 3/2013, Prediction: 185.2098388671875
Step [  53/151] -> Date: 4/2013, Prediction: 173.8579864501953
Step [  54/151] -> Date: 5/2013, Prediction: 175.3047332763672
Step [  55/151] -> Date: 6/2013, Prediction: 153.39993286132812
Step [  56/151] -> Date: 7/2013, Prediction: 152.0930938720703
Step [  57/151] -> Date: 8/2013, Prediction: 141.27037048339844
Step [  58/151] -> Date: 9/2013, Prediction: 173.5419464111328
Step [  59/151] -> Date: 10/2013, Prediction: 179.08151245117188
Step [  60/151] -> Date: 11/2013, Prediction: 163.25668334960938
Step [  61/151] -> Date: 12/2013, Prediction: 177.915283203125
Step [  62/151] -> Date: 1/2014, Prediction: 153.00563049316406
Step [  63/151] -> Date: 2/2014, Prediction: 167.96324157714844
Step [  64/151] -> Date: 3/2014, Prediction: 195.33473205566406
Step [  65/151] -> Date: 4/2014, Prediction: 183.44627380371094
Step [  66/151] -> Date: 5/2014, Prediction: 165.53082275390625
Step [  67/151] -> Date: 6/2014, Prediction: 169.57968139648438
Step [  68/151] -> Date: 7/2014, Prediction: 153.98443603515625
Step [  69/151] -> Date: 8/2014, Prediction: 151.57931518554688
Step [  70/151] -> Date: 9/2014, Prediction: 145.86126708984375
Step [  71/151] -> Date: 10/2014, Prediction: 169.60906982421875
Step [  72/151] -> Date: 11/2014, Prediction: 169.40338134765625
Step [  73/151] -> Date: 12/2014, Prediction: 127.48925018310547
Step [  74/151] -> Date: 1/2015, Prediction: 147.99891662597656
Step [  75/151] -> Date: 2/2015, Prediction: 157.17807006835938
Step [  76/151] -> Date: 3/2015, Prediction: 161.31736755371094
Step [  77/151] -> Date: 4/2015, Prediction: 159.5041961669922
Step [  78/151] -> Date: 5/2015, Prediction: 144.6641845703125
Step [  79/151] -> Date: 6/2015, Prediction: 131.28799438476562
Step [  80/151] -> Date: 7/2015, Prediction: 127.70702362060547
Step [  81/151] -> Date: 8/2015, Prediction: 113.42255401611328
Step [  82/151] -> Date: 9/2015, Prediction: 128.21841430664062
Step [  83/151] -> Date: 10/2015, Prediction: 131.3465118408203
Step [  84/151] -> Date: 11/2015, Prediction: 136.4678192138672
Step [  85/151] -> Date: 12/2015, Prediction: 116.64663696289062
Step [  86/151] -> Date: 1/2016, Prediction: 112.91236877441406
Step [  87/151] -> Date: 2/2016, Prediction: 110.94215393066406
Step [  88/151] -> Date: 3/2016, Prediction: 87.91905212402344
Step [  89/151] -> Date: 4/2016, Prediction: 118.6678237915039
Step [  90/151] -> Date: 5/2016, Prediction: 104.300537109375
Step [  91/151] -> Date: 6/2016, Prediction: 85.27693176269531
Step [  92/151] -> Date: 7/2016, Prediction: 87.85067749023438
Step [  93/151] -> Date: 8/2016, Prediction: 80.32556915283203
Step [  94/151] -> Date: 9/2016, Prediction: 81.68911743164062
Step [  95/151] -> Date: 10/2016, Prediction: 69.23426818847656
Step [  96/151] -> Date: 11/2016, Prediction: 71.94180297851562
Step [  97/151] -> Date: 12/2016, Prediction: 38.77656555175781
Step [  98/151] -> Date: 1/2017, Prediction: 60.67491149902344
Step [  99/151] -> Date: 2/2017, Prediction: 40.0877685546875
Step [ 100/151] -> Date: 3/2017, Prediction: 29.804988861083984
Step [ 101/151] -> Date: 4/2017, Prediction: 42.26557159423828
Step [ 102/151] -> Date: 5/2017, Prediction: 60.99828338623047
Step [ 103/151] -> Date: 6/2017, Prediction: 28.170658111572266
Step [ 104/151] -> Date: 7/2017, Prediction: 53.81751251220703
Step [ 105/151] -> Date: 8/2017, Prediction: 34.294166564941406
Step [ 106/151] -> Date: 9/2017, Prediction: 32.3311653137207
Step [ 107/151] -> Date: 10/2017, Prediction: 29.346345901489258
Step [ 108/151] -> Date: 11/2017, Prediction: 49.499481201171875
Step [ 109/151] -> Date: 12/2017, Prediction: 21.175670623779297
Step [ 110/151] -> Date: 1/2018, Prediction: 27.916820526123047
Step [ 111/151] -> Date: 2/2018, Prediction: 26.885099411010742
Step [ 112/151] -> Date: 3/2018, Prediction: 23.84410285949707
Step [ 113/151] -> Date: 4/2018, Prediction: 5.3992719650268555
Step [ 114/151] -> Date: 5/2018, Prediction: 5.544604301452637
Step [ 115/151] -> Date: 6/2018, Prediction: 19.704496383666992
Step [ 116/151] -> Date: 7/2018, Prediction: 5.291503429412842
Step [ 117/151] -> Date: 8/2018, Prediction: 5.11471700668335
Step [ 118/151] -> Date: 9/2018, Prediction: 5.327438831329346
Step [ 119/151] -> Date: 10/2018, Prediction: 17.653152465820312
Step [ 120/151] -> Date: 11/2018, Prediction: 6.27944278717041
Step [ 121/151] -> Date: 12/2018, Prediction: 5.679694175720215
Step [ 122/151] -> Date: 1/2019, Prediction: 5.616278171539307
Step [ 123/151] -> Date: 2/2019, Prediction: 5.751806735992432
Step [ 124/151] -> Date: 3/2019, Prediction: 5.693853855133057
Step [ 125/151] -> Date: 4/2019, Prediction: 5.578138828277588
Step [ 126/151] -> Date: 5/2019, Prediction: 9.194005966186523
Step [ 127/151] -> Date: 6/2019, Prediction: 11.131818771362305
Step [ 128/151] -> Date: 7/2019, Prediction: 9.199760437011719
Step [ 129/151] -> Date: 8/2019, Prediction: 5.410804271697998
Step [ 130/151] -> Date: 9/2019, Prediction: 5.356124401092529
Step [ 131/151] -> Date: 10/2019, Prediction: 5.4267659187316895
Step [ 132/151] -> Date: 11/2019, Prediction: 5.568324089050293
Step [ 133/151] -> Date: 12/2019, Prediction: 16.88243293762207
Step [ 134/151] -> Date: 1/2020, Prediction: 14.55079460144043
Step [ 135/151] -> Date: 2/2020, Prediction: 11.781408309936523
Step [ 136/151] -> Date: 3/2020, Prediction: 12.86263656616211
Step [ 137/151] -> Date: 4/2020, Prediction: 8.38613510131836
Step [ 138/151] -> Date: 5/2020, Prediction: 9.39638900756836
Step [ 139/151] -> Date: 6/2020, Prediction: 15.030900955200195
Step [ 140/151] -> Date: 7/2020, Prediction: 12.997808456420898
Step [ 141/151] -> Date: 8/2020, Prediction: 18.701290130615234
Step [ 142/151] -> Date: 9/2020, Prediction: 23.242578506469727
Step [ 143/151] -> Date: 10/2020, Prediction: 14.04496955871582
Step [ 144/151] -> Date: 11/2020, Prediction: 10.313806533813477
Step [ 145/151] -> Date: 12/2020, Prediction: 41.17472839355469
Step [ 146/151] -> Date: 1/2021, Prediction: 35.466552734375
Step [ 147/151] -> Date: 2/2021, Prediction: 33.649410247802734
Step [ 148/151] -> Date: 3/2021, Prediction: 35.361724853515625
Step [ 149/151] -> Date: 4/2021, Prediction: 39.08994674682617
Step [ 150/151] -> Date: 5/2021, Prediction: 36.63167190551758
Step [ 151/151] -> Date: 6/2021, Prediction: 34.820350646972656
----------------------------------------------------------------------------------------------------
Plotting data...
----------------------------------------------------------------------------------------------------
All newly generated files moved to saved outputs and shared with the cloud(Dropbox) successfully!
